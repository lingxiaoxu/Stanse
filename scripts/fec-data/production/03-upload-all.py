#!/usr/bin/env python3
"""
å®Œæ•´çš„FECæ•°æ®ä¸Šä¼ åˆ°Firebase

è¿™ä¸ªè„šæœ¬å°†:
1. è§£ææ‰€æœ‰ä¸‹è½½çš„FECæ•°æ®æ–‡ä»¶
2. ä¸Šä¼ æ‰€æœ‰åŸå§‹æ•°æ®åˆ°Firestoreå„collection
3. æ„å»ºå¤„ç†åçš„æ•°æ®ï¼ˆå…¬å¸ç´¢å¼•ã€æ±‡æ€»ç­‰ï¼‰
4. è®°å½•å…ƒæ•°æ®
5. æ¸…ç†æœ¬åœ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
"""

import sys
import re
import time
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import subprocess

# å°è¯•å¯¼å…¥Firebase
try:
    import firebase_admin
    from firebase_admin import credentials, firestore
    from google.cloud import firestore as gcloud_firestore
except ImportError:
    print('âŒ Firebaseåº“æœªå®‰è£…')
    print('è¯·è¿è¡Œ: pip install firebase-admin google-cloud-firestore')
    sys.exit(1)

# é…ç½®
DATA_DIR = Path(__file__).parent / 'raw_data'
PROJECT_ID = 'stanseproject'
BATCH_SIZE = 200  # Firestoreæ‰¹æ¬¡å¤§å°ï¼ˆé™ä½ä»¥é¿å…é…é¢é—®é¢˜ï¼‰
DELETE_LOCAL_AFTER_UPLOAD = False  # ä¸Šä¼ åæ˜¯å¦åˆ é™¤æœ¬åœ°æ–‡ä»¶
DELAY_BETWEEN_BATCHES = 0.5  # æ‰¹æ¬¡ä¹‹é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
SKIP_IF_EXISTS = True  # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨åˆ™è·³è¿‡

# å…¨å±€å˜é‡
db = None

def init_firestore():
    """åˆå§‹åŒ–Firestoreè¿æ¥"""
    global db
    print('\nğŸ”§ åˆå§‹åŒ–Firestoreè¿æ¥...')

    try:
        if not firebase_admin._apps:
            try:
                # ä½¿ç”¨gcloud access token
                result = subprocess.run(
                    ['gcloud', 'auth', 'print-access-token'],
                    capture_output=True, text=True, check=True
                )
                access_token = result.stdout.strip()

                from google.oauth2 import credentials as oauth_creds
                cred = oauth_creds.Credentials(access_token)
                firebase_admin.initialize_app(cred, options={'projectId': PROJECT_ID})
                print('  âœ“ ä½¿ç”¨gcloud access tokenè®¤è¯')

            except subprocess.CalledProcessError:
                firebase_admin.initialize_app(options={'projectId': PROJECT_ID})
                print('  âœ“ ä½¿ç”¨Application Default Credentials')

        db = firestore.client()

        # æµ‹è¯•è¿æ¥
        test_ref = db.collection('_connection_test').document('test')
        test_ref.set({'timestamp': datetime.utcnow()})
        test_ref.delete()

        print(f'âœ… Firestoreå·²è¿æ¥ (é¡¹ç›®: {PROJECT_ID})')
        return db

    except Exception as e:
        print(f'âŒ Firestoreè¿æ¥å¤±è´¥: {e}')
        print('\nğŸ’¡ è¯·è¿è¡Œ: gcloud auth login')
        sys.exit(1)


def normalize_company_name(name):
    """æ ‡å‡†åŒ–å…¬å¸åç§°ç”¨äºåŒ¹é…"""
    if not name:
        return ''
    normalized = name.lower()
    # ç§»é™¤å¸¸è§åç¼€
    suffixes = ['corporation', 'corp', 'inc', 'incorporated', 'company', 'co',
                'llc', 'lp', 'ltd', 'limited', 'political action committee', 'pac']
    for suffix in suffixes:
        normalized = re.sub(rf'\b{suffix}\b\.?', '', normalized)
    # ç§»é™¤æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼
    normalized = re.sub(r'[^\w\s]', '', normalized)
    normalized = re.sub(r'\s+', '', normalized).strip()
    return normalized


def upload_committees(year, year_suffix):
    """ä¸Šä¼ å§”å‘˜ä¼šæ•°æ®åˆ° fec_raw_committees"""
    collection_name = 'fec_raw_committees'
    file_path = DATA_DIR / 'committees' / 'cm.txt'

    if not file_path.exists():
        print(f'âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return 0

    print(f'\nğŸ“¤ ä¸Šä¼ å§”å‘˜ä¼šæ•°æ® ({year})...')

    uploaded = 0
    batch = db.batch()
    batch_count = 0

    with open(file_path, 'r', encoding='latin-1') as f:
        for line in f:
            fields = line.strip().split('|')
            if len(fields) < 15:
                continue

            committee_id = fields[0]
            if not committee_id:
                continue

            doc_id = f'{committee_id}_{year}'
            doc_ref = db.collection(collection_name).document(doc_id)

            doc_data = {
                'committee_id': fields[0],
                'committee_name': fields[1],
                'treasurer_name': fields[2],
                'street_1': fields[3],
                'street_2': fields[4],
                'city': fields[5],
                'state': fields[6],
                'zip': fields[7],
                'designation': fields[8],
                'committee_type': fields[9],
                'party': fields[10],
                'filing_frequency': fields[11],
                'interest_group_category': fields[12],
                'connected_org_name': fields[13],
                'candidate_id': fields[14],

                # å…ƒæ•°æ®
                'data_year': year,
                'election_cycle': f'{year-1}-{year}',
                'source_file': f'cm{year_suffix}.zip',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_count += 1
            uploaded += 1

            # æ¯BATCH_SIZEæ¡æäº¤ä¸€æ¬¡
            if batch_count >= BATCH_SIZE:
                batch.commit()
                print(f'  å·²ä¸Šä¼  {uploaded} æ¡è®°å½•...')
                time.sleep(DELAY_BETWEEN_BATCHES)  # å»¶è¿Ÿä»¥é¿å…é…é¢é—®é¢˜
                batch = db.batch()
                batch_count = 0

    # æäº¤å‰©ä½™çš„è®°å½•
    if batch_count > 0:
        batch.commit()
        time.sleep(DELAY_BETWEEN_BATCHES)

    print(f'âœ… æˆåŠŸä¸Šä¼  {uploaded} æ¡å§”å‘˜ä¼šè®°å½•')
    return uploaded


def upload_candidates(year, year_suffix):
    """ä¸Šä¼ å€™é€‰äººæ•°æ®åˆ° fec_raw_candidates"""
    collection_name = 'fec_raw_candidates'
    file_path = DATA_DIR / 'candidates' / 'cn.txt'

    if not file_path.exists():
        print(f'âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return 0

    print(f'\nğŸ“¤ ä¸Šä¼ å€™é€‰äººæ•°æ® ({year})...')

    uploaded = 0
    batch = db.batch()
    batch_count = 0

    with open(file_path, 'r', encoding='latin-1') as f:
        for line in f:
            fields = line.strip().split('|')
            if len(fields) < 15:
                continue

            candidate_id = fields[0]
            if not candidate_id:
                continue

            doc_id = f'{candidate_id}_{year}'
            doc_ref = db.collection(collection_name).document(doc_id)

            doc_data = {
                'candidate_id': fields[0],
                'candidate_name': fields[1],
                'party_affiliation': fields[2],
                'election_year': int(fields[3]) if fields[3] else year,
                'office_sought': fields[4],
                'state': fields[5],
                'district': fields[6],
                'incumbent_challenger_status': fields[7],
                'candidate_status': fields[8],
                'principal_committee_id': fields[9],
                'street_1': fields[10],
                'street_2': fields[11],
                'city': fields[12],
                'state_full': fields[13],
                'zip': fields[14],

                # å…ƒæ•°æ®
                'data_year': year,
                'election_cycle': f'{year-1}-{year}',
                'source_file': f'cn{year_suffix}.zip',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_count += 1
            uploaded += 1

            if batch_count >= BATCH_SIZE:
                batch.commit()
                print(f'  å·²ä¸Šä¼  {uploaded} æ¡è®°å½•...')
                batch = db.batch()
                batch_count = 0

    if batch_count > 0:
        batch.commit()

    print(f'âœ… æˆåŠŸä¸Šä¼  {uploaded} æ¡å€™é€‰äººè®°å½•')
    return uploaded


def upload_contributions_pac_to_candidate(year, year_suffix):
    """ä¸Šä¼ PACå¯¹å€™é€‰äººææ¬¾æ•°æ®åˆ° fec_raw_contributions_pac_to_candidate"""
    collection_name = 'fec_raw_contributions_pac_to_candidate'

    # FECä½¿ç”¨ä¸åŒçš„æ–‡ä»¶å
    file_path = DATA_DIR / 'contributions' / 'itpas2.txt'
    if not file_path.exists():
        file_path = DATA_DIR / 'contributions' / 'pas2.txt'

    if not file_path.exists():
        print(f'âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return 0

    print(f'\nğŸ“¤ ä¸Šä¼ PACææ¬¾æ•°æ® ({year})...')

    uploaded = 0
    batch = db.batch()
    batch_count = 0

    with open(file_path, 'r', encoding='latin-1') as f:
        for line in f:
            fields = line.strip().split('|')
            if len(fields) < 17:
                continue

            committee_id = fields[0]
            candidate_id = fields[16]
            if not committee_id or not candidate_id:
                continue

            # ä½¿ç”¨auto-generated ID
            doc_ref = db.collection(collection_name).document()

            # è§£æé‡‘é¢ï¼ˆä»¥ç¾åˆ†ä¸ºå•ä½ï¼‰
            try:
                amount_str = fields[14]
                amount = int(float(amount_str) * 100) if amount_str else 0
            except (ValueError, IndexError):
                amount = 0

            doc_data = {
                'committee_id': fields[0],
                'amendment_indicator': fields[1],
                'report_type': fields[2],
                'primary_general_indicator': fields[3],
                'image_number': fields[4],
                'transaction_type': fields[5],
                'entity_type': fields[6],
                'contributor_name': fields[7],
                'city': fields[8],
                'state': fields[9],
                'zip': fields[10],
                'employer': fields[11],
                'occupation': fields[12],
                'transaction_date': fields[13],
                'transaction_amount': amount,
                'other_id': fields[15],
                'candidate_id': fields[16],
                'transaction_id': fields[17] if len(fields) > 17 else '',
                'file_number': fields[18] if len(fields) > 18 else '',
                'memo_code': fields[19] if len(fields) > 19 else '',
                'memo_text': fields[20] if len(fields) > 20 else '',

                # å…ƒæ•°æ®
                'data_year': year,
                'election_cycle': f'{year-1}-{year}',
                'source_file': f'pas2{year_suffix}.zip',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_count += 1
            uploaded += 1

            if batch_count >= BATCH_SIZE:
                batch.commit()
                print(f'  å·²ä¸Šä¼  {uploaded} æ¡è®°å½•...')
                batch = db.batch()
                batch_count = 0

    if batch_count > 0:
        batch.commit()

    print(f'âœ… æˆåŠŸä¸Šä¼  {uploaded} æ¡PACææ¬¾è®°å½•')
    return uploaded


def build_company_index():
    """æ„å»ºå…¬å¸ç´¢å¼• - fec_company_index"""
    print('\nğŸ”¨ æ„å»ºå…¬å¸ç´¢å¼•...')

    # ä»æ‰€æœ‰å§”å‘˜ä¼šè®°å½•ä¸­æå–å…¬å¸åç§°
    companies = defaultdict(lambda: {
        'original_names': set(),
        'committee_ids': defaultdict(set)
    })

    # æŸ¥è¯¢æ‰€æœ‰å§”å‘˜ä¼š
    committees_ref = db.collection('fec_raw_committees')
    docs = committees_ref.stream()

    count = 0
    for doc in docs:
        data = doc.to_dict()
        connected_org = data.get('connected_org_name', '')
        committee_id = data.get('committee_id', '')
        data_year = data.get('data_year', 0)

        if connected_org and committee_id:
            normalized = normalize_company_name(connected_org)
            if normalized:
                companies[normalized]['original_names'].add(connected_org)
                companies[normalized]['committee_ids'][committee_id].add(data_year)

        count += 1
        if count % 1000 == 0:
            print(f'  å·²å¤„ç† {count} æ¡å§”å‘˜ä¼šè®°å½•...')

    # ä¸Šä¼ å…¬å¸ç´¢å¼•
    print(f'\n  ä¸Šä¼  {len(companies)} ä¸ªå…¬å¸ç´¢å¼•...')
    batch = db.batch()
    batch_count = 0
    uploaded = 0

    for normalized_name, info in companies.items():
        doc_ref = db.collection('fec_company_index').document(normalized_name)

        # æå–æœç´¢å…³é”®è¯
        search_keywords = set()
        for name in info['original_names']:
            words = re.findall(r'\w+', name.lower())
            search_keywords.update(words)
        search_keywords.add(normalized_name)

        # æ ¼å¼åŒ–committee_ids
        committee_ids_list = []
        for cid, years in info['committee_ids'].items():
            committee_ids_list.append({
                'committee_id': cid,
                'years': sorted(list(years), reverse=True)
            })

        doc_data = {
            'normalized_name': normalized_name,
            'original_names': list(info['original_names']),
            'search_keywords': list(search_keywords),
            'committee_ids': committee_ids_list,
            'total_committees': len(committee_ids_list),
            'created_at': datetime.utcnow(),
            'last_updated': datetime.utcnow()
        }

        batch.set(doc_ref, doc_data)
        batch_count += 1
        uploaded += 1

        if batch_count >= BATCH_SIZE:
            batch.commit()
            batch = db.batch()
            batch_count = 0

    if batch_count > 0:
        batch.commit()

    print(f'âœ… æˆåŠŸæ„å»º {uploaded} ä¸ªå…¬å¸ç´¢å¼•')
    return uploaded


def calculate_company_summaries(year):
    """è®¡ç®—æ¯ä¸ªå…¬å¸çš„æ”¿å…šææ¬¾æ±‡æ€» - fec_company_party_summary"""
    print(f'\nğŸ”¨ è®¡ç®—å…¬å¸æ±‡æ€» ({year})...')

    # 1. è¯»å–å…¬å¸ç´¢å¼•
    company_index = {}
    index_ref = db.collection('fec_company_index')
    for doc in index_ref.stream():
        data = doc.to_dict()
        normalized = data['normalized_name']
        company_index[normalized] = data

    print(f'  æ‰¾åˆ° {len(company_index)} ä¸ªå…¬å¸')

    # 2. è·å–å€™é€‰äººæ”¿å…šæ˜ å°„
    candidate_parties = {}
    candidates_ref = db.collection('fec_raw_candidates').where('data_year', '==', year)
    for doc in candidates_ref.stream():
        data = doc.to_dict()
        candidate_parties[data['candidate_id']] = data.get('party_affiliation', 'UNKNOWN')

    print(f'  æ‰¾åˆ° {len(candidate_parties)} ä¸ªå€™é€‰äºº')

    # 3. è®¡ç®—æ¯ä¸ªå…¬å¸çš„ææ¬¾æ±‡æ€»
    summaries_uploaded = 0
    batch = db.batch()
    batch_count = 0

    for normalized_name, company_data in company_index.items():
        # è·å–è¯¥å…¬å¸çš„æ‰€æœ‰PAC ID
        committee_ids = [item['committee_id'] for item in company_data['committee_ids']]

        if not committee_ids:
            continue

        # æŸ¥è¯¢è¿™äº›PACçš„æ‰€æœ‰ææ¬¾
        contributions_ref = db.collection('fec_raw_contributions_pac_to_candidate') \
            .where('committee_id', 'in', committee_ids[:10]) \
            .where('data_year', '==', year)

        party_totals = defaultdict(lambda: {
            'total_amount': 0,
            'contribution_count': 0,
            'recipients': defaultdict(int)
        })

        for doc in contributions_ref.stream():
            data = doc.to_dict()
            candidate_id = data.get('candidate_id')
            amount = data.get('transaction_amount', 0)

            if candidate_id and amount > 0:
                party = candidate_parties.get(candidate_id, 'UNKNOWN')
                party_totals[party]['total_amount'] += amount
                party_totals[party]['contribution_count'] += 1
                party_totals[party]['recipients'][candidate_id] += amount

        if not party_totals:
            continue

        # æ„å»ºæ±‡æ€»æ–‡æ¡£
        party_totals_formatted = {}
        total_contributed = 0
        total_contributions = 0

        for party, info in party_totals.items():
            # Top recipients
            top_recipients = sorted(
                info['recipients'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]

            party_totals_formatted[party] = {
                'total_amount': info['total_amount'],
                'contribution_count': info['contribution_count'],
                'top_recipients': [
                    {'candidate_id': cid, 'amount': amt}
                    for cid, amt in top_recipients
                ]
            }

            total_contributed += info['total_amount']
            total_contributions += info['contribution_count']

        # ä¸Šä¼ æ±‡æ€»
        doc_id = f'{normalized_name}_{year}'
        doc_ref = db.collection('fec_company_party_summary').document(doc_id)

        doc_data = {
            'company_name': company_data['original_names'][0],
            'normalized_name': normalized_name,
            'data_year': year,
            'election_cycle': f'{year-1}-{year}',
            'party_totals': party_totals_formatted,
            'total_contributed': total_contributed,
            'total_contributions': total_contributions,
            'committee_ids': committee_ids,
            'calculated_at': datetime.utcnow(),
            'last_updated': datetime.utcnow()
        }

        batch.set(doc_ref, doc_data)
        batch_count += 1
        summaries_uploaded += 1

        if batch_count >= BATCH_SIZE:
            batch.commit()
            print(f'  å·²ä¸Šä¼  {summaries_uploaded} ä¸ªå…¬å¸æ±‡æ€»...')
            batch = db.batch()
            batch_count = 0

    if batch_count > 0:
        batch.commit()

    print(f'âœ… æˆåŠŸè®¡ç®— {summaries_uploaded} ä¸ªå…¬å¸æ±‡æ€»')
    return summaries_uploaded


def record_metadata(data_type, year, year_suffix, records_count):
    """è®°å½•ä¸Šä¼ å…ƒæ•°æ®åˆ° fec_data_metadata"""
    doc_id = f'{data_type}_{year}'
    doc_ref = db.collection('fec_data_metadata').document(doc_id)

    doc_data = {
        'data_type': data_type,
        'data_year': year,
        'election_cycle': f'{year-1}-{year}',
        'source_file': f'{data_type}{year_suffix}.zip',
        'source_url': f'https://www.fec.gov/files/bulk-downloads/{year}/{data_type}{year_suffix}.zip',
        'records_count': records_count,
        'upload_completed_at': datetime.utcnow(),
        'upload_status': 'completed'
    }

    doc_ref.set(doc_data)


def cleanup_local_files():
    """æ¸…ç†æœ¬åœ°æ•°æ®æ–‡ä»¶"""
    if not DELETE_LOCAL_AFTER_UPLOAD:
        print('\nğŸ’¡ æœ¬åœ°æ–‡ä»¶ä¿ç•™ï¼ˆDELETE_LOCAL_AFTER_UPLOAD=Falseï¼‰')
        return

    print('\nğŸ§¹ æ¸…ç†æœ¬åœ°æ–‡ä»¶...')
    import shutil

    if DATA_DIR.exists():
        shutil.rmtree(DATA_DIR)
        print('âœ… æœ¬åœ°æ–‡ä»¶å·²åˆ é™¤')


def main():
    """ä¸»å‡½æ•°"""
    print('\n' + '='*70)
    print('ğŸš€ FECæ•°æ®å®Œæ•´ä¸Šä¼ åˆ°Firebase')
    print('='*70)

    # 1. åˆå§‹åŒ–Firebase
    init_firestore()

    # 2. æ£€æŸ¥æ•°æ®ç›®å½•
    if not DATA_DIR.exists():
        print(f'\nâŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}')
        print('è¯·å…ˆè¿è¡Œ: python3 download_fec_data.py')
        sys.exit(1)

    # å®šä¹‰è¦å¤„ç†çš„å¹´ä»½
    years_to_process = [
        (2024, '24'),
        (2022, '22'),
        (2020, '20'),
        (2018, '18'),
        (2016, '16'),
    ]

    # 3. ä¸Šä¼ æ¯å¹´çš„æ•°æ®
    for year, year_suffix in years_to_process:
        print(f'\n{"="*70}')
        print(f'å¤„ç† {year} å¹´æ•°æ®')
        print(f'{"="*70}')

        # ä¸Šä¼ åŸå§‹æ•°æ®
        committees_count = upload_committees(year, year_suffix)
        candidates_count = upload_candidates(year, year_suffix)
        contributions_count = upload_contributions_pac_to_candidate(year, year_suffix)

        # è®°å½•å…ƒæ•°æ®
        record_metadata('committees', year, year_suffix, committees_count)
        record_metadata('candidates', year, year_suffix, candidates_count)
        record_metadata('contributions', year, year_suffix, contributions_count)

    # 4. æ„å»ºå…¬å¸ç´¢å¼•ï¼ˆåªéœ€è¦è¿è¡Œä¸€æ¬¡ï¼Œå› ä¸ºå®ƒä¼šæ±‡æ€»æ‰€æœ‰å¹´ä»½ï¼‰
    build_company_index()

    # 5. è®¡ç®—æ¯å¹´çš„å…¬å¸æ±‡æ€»
    for year, _ in years_to_process:
        calculate_company_summaries(year)

    # 6. æ¸…ç†æœ¬åœ°æ–‡ä»¶
    cleanup_local_files()

    print('\n' + '='*70)
    print('âœ… æ‰€æœ‰æ•°æ®ä¸Šä¼ å®Œæˆï¼')
    print('='*70)
    print('\nğŸ’¡ ä¸‹ä¸€æ­¥:')
    print('  1. è®¿é—® Firebase Console ç¡®è®¤æ•°æ®')
    print('  2. æ„å»ºå‰ç«¯æŸ¥è¯¢API')
    print('  3. è®¾ç½®å®šæœŸæ›´æ–°è®¡åˆ’')
    print()


if __name__ == '__main__':
    main()
