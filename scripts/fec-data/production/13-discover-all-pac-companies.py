#!/usr/bin/env python3
"""
å‘ç°æ‰€æœ‰æœ‰PACæ•°æ®çš„å…¬å¸

ç›®æ ‡:
1. æ‰«æ fec_raw_committees ä¸­æ‰€æœ‰ PAC (type='Q')
2. æ£€æŸ¥æ¯ä¸ªPACæ˜¯å¦æœ‰transfersæ•°æ®
3. æŒ‰ç…§connected_org_nameåˆ†ç»„ï¼Œæ‰¾å‡ºæ‰€æœ‰æœ‰PACææ¬¾çš„å…¬å¸
4. è¾“å‡ºå…¬å¸åˆ—è¡¨ï¼Œå‡†å¤‡è¿›è¡Œåç»­å¤„ç†

è¿è¡Œæ–¹å¼:
    python3 13-discover-all-pac-companies.py --scan
    python3 13-discover-all-pac-companies.py --update-index  # æ›´æ–°fec_company_indexå’Œfec_company_name_variants

è¾“å‡º:
    - logs/fec-data/discovered_pac_companies.json
    - åŒ…å«æ‰€æœ‰æœ‰PACæ•°æ®çš„å…¬å¸åŠå…¶ç›¸å…³ä¿¡æ¯
"""

import os
import sys
import json
import argparse
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Set
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.abspath(os.path.join(SCRIPT_DIR, '../../..')))

# å¯¼å…¥Firebase Admin
import firebase_admin
from firebase_admin import credentials, firestore

# åˆå§‹åŒ–Firebase
if not firebase_admin._apps:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {
        'projectId': 'stanseproject'
    })

db = firestore.client()

# æ—¥å¿—ç›®å½•
LOGS_DIR = os.path.join(SCRIPT_DIR, '../../../logs/fec-data')
os.makedirs(LOGS_DIR, exist_ok=True)


class PACCompanyDiscovery:
    def __init__(self, data_year: int = 2024, dry_run: bool = False):
        self.db = db
        self.data_year = data_year
        self.dry_run = dry_run

        # å‘ç°çš„å…¬å¸ä¿¡æ¯
        self.discovered_companies = {}  # {normalized_name: company_info}
        self.existing_index = set()  # ç°æœ‰çš„indexä¸­çš„normalized_name

        print(f"âœ… Firebase initialized (project: stanseproject)")
        print(f"ğŸ“… Data year: {data_year}")
        if dry_run:
            print(f"ğŸ” DRY RUN MODE - No writes to Firebase")
        print()

    def normalize_company_name(self, name: str) -> str:
        """è§„èŒƒåŒ–å…¬å¸åç§°"""
        if not name:
            return ""

        # è½¬å°å†™ï¼Œå»ç©ºæ ¼
        normalized = name.lower().strip()

        # ç§»é™¤ç¬¦å·
        normalized = normalized.replace('&', '').replace(',', '').replace('.', '').replace("'", '')

        # ç§»é™¤å¸¸è§åç¼€
        for suffix in [' inc', ' incorporated', ' corporation', ' corp', ' company', ' co', ' ltd', ' llc', ' limited']:
            if normalized.endswith(suffix):
                normalized = normalized[:-len(suffix)].strip()

        # æ¸…ç†å¤šä½™ç©ºæ ¼
        normalized = ' '.join(normalized.split())

        return normalized

    def extract_company_from_committee_name(self, committee_name: str) -> str:
        """
        ä»committee_nameä¸­æå–å…¬å¸åç§°

        é€‚ç”¨äºconnected_org_name='NONE'çš„æƒ…å†µ

        ç¤ºä¾‹:
        - "ALASKA STATE MEDICAL ASSOCIATION POLITICAL ACTION COMMITTEE (ALPAC)" â†’ "ALASKA STATE MEDICAL ASSOCIATION"
        - "AKIN GUMP STRAUSS HAUER & FELD LLP CIVIC ACTION COMMITTEE" â†’ "AKIN GUMP STRAUSS HAUER & FELD LLP"
        - "BRACEPAC" â†’ "BRACEPAC"
        """
        if not committee_name:
            return ""

        name = committee_name.strip()
        original_name = name

        # ç§»é™¤æ‹¬å·åŠå…¶å†…å®¹ (å¦‚ "(ALPAC)") - å…ˆåšè¿™ä¸ª
        import re
        name = re.sub(r'\([^)]*\)', '', name).strip()

        # ç§»é™¤å¸¸è§çš„PAC/å§”å‘˜ä¼šç›¸å…³åç¼€ - ä½†è¦å°å¿ƒä¸è¦ç§»é™¤å¤ªå¤š
        # æŒ‰é¡ºåºå°è¯•ï¼Œä»æœ€å…·ä½“çš„åˆ°æœ€é€šç”¨çš„
        pac_suffixes = [
            ' POLITICAL ACTION COMMITTEE',
            ' CIVIC ACTION COMMITTEE',
            ' FEDERAL POLITICAL ACTION COMMITTEE',
            ' FEDERAL PAC',
        ]

        # å°è¯•åŒ¹é…å¹¶ç§»é™¤ (éœ€è¦æœ‰å‰å¯¼ç©ºæ ¼ï¼Œé¿å…è¯¯åŒ¹é…)
        for suffix in pac_suffixes:
            if name.upper().endswith(suffix.strip()):
                name = name[:-(len(suffix.strip()))].strip()
                break

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä¸Šè¿°é•¿åç¼€ï¼Œæ£€æŸ¥æ˜¯å¦ä»¥å•ç‹¬çš„PACæˆ–COMMITTEEç»“å°¾
        # ä½†åªåœ¨æœ‰å¤šä¸ªè¯çš„æƒ…å†µä¸‹æ‰ç§»é™¤ (é¿å…"BRACEPAC"å˜æˆ"BRACE")
        if ' ' in name:
            if name.upper().endswith(' PAC'):
                name = name[:-4].strip()
            elif name.upper().endswith(' COMMITTEE'):
                name = name[:-10].strip()

        # å¦‚æœæå–åçš„åç§°å¤ªçŸ­ï¼ˆå¯èƒ½æå–å¤±è´¥ï¼‰ï¼Œè¿”å›åŸå
        if len(name) < 3:
            return original_name

        return name

    def load_existing_index(self):
        """åŠ è½½ç°æœ‰çš„fec_company_index"""
        print("ğŸ“‚ Loading existing fec_company_index...")

        try:
            docs = self.db.collection('fec_company_index').stream()
            count = 0
            for doc in docs:
                data = doc.to_dict()
                normalized_name = data.get('normalized_name', '')
                if normalized_name:
                    self.existing_index.add(normalized_name)
                    count += 1

            print(f"   âœ… Loaded {count} existing companies from index\\n")
        except Exception as e:
            print(f"   âš ï¸  Error loading index: {str(e)}\\n")

    def scan_all_pac_committees(self) -> Dict[str, List[Dict]]:
        """
        æ‰«ææ‰€æœ‰PACå§”å‘˜ä¼šï¼ŒæŒ‰ç…§connected_org_nameåˆ†ç»„

        Returns:
            {connected_org_name: [committee1, committee2, ...]}
        """
        print("=" * 70)
        print("ğŸ” Scanning all PAC committees (type='Q')...")
        print("=" * 70)

        committees_by_org = defaultdict(list)

        try:
            # æŸ¥è¯¢æ‰€æœ‰ PAC (committee_type='Q')
            committee_ref = self.db.collection('fec_raw_committees')

            # Firestore doesn't support offset-based pagination well, so we'll use cursor-based
            batch_size = 1000
            last_doc = None
            total_pacs = 0

            while True:
                # åªæŸ¥è¯¢ committee_type='Q', ä¸é™åˆ¶year (å› ä¸ºyearå­—æ®µå¯èƒ½ä¸å­˜åœ¨æˆ–æ ¼å¼ä¸ä¸€è‡´)
                query = committee_ref.where(
                    filter=firestore.FieldFilter('committee_type', '==', 'Q')
                ).limit(batch_size)

                if last_doc:
                    query = query.start_after(last_doc)

                docs = list(query.stream())

                if not docs:
                    break

                for doc in docs:
                    data = doc.to_dict()
                    connected_org = data.get('connected_org_name', '').strip()
                    committee_name = data.get('committee_name', '').strip()

                    # å¦‚æœconnected_orgæ˜¯NONEï¼Œä»committee_nameæå–å…¬å¸å
                    if connected_org.upper() == 'NONE':
                        extracted_org = self.extract_company_from_committee_name(committee_name)
                        if extracted_org:
                            connected_org = extracted_org
                        else:
                            # æå–å¤±è´¥ï¼Œè·³è¿‡
                            continue

                    # è·³è¿‡ç©ºå€¼
                    if connected_org:
                        committees_by_org[connected_org].append({
                            'committee_id': data.get('committee_id', ''),
                            'committee_name': committee_name,
                            'connected_org_name': connected_org
                        })
                        total_pacs += 1

                last_doc = docs[-1]
                print(f"   Processed {total_pacs} PACs so far...")

                if len(docs) < batch_size:
                    break

            print(f"\\n   âœ… Found {total_pacs} PAC committees from {len(committees_by_org)} organizations\\n")

        except Exception as e:
            print(f"   âŒ Error scanning committees: {str(e)}\\n")
            import traceback
            traceback.print_exc()

        return dict(committees_by_org)

    def check_committee_has_transfers(self, committee_id: str) -> Tuple[bool, int, float]:
        """
        æ£€æŸ¥å§”å‘˜ä¼šæ˜¯å¦æœ‰è½¬è´¦è®°å½•

        Returns:
            (has_transfers, transfer_count, total_amount)
        """
        try:
            transfer_ref = self.db.collection('fec_raw_transfers')

            # åªæŸ¥è¯¢1æ¡è®°å½•æ¥å¿«é€Ÿæ£€æŸ¥
            docs = list(transfer_ref.where(
                filter=firestore.FieldFilter('committee_id', '==', committee_id)
            ).limit(1).stream())

            has_transfers = len(docs) > 0

            if has_transfers:
                # å¦‚æœæœ‰transferï¼Œç»Ÿè®¡æ€»æ•°å’Œé‡‘é¢
                # ç”±äºFirestoreæ²¡æœ‰èšåˆæŸ¥è¯¢ï¼Œæˆ‘ä»¬æš‚æ—¶è¿”å›has_transferså³å¯
                # åç»­æ”¶é›†æ—¶ä¼šè¯¦ç»†ç»Ÿè®¡
                return True, 1, 0.0

            return False, 0, 0.0

        except Exception as e:
            return False, 0, 0.0

    def analyze_pac_companies(self, committees_by_org: Dict[str, List[Dict]]):
        """
        åˆ†ææ¯ä¸ªç»„ç»‡çš„PACï¼Œæ£€æŸ¥æ˜¯å¦æœ‰transfers
        """
        print("=" * 70)
        print("ğŸ” Analyzing PAC transfers for each organization...")
        print("=" * 70)

        total_orgs = len(committees_by_org)
        orgs_with_transfers = 0
        orgs_without_transfers = 0

        for i, (org_name, committees) in enumerate(committees_by_org.items(), 1):
            if i % 100 == 0:
                print(f"   Progress: {i}/{total_orgs} organizations...")

            # æ£€æŸ¥è‡³å°‘ä¸€ä¸ªcommitteeæœ‰transfers
            has_any_transfer = False
            total_committees = len(committees)
            committees_with_transfers = []

            for committee in committees:
                has_transfer, count, amount = self.check_committee_has_transfers(committee['committee_id'])
                if has_transfer:
                    has_any_transfer = True
                    committees_with_transfers.append(committee)

            if has_any_transfer:
                # è§„èŒƒåŒ–å…¬å¸åç§°
                normalized_name = self.normalize_company_name(org_name)

                # æ£€æŸ¥æ˜¯å¦å·²åœ¨indexä¸­
                is_new = normalized_name not in self.existing_index

                # ä¿å­˜å…¬å¸ä¿¡æ¯
                self.discovered_companies[normalized_name] = {
                    'original_name': org_name,
                    'normalized_name': normalized_name,
                    'total_pac_committees': total_committees,
                    'committees_with_transfers': len(committees_with_transfers),
                    'committees': committees_with_transfers,
                    'is_new_company': is_new,
                    'data_year': self.data_year
                }

                orgs_with_transfers += 1
            else:
                orgs_without_transfers += 1

        print(f"\\n   âœ… Analysis complete:")
        print(f"      Organizations with transfers: {orgs_with_transfers}")
        print(f"      Organizations without transfers: {orgs_without_transfers}")
        print(f"      New companies (not in index): {sum(1 for c in self.discovered_companies.values() if c['is_new_company'])}")
        print()

    def save_discovery_report(self):
        """ä¿å­˜å‘ç°æŠ¥å‘Šåˆ°JSONæ–‡ä»¶"""
        output_file = os.path.join(LOGS_DIR, f'discovered_pac_companies_{self.data_year}.json')

        report = {
            'scan_date': datetime.now().isoformat(),
            'data_year': self.data_year,
            'total_companies': len(self.discovered_companies),
            'new_companies': sum(1 for c in self.discovered_companies.values() if c['is_new_company']),
            'existing_companies': sum(1 for c in self.discovered_companies.values() if not c['is_new_company']),
            'companies': self.discovered_companies
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ Discovery report saved to: {output_file}")
        print()

    def update_company_index_and_variants(self):
        """
        ä¸ºæ–°å‘ç°çš„å…¬å¸æ›´æ–° fec_company_index å’Œ fec_company_name_variants

        æ³¨æ„: åªæ›´æ–°is_new_company=Trueçš„å…¬å¸
        """
        print("=" * 70)
        print("ğŸ“ Updating fec_company_index and fec_company_name_variants...")
        print("=" * 70)

        new_companies = [c for c in self.discovered_companies.values() if c['is_new_company']]

        if not new_companies:
            print("   â„¹ï¸  No new companies to add")
            return

        print(f"   ğŸ“Š {len(new_companies)} new companies to add\\n")

        success_count = 0
        error_count = 0

        for i, company_info in enumerate(new_companies, 1):
            try:
                normalized_name = company_info['normalized_name']
                original_name = company_info['original_name']

                print(f"   [{i}/{len(new_companies)}] {original_name}")

                if self.dry_run:
                    print(f"      [DRY RUN] Would add to index: {normalized_name}")
                    success_count += 1
                    continue

                # Sanitize document IDs (Firestoreä¸å…è®¸"/"å­—ç¬¦)
                safe_normalized_name = normalized_name.replace('/', '-').replace('\\', '-')
                safe_original_name = original_name.replace('/', '-').replace('\\', '-')

                # 1. æ·»åŠ åˆ° fec_company_index
                index_ref = self.db.collection('fec_company_index').document(safe_normalized_name)
                index_ref.set({
                    'normalized_name': normalized_name,
                    'original_names': [original_name],
                    'has_pac_data': True,
                    'created_at': firestore.SERVER_TIMESTAMP,
                    'last_updated': firestore.SERVER_TIMESTAMP,
                    'source': 'pac_discovery'
                }, merge=True)

                # 2. æ·»åŠ åˆ° fec_company_name_variants
                variant_doc_id = f"{safe_normalized_name}_{safe_original_name.lower().replace(' ', '_')}"
                variant_ref = self.db.collection('fec_company_name_variants').document(variant_doc_id)
                variant_ref.set({
                    'normalized_name': normalized_name,
                    'variant_name': original_name,
                    'variant_name_lower': original_name.lower(),
                    'created_at': firestore.SERVER_TIMESTAMP,
                    'source': 'pac_discovery'
                })

                print(f"      âœ… Added to index and variants")
                success_count += 1

            except Exception as e:
                print(f"      âŒ Error: {str(e)}")
                error_count += 1

        print(f"\\n   âœ… Update complete:")
        print(f"      Success: {success_count}")
        print(f"      Errors: {error_count}")
        print()

    def run_discovery(self):
        """è¿è¡Œå®Œæ•´çš„å‘ç°æµç¨‹"""
        start_time = time.time()

        print(f"\\n{'='*70}")
        print(f"ğŸ” PAC Company Discovery")
        print(f"{'='*70}")
        print(f"ğŸ•’ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\\n")

        # 1. åŠ è½½ç°æœ‰index
        self.load_existing_index()

        # 2. æ‰«ææ‰€æœ‰PACå§”å‘˜ä¼š
        committees_by_org = self.scan_all_pac_committees()

        # 3. åˆ†ææ¯ä¸ªç»„ç»‡çš„transfers
        self.analyze_pac_companies(committees_by_org)

        # 4. ä¿å­˜å‘ç°æŠ¥å‘Š
        self.save_discovery_report()

        execution_time = time.time() - start_time

        print(f"{'='*70}")
        print(f"âœ… Discovery Complete")
        print(f"{'='*70}")
        print(f"ğŸ“Š Total companies found: {len(self.discovered_companies)}")
        print(f"ğŸ†• New companies: {sum(1 for c in self.discovered_companies.values() if c['is_new_company'])}")
        print(f"â™»ï¸  Existing companies: {sum(1 for c in self.discovered_companies.values() if not c['is_new_company'])}")
        print(f"ğŸ•’ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  Execution time: {execution_time:.1f} seconds")
        print(f"{'='*70}\\n")


def main():
    parser = argparse.ArgumentParser(description='Discover all companies with PAC donation data')
    parser.add_argument('--scan', action='store_true', help='Scan and discover all PAC companies')
    parser.add_argument('--update-index', action='store_true', help='Update fec_company_index and variants for new companies')
    parser.add_argument('--year', type=int, default=2024, help='Data year (default: 2024)')
    parser.add_argument('--dry-run', action='store_true', help='Dry run mode (no writes)')

    args = parser.parse_args()

    if not args.scan and not args.update_index:
        print("âŒ Please specify --scan or --update-index")
        print("   Example: python3 13-discover-all-pac-companies.py --scan")
        sys.exit(1)

    discovery = PACCompanyDiscovery(data_year=args.year, dry_run=args.dry_run)

    if args.scan:
        discovery.run_discovery()

    if args.update_index:
        # Load discovery report
        report_file = os.path.join(LOGS_DIR, f'discovered_pac_companies_{args.year}.json')
        if not os.path.exists(report_file):
            print(f"âŒ Discovery report not found: {report_file}")
            print(f"   Please run --scan first")
            sys.exit(1)

        with open(report_file, 'r', encoding='utf-8') as f:
            report = json.load(f)

        discovery.discovered_companies = report['companies']
        discovery.load_existing_index()
        discovery.update_company_index_and_variants()


if __name__ == "__main__":
    main()
