#!/usr/bin/env python3
"""
è‡ªåŠ¨æ„å»ºå…¬å¸åç§°å˜ä½“æ˜ å°„è¡¨
ä»æ‰€æœ‰20,934ä¸ªå§”å‘˜ä¼šè®°å½•ä¸­æå–å…¬å¸åç§°ï¼Œä½¿ç”¨æ¨¡ç³ŠåŒ¹é…è‡ªåŠ¨åˆ†ç»„å˜ä½“

æ–°å¢åŠŸèƒ½:
--cleanup: æ¸…ç†ç©ºè®°å½•
--rebuild-from-index: ä»fec_company_indexé‡å»ºvariants
"""

import sys
import os
import argparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import json
import re
import hashlib

try:
    import firebase_admin
    from firebase_admin import credentials, firestore
    from rapidfuzz import fuzz, process
    import requests
except ImportError as e:
    print(f'âŒ ç¼ºå°‘ä¾èµ–åº“: {e}')
    print('å®‰è£…: pip install firebase-admin rapidfuzz requests')
    sys.exit(1)

PROJECT_ID = 'stanseproject'
REPORTS_DIR = Path(__file__).parent.parent / 'reports'
PROGRESS_FILE = REPORTS_DIR / '12-variant-building-progress.json'

db = None
gemini_api_key = None

# ============================================================================
# SP500 DATA - Import from unified data source
# ============================================================================
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from data.sp500Companies import SP500_TICKERS, TICKER_TO_NAME

# Build SP500_COMPANIES dict from unified data (ticker -> name)
SP500_COMPANIES = {ticker: TICKER_TO_NAME[ticker] for ticker in SP500_TICKERS}

# æ‰‹åŠ¨è¦†ç›–çš„9å®¶å·²éªŒè¯å…¬å¸
VERIFIED_COMPANIES = {
    'GOOGLE': {
        'canonical_name': 'GOOGLE',
        'display_name': 'Google Inc.',
        'variants': ['GOOGLE INC', 'GOOGLE LLC', 'ALPHABET INC', 'ALPHABET', 'GOOGLE'],
        'stock_ticker': 'GOOGL',
        'industry': 'Technology'
    },
    'MICROSOFT': {
        'canonical_name': 'MICROSOFT',
        'display_name': 'Microsoft Corporation',
        'variants': ['MICROSOFT CORP', 'MICROSOFT CORPORATION', 'MICROSOFT'],
        'stock_ticker': 'MSFT',
        'industry': 'Technology'
    },
    'AMAZON': {
        'canonical_name': 'AMAZON',
        'display_name': 'Amazon.com Inc.',
        'variants': ['AMAZON.COM INC', 'AMAZON COM INC', 'AMAZON', 'AMAZON INC'],
        'stock_ticker': 'AMZN',
        'industry': 'Technology'
    },
    'APPLE': {
        'canonical_name': 'APPLE',
        'display_name': 'Apple Inc.',
        'variants': ['APPLE INC', 'APPLE COMPUTER INC', 'APPLE'],
        'stock_ticker': 'AAPL',
        'industry': 'Technology'
    },
    'META': {
        'canonical_name': 'META',
        'display_name': 'Meta Platforms Inc.',
        'variants': ['META PLATFORMS INC', 'FACEBOOK INC', 'FACEBOOK', 'META'],
        'stock_ticker': 'META',
        'industry': 'Technology'
    },
    'JPMORGAN': {
        'canonical_name': 'JPMORGAN',
        'display_name': 'JPMorgan Chase & Co.',
        'variants': ['JPMORGAN CHASE & CO', 'JP MORGAN CHASE', 'JPMORGAN', 'JPMORGAN CHASE'],
        'stock_ticker': 'JPM',
        'industry': 'Financial Services'
    },
    'GOLDMAN SACHS': {
        'canonical_name': 'GOLDMAN SACHS',
        'display_name': 'Goldman Sachs Group Inc.',
        'variants': ['GOLDMAN SACHS GROUP INC', 'GOLDMAN SACHS', 'GOLDMAN SACHS & CO'],
        'stock_ticker': 'GS',
        'industry': 'Financial Services'
    },
    'BOEING': {
        'canonical_name': 'BOEING',
        'display_name': 'The Boeing Company',
        'variants': ['BOEING CO', 'BOEING COMPANY', 'BOEING', 'THE BOEING COMPANY'],
        'stock_ticker': 'BA',
        'industry': 'Aerospace & Defense'
    },
    'LOCKHEED MARTIN': {
        'canonical_name': 'LOCKHEED MARTIN',
        'display_name': 'Lockheed Martin Corporation',
        'variants': ['LOCKHEED MARTIN CORP', 'LOCKHEED MARTIN CORPORATION', 'LOCKHEED MARTIN'],
        'stock_ticker': 'LMT',
        'industry': 'Aerospace & Defense'
    }
}

def init_firestore():
    """åˆå§‹åŒ–Firestore - ä½¿ç”¨ADC"""
    global db
    print('ğŸ”§ åˆå§‹åŒ–Firestoreè¿æ¥...')

    try:
        if not firebase_admin._apps:
            print('  â„¹ï¸  ä½¿ç”¨é»˜è®¤å‡­è¯é“¾ï¼ˆgcloud/ç¯å¢ƒå˜é‡ï¼‰...')
            firebase_admin.initialize_app(options={'projectId': PROJECT_ID})

        db = firestore.client()
        print(f'âœ… Firestoreå·²è¿æ¥ (é¡¹ç›®: {PROJECT_ID})\n')
        return db
    except Exception as e:
        print(f'âŒ å¤±è´¥: {e}')
        sys.exit(1)

def normalize_name(name):
    """æ ‡å‡†åŒ–å…¬å¸åç§°"""
    if not name:
        return ''

    # è½¬å¤§å†™
    name = name.upper().strip()

    # ç§»é™¤å¸¸è§åç¼€
    suffixes = [
        ' INC', ' CORP', ' LLC', ' LLP', ' LP', ' LTD', ' CO',
        ' CORPORATION', ' INCORPORATED', ' COMPANY', ' LIMITED',
        ' & CO', ' AND CO', ',', '.'
    ]

    for suffix in suffixes:
        if name.endswith(suffix):
            name = name[:-len(suffix)].strip()

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    name = re.sub(r'[^\w\s&-]', '', name)

    # æ ‡å‡†åŒ–ç©ºæ ¼
    name = ' '.join(name.split())

    return name

def is_verified_company(normalized_name):
    """æ£€æŸ¥æ˜¯å¦æ˜¯å·²éªŒè¯çš„å…¬å¸ï¼ˆæˆ–å…¶å˜ä½“ï¼‰"""
    for canonical, data in VERIFIED_COMPANIES.items():
        for variant in data['variants']:
            if normalize_name(variant) == normalized_name:
                return canonical
    return None

def extract_all_company_names():
    """ä»æ‰€æœ‰å§”å‘˜ä¼šä¸­æå–å…¬å¸åç§°"""
    print('ğŸ“¥ ä»Firestoreè¯»å–æ‰€æœ‰å§”å‘˜ä¼šè®°å½•...')

    companies = {}  # {normalized_name: {'original': [åŸå§‹å], 'committee_ids': [id]}}
    verified_mapping = {}  # {normalized_name: canonical_name} for verified companies

    try:
        # è¯»å–æ‰€æœ‰å§”å‘˜ä¼š
        committees_ref = db.collection('fec_raw_committees')
        docs = committees_ref.stream()

        count = 0
        for doc in docs:
            count += 1
            if count % 1000 == 0:
                print(f'  å¤„ç†ä¸­: {count} æ¡å§”å‘˜ä¼šè®°å½•...')

            data = doc.to_dict()
            committee_id = data.get('committee_id', '')
            org_name = data.get('connected_org_name', '').strip()

            if not org_name or org_name == '':
                continue

            # æ ‡å‡†åŒ–åç§°
            normalized = normalize_name(org_name)
            if not normalized:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²éªŒè¯å…¬å¸
            verified = is_verified_company(normalized)
            if verified:
                if normalized not in verified_mapping:
                    verified_mapping[normalized] = verified
                continue  # è·³è¿‡å·²éªŒè¯å…¬å¸ï¼Œç¨åå•ç‹¬å¤„ç†

            # æ·»åŠ åˆ°å…¬å¸åˆ—è¡¨
            if normalized not in companies:
                companies[normalized] = {
                    'original': [],
                    'committee_ids': []
                }

            if org_name not in companies[normalized]['original']:
                companies[normalized]['original'].append(org_name)

            if committee_id not in companies[normalized]['committee_ids']:
                companies[normalized]['committee_ids'].append(committee_id)

        print(f'\nâœ… å¤„ç†å®Œæˆ: {count} æ¡å§”å‘˜ä¼šè®°å½•')
        print(f'  å‘ç° {len(companies)} ä¸ªç‹¬ç‰¹æ ‡å‡†åŒ–å…¬å¸åç§°')
        print(f'  å‘ç° {len(verified_mapping)} ä¸ªå·²éªŒè¯å…¬å¸å˜ä½“\n')

        return companies, verified_mapping

    except Exception as e:
        print(f'âŒ è¯»å–å¤±è´¥: {e}')
        sys.exit(1)

def group_similar_companies(companies, similarity_threshold=85):
    """ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…å°†ç›¸ä¼¼çš„å…¬å¸åç§°åˆ†ç»„"""
    print(f'ğŸ” åˆ†ç»„ç›¸ä¼¼å…¬å¸åç§° (ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}%)...')

    company_names = list(companies.keys())
    grouped = {}  # {canonical_name: [variant_names]}
    processed = set()

    for i, name in enumerate(company_names):
        if name in processed:
            continue

        if (i + 1) % 500 == 0:
            print(f'  å¤„ç†ä¸­: {i + 1}/{len(company_names)} å…¬å¸...')

        # æ‰¾å‡ºæ‰€æœ‰ç›¸ä¼¼çš„åç§°
        similar = [name]
        processed.add(name)

        # ä¸å‰©ä½™åç§°æ¯”è¾ƒ
        for other_name in company_names[i+1:]:
            if other_name in processed:
                continue

            # ä½¿ç”¨token_sort_ratioå¤„ç†è¯åºä¸åŒçš„æƒ…å†µ
            score = fuzz.token_sort_ratio(name, other_name)

            if score >= similarity_threshold:
                similar.append(other_name)
                processed.add(other_name)

        # é€‰æ‹©æœ€çŸ­çš„åç§°ä½œä¸ºcanonical name
        canonical = min(similar, key=len)
        grouped[canonical] = similar

    print(f'\nâœ… åˆ†ç»„å®Œæˆ:')
    print(f'  {len(company_names)} ä¸ªåç§° â†’ {len(grouped)} ä¸ªå…¬å¸ç»„')
    print(f'  å¹³å‡æ¯ç»„ {len(company_names)/len(grouped):.1f} ä¸ªå˜ä½“\n')

    return grouped

def build_variant_documents(grouped, companies):
    """æ„å»ºvariantæ–‡æ¡£"""
    print('ğŸ“ æ„å»ºvariantæ–‡æ¡£...')

    variant_docs = []

    for canonical, variants in grouped.items():
        # æ”¶é›†æ‰€æœ‰committee_idså’ŒåŸå§‹åç§°
        all_committee_ids = []
        all_original_names = []

        for variant in variants:
            all_committee_ids.extend(companies[variant]['committee_ids'])
            all_original_names.extend(companies[variant]['original'])

        # å»é‡
        all_committee_ids = list(set(all_committee_ids))
        all_original_names = list(set(all_original_names))

        # é€‰æ‹©æœ€å¸¸è§çš„åŸå§‹åç§°ä½œä¸ºdisplay_name
        display_name = max(all_original_names, key=len) if all_original_names else canonical

        doc = {
            'canonical_name': canonical,
            'display_name': display_name,
            'variants': variants,
            'original_names': all_original_names,
            'committee_ids': all_committee_ids,
            'committee_count': len(all_committee_ids),
            'variant_count': len(variants),
            'created_at': datetime.utcnow(),
            'last_updated': datetime.utcnow(),
            'is_verified': False
        }

        variant_docs.append(doc)

    print(f'âœ… åˆ›å»ºäº† {len(variant_docs)} ä¸ªvariantæ–‡æ¡£\n')
    return variant_docs

def add_verified_companies(variant_docs):
    """æ·»åŠ å·²éªŒè¯çš„å…¬å¸"""
    print('âœ… æ·»åŠ 9ä¸ªå·²éªŒè¯å…¬å¸...')

    # ä¸ºæ¯ä¸ªå·²éªŒè¯å…¬å¸åˆ›å»ºæ–‡æ¡£
    for canonical, data in VERIFIED_COMPANIES.items():
        # æ”¶é›†committee_ids
        committee_ids = []
        for variant in data['variants']:
            normalized = normalize_name(variant)
            # åœ¨Firestoreä¸­æŸ¥æ‰¾åŒ¹é…çš„å§”å‘˜ä¼š
            committees_ref = db.collection('fec_raw_committees')
            query = committees_ref.where('connected_org_nm', '>=', variant.upper()).where('connected_org_nm', '<=', variant.upper() + '\uf8ff')

            for doc in query.stream():
                committee_ids.append(doc.to_dict().get('committee_id'))

        doc = {
            'canonical_name': canonical,
            'display_name': data['display_name'],
            'variants': data['variants'],
            'original_names': data['variants'],
            'committee_ids': list(set(committee_ids)),
            'committee_count': len(set(committee_ids)),
            'variant_count': len(data['variants']),
            'stock_ticker': data.get('stock_ticker', ''),
            'industry': data.get('industry', ''),
            'created_at': datetime.utcnow(),
            'last_updated': datetime.utcnow(),
            'is_verified': True
        }

        variant_docs.append(doc)
        print(f'  âœ“ {canonical}: {len(set(committee_ids))} ä¸ªå§”å‘˜ä¼š')

    print()
    return variant_docs

def upload_variants(variant_docs, batch_size=500):
    """ä¸Šä¼ variantæ–‡æ¡£åˆ°Firestore"""
    print(f'ğŸ“¤ ä¸Šä¼  {len(variant_docs)} ä¸ªvariantæ–‡æ¡£åˆ°Firestore...')

    collection_ref = db.collection('fec_company_name_variants')
    uploaded = 0

    # æŒ‰batchä¸Šä¼ 
    for i in range(0, len(variant_docs), batch_size):
        batch = db.batch()
        batch_docs = variant_docs[i:i+batch_size]

        for doc in batch_docs:
            # ä½¿ç”¨canonical_nameä½œä¸ºdocument ID
            doc_id = doc['canonical_name'].lower().replace(' ', '_')
            doc_ref = collection_ref.document(doc_id)
            batch.set(doc_ref, doc)
            uploaded += 1

        batch.commit()
        print(f'  å·²ä¸Šä¼  {uploaded}/{len(variant_docs)} ä¸ªæ–‡æ¡£...')

    print(f'\nâœ… ä¸Šä¼ å®Œæˆ: {uploaded} ä¸ªæ–‡æ¡£\n')
    return uploaded

def cleanup_empty_variants():
    """æ¸…ç†ç©ºçš„variantè®°å½•"""
    print('='*80)
    print('ğŸ§¹ æ¸…ç†ç©ºçš„variantè®°å½•')
    print('='*80 + '\n')

    docs = list(db.collection('fec_company_name_variants').stream())

    empty_docs = []
    for doc in docs:
        data = doc.to_dict()
        normalized_name = data.get('normalized_name', '')
        variant_name = data.get('variant_name', '')

        if not normalized_name or not variant_name:
            empty_docs.append(doc.id)

    print(f'ğŸ“Š æ€»è®°å½•æ•°: {len(docs)}')
    print(f'ğŸ“Š ç©ºè®°å½•æ•°: {len(empty_docs)}')

    if not empty_docs:
        print('âœ… æ²¡æœ‰ç©ºè®°å½•éœ€è¦æ¸…ç†\n')
        return 0

    print(f'\nğŸ—‘ï¸  åˆ é™¤ {len(empty_docs)} ä¸ªç©ºè®°å½•...')

    # æ‰¹é‡åˆ é™¤
    batch_size = 500
    deleted = 0

    for i in range(0, len(empty_docs), batch_size):
        batch = db.batch()
        batch_docs = empty_docs[i:i+batch_size]

        for doc_id in batch_docs:
            doc_ref = db.collection('fec_company_name_variants').document(doc_id)
            batch.delete(doc_ref)
            deleted += 1

        batch.commit()
        print(f'  å·²åˆ é™¤ {deleted}/{len(empty_docs)}...')

    print(f'\nâœ… æ¸…ç†å®Œæˆ: åˆ é™¤äº† {deleted} ä¸ªç©ºè®°å½•\n')
    return deleted


def rebuild_variants_from_index():
    """ä» fec_company_index é‡å»º variants"""
    print('='*80)
    print('ğŸ”¨ ä» fec_company_index é‡å»º variants')
    print('='*80 + '\n')

    # è·å–ç°æœ‰variants
    print('ğŸ“‚ åŠ è½½ç°æœ‰ variants...')
    existing_docs = list(db.collection('fec_company_name_variants').stream())
    variants_by_normalized = {}

    for doc in existing_docs:
        data = doc.to_dict()
        normalized_name = data.get('normalized_name', '')
        variant_name = data.get('variant_name', '')

        if normalized_name and variant_name:
            if normalized_name not in variants_by_normalized:
                variants_by_normalized[normalized_name] = set()
            variants_by_normalized[normalized_name].add(variant_name)

    print(f'  å·²æœ‰ {len(variants_by_normalized)} ä¸ªå…¬å¸çš„ variants')

    # è·å–æ‰€æœ‰ index ä¸­çš„å…¬å¸
    print('\nğŸ“‚ åŠ è½½ fec_company_index...')
    index_docs = list(db.collection('fec_company_index').stream())
    print(f'  æ‰¾åˆ° {len(index_docs)} ä¸ªå…¬å¸')

    # æ‰¾å‡ºç¼ºå¤±çš„ variants
    missing_variants = []

    for doc in index_docs:
        data = doc.to_dict()
        normalized_name = data.get('normalized_name', '')
        company_name = data.get('company_name', '')  # ä½¿ç”¨company_nameè€Œä¸æ˜¯original_names

        if not normalized_name:
            continue

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰variant
        if normalized_name not in variants_by_normalized:
            # å®Œå…¨ç¼ºå¤±,éœ€è¦æ·»åŠ company_nameä½œä¸ºvariant
            if company_name:
                missing_variants.append({
                    'normalized_name': normalized_name,
                    'variant_name': company_name,
                    'variant_name_lower': company_name.lower(),
                    'source': 'rebuild_from_index'
                })
        else:
            # å·²å­˜åœ¨,æ£€æŸ¥company_nameæ˜¯å¦åœ¨variantsä¸­
            existing = variants_by_normalized[normalized_name]
            if company_name and company_name not in existing:
                missing_variants.append({
                    'normalized_name': normalized_name,
                    'variant_name': company_name,
                    'variant_name_lower': company_name.lower(),
                    'source': 'rebuild_from_index'
                })

    print(f'\nğŸ“Š éœ€è¦æ·»åŠ  {len(missing_variants)} ä¸ªç¼ºå¤±çš„ variant è®°å½•')

    if not missing_variants:
        print('âœ… æ²¡æœ‰ç¼ºå¤±çš„ variants\n')
        return 0

    print(f'\nğŸ“ æ·»åŠ  {len(missing_variants)} ä¸ª variant è®°å½•...')

    # æ‰¹é‡æ·»åŠ 
    batch_size = 500
    added = 0

    for i in range(0, len(missing_variants), batch_size):
        batch = db.batch()
        batch_docs = missing_variants[i:i+batch_size]

        for variant in batch_docs:
            # ç”Ÿæˆdoc_id (é¿å…ç‰¹æ®Šå­—ç¬¦)
            norm_clean = variant['normalized_name'].replace('/', '-').replace('\\', '-')
            var_clean = variant['variant_name'].lower().replace(' ', '_').replace('/', '-').replace('\\', '-')
            doc_id = f"{norm_clean}_{var_clean}"[:1500]  # Firestore doc ID limit

            doc_ref = db.collection('fec_company_name_variants').document(doc_id)
            batch.set(doc_ref, variant)
            added += 1

        batch.commit()
        print(f'  å·²æ·»åŠ  {added}/{len(missing_variants)}...')

    print(f'\nâœ… é‡å»ºå®Œæˆ: æ·»åŠ äº† {added} ä¸ª variant è®°å½•\n')
    return added


def init_gemini_api():
    """åˆå§‹åŒ– Gemini API"""
    global gemini_api_key

    gemini_api_key = os.getenv('GEMINI_API_KEY')
    if not gemini_api_key:
        try:
            import subprocess
            result = subprocess.run(
                ['gcloud', 'secrets', 'versions', 'access', 'latest',
                 '--secret', 'gemini-api-key', '--project', 'gen-lang-client-0960644135'],
                capture_output=True, text=True, check=True
            )
            gemini_api_key = result.stdout.strip()
            print('âœ… Gemini API Key loaded from Secret Manager\n')
        except Exception as e:
            print(f'âŒ Failed to load Gemini API Key: {e}')
            print('   Set environment variable: export GEMINI_API_KEY=...')
            sys.exit(1)
    else:
        print('âœ… Gemini API Key loaded from environment\n')

    return gemini_api_key


def generate_variants_with_ai(company_name, normalized_name):
    """ä½¿ç”¨ Gemini AI ç”Ÿæˆå…¬å¸åç§°å˜ä½“"""
    prompt = f"""Given the company name: "{company_name}"

Generate at least 2 realistic name variations that would appear in FEC political donation records.
Include:
- Official full name
- Common abbreviations
- Name without legal suffixes (Inc., Corp., etc.)

Return ONLY a JSON array of strings, no explanation.
Example: ["Apple Inc.", "Apple Computer Inc.", "Apple"]
"""

    try:
        url = f'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={gemini_api_key}'
        payload = {
            'contents': [{'parts': [{'text': prompt}]}],
            'generationConfig': {'temperature': 0.3, 'maxOutputTokens': 200}
        }

        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()

        result = response.json()
        text = result['candidates'][0]['content']['parts'][0]['text'].strip()

        # Extract JSON array
        import json as json_lib
        match = re.search(r'\[.*\]', text, re.DOTALL)
        if match:
            variants = json_lib.loads(match.group(0))
            return variants if len(variants) >= 2 else [company_name, normalized_name]

        return [company_name, normalized_name]

    except Exception as e:
        # Fallback to simple variants
        return [company_name, normalized_name]


def match_sp500_ticker(normalized_name):
    """åŒ¹é… SP500 ticker (å¦‚æœæœ‰çš„è¯)"""
    normalized_lower = normalized_name.lower()

    for ticker, full_name in SP500_COMPANIES.items():
        norm_sp500 = normalize_name(full_name).lower()

        # Exact match or very close match
        if normalized_lower == norm_sp500 or normalized_lower in norm_sp500:
            return ticker

    return None


# æ‰‹åŠ¨æ˜ å°„: normalized_name -> (ticker, full_name)
# è¿™ä¸ªæ˜ å°„ç”¨äºç›´æ¥ä¸º SP500 å…¬å¸æ·»åŠ  ticker å­—æ®µ
MANUAL_TICKER_MAPPING = {
    'apple': ('AAPL', 'Apple Inc.'),
    'microsoft': ('MSFT', 'Microsoft Corporation'),
    'alphabet': ('GOOGL', 'Alphabet Inc.'),
    'google': ('GOOGL', 'Alphabet Inc.'),
    'amazon': ('AMZN', 'Amazon.com Inc.'),
    'nvidia': ('NVDA', 'NVIDIA Corporation'),
    'meta platforms': ('META', 'Meta Platforms Inc.'),
    'facebook': ('META', 'Meta Platforms Inc.'),
    'tesla': ('TSLA', 'Tesla Inc.'),
    'broadcom': ('AVGO', 'Broadcom Inc.'),
    'oracle': ('ORCL', 'Oracle Corporation'),
    'salesforce': ('CRM', 'Salesforce Inc.'),
    'advanced micro devices': ('AMD', 'Advanced Micro Devices Inc.'),
    'intel': ('INTC', 'Intel Corporation'),
    'international business machines': ('IBM', 'International Business Machines Corporation'),
    'ibm': ('IBM', 'International Business Machines Corporation'),
    'cisco systems': ('CSCO', 'Cisco Systems Inc.'),
    'cisco': ('CSCO', 'Cisco Systems Inc.'),
    'adobe': ('ADBE', 'Adobe Inc.'),
    'berkshire hathaway': ('BRK.B', 'Berkshire Hathaway Inc.'),
    'jpmorgan chase': ('JPM', 'JPMorgan Chase & Co.'),
    'jp morgan chase': ('JPM', 'JPMorgan Chase & Co.'),
    'visa': ('V', 'Visa Inc.'),
    'mastercard': ('MA', 'Mastercard Incorporated'),
    'bank of america': ('BAC', 'Bank of America Corporation'),
    'wells fargo': ('WFC', 'Wells Fargo & Company'),
    'goldman sachs': ('GS', 'Goldman Sachs Group Inc.'),
    'morgan stanley': ('MS', 'Morgan Stanley'),
    'blackrock': ('BLK', 'BlackRock Inc.'),
    'citigroup': ('C', 'Citigroup Inc.'),
    'unitedhealth': ('UNH', 'UnitedHealth Group Incorporated'),
    'johnson': ('JNJ', 'Johnson & Johnson'),
    'eli lilly': ('LLY', 'Eli Lilly and Company'),
    'pfizer': ('PFE', 'Pfizer Inc.'),
    'merck': ('MRK', 'Merck & Co. Inc.'),
    'abbvie': ('ABBV', 'AbbVie Inc.'),
    'thermo fisher scientific': ('TMO', 'Thermo Fisher Scientific Inc.'),
    'abbott laboratories': ('ABT', 'Abbott Laboratories'),
    'abbott': ('ABT', 'Abbott Laboratories'),
    'cvs health': ('CVS', 'CVS Health Corporation'),
    'bristol-myers squibb': ('BMY', 'Bristol-Myers Squibb Company'),
    'walmart': ('WMT', 'Walmart Inc.'),
    'procter': ('PG', 'The Procter & Gamble Company'),
    'coca-cola': ('KO', 'The Coca-Cola Company'),
    'pepsico': ('PEP', 'PepsiCo Inc.'),
    'costco wholesale': ('COST', 'Costco Wholesale Corporation'),
    'costco': ('COST', 'Costco Wholesale Corporation'),
    'home depot': ('HD', 'The Home Depot Inc.'),
    'mcdonald': ('MCD', 'McDonald\'s Corporation'),
    'nike': ('NKE', 'Nike Inc.'),
    'starbucks': ('SBUX', 'Starbucks Corporation'),
    'target': ('TGT', 'Target Corporation'),
    'lowe': ('LOW', 'Lowe\'s Companies Inc.'),
    'walt disney': ('DIS', 'The Walt Disney Company'),
    'disney': ('DIS', 'The Walt Disney Company'),
    'exxon mobil': ('XOM', 'Exxon Mobil Corporation'),
    'chevron': ('CVX', 'Chevron Corporation'),
    'conocophillips': ('COP', 'ConocoPhillips'),
    'schlumberger': ('SLB', 'Schlumberger Limited'),
    'eog resources': ('EOG', 'EOG Resources Inc.'),
    'occidental petroleum': ('OXY', 'Occidental Petroleum Corporation'),
    'phillips 66': ('PSX', 'Phillips 66'),
    'valero energy': ('VLO', 'Valero Energy Corporation'),
    'valero': ('VLO', 'Valero Energy Corporation'),
    'general electric': ('GE', 'General Electric Company'),
    'caterpillar': ('CAT', 'Caterpillar Inc.'),
    'raytheon technologies': ('RTX', 'Raytheon Technologies Corporation'),
    'raytheon': ('RTX', 'Raytheon Technologies Corporation'),
    'honeywell international': ('HON', 'Honeywell International Inc.'),
    'honeywell': ('HON', 'Honeywell International Inc.'),
    'united parcel service': ('UPS', 'United Parcel Service Inc.'),
    'ups': ('UPS', 'United Parcel Service Inc.'),
    'boeing': ('BA', 'The Boeing Company'),
    'lockheed martin': ('LMT', 'Lockheed Martin Corporation'),
    'deere': ('DE', 'Deere & Company'),
    'john deere': ('DE', 'Deere & Company'),
    'northrop grumman': ('NOC', 'Northrop Grumman Corporation'),
    'general dynamics': ('GD', 'General Dynamics Corporation'),
    'netflix': ('NFLX', 'Netflix Inc.'),
    'comcast': ('CMCSA', 'Comcast Corporation'),
    'att': ('T', 'AT&T Inc.'),
    'verizon communications': ('VZ', 'Verizon Communications Inc.'),
    'verizon': ('VZ', 'Verizon Communications Inc.'),
    't-mobile': ('TMUS', 'T-Mobile US Inc.'),
    'nextera energy': ('NEE', 'NextEra Energy Inc.'),
    'nextera': ('NEE', 'NextEra Energy Inc.'),
    'duke energy': ('DUK', 'Duke Energy Corporation'),
    'southern': ('SO', 'The Southern Company'),
    'dominion energy': ('D', 'Dominion Energy Inc.'),
    'dominion': ('D', 'Dominion Energy Inc.'),
    'american electric power': ('AEP', 'American Electric Power Company Inc.'),
    'linde': ('LIN', 'Linde plc'),
    'air products': ('APD', 'Air Products and Chemicals Inc.'),
    'sherwin-williams': ('SHW', 'The Sherwin-Williams Company'),
    'freeport-mcmoran': ('FCX', 'Freeport-McMoRan Inc.'),
    'newmont': ('NEM', 'Newmont Corporation'),
    'prologis': ('PLD', 'Prologis Inc.'),
    'american tower': ('AMT', 'American Tower Corporation'),
    'crown castle': ('CCI', 'Crown Castle Inc.'),
    'equinix': ('EQIX', 'Equinix Inc.'),
    'simon property': ('SPG', 'Simon Property Group Inc.'),
}


def add_sp500_tickers_manually():
    """æ‰‹åŠ¨ä¸º SP500 å…¬å¸æ·»åŠ  ticker å­—æ®µ"""
    print('='*80)
    print('ğŸ·ï¸  æ‰‹åŠ¨ä¸º SP500 å…¬å¸æ·»åŠ  ticker å­—æ®µ')
    print('='*80 + '\n')

    print(f'ğŸ“‚ åŠ è½½ç°æœ‰ fec_company_name_variants...')
    all_docs = list(db.collection('fec_company_name_variants').stream())
    print(f'   æ‰¾åˆ° {len(all_docs)} ä¸ªæ–‡æ¡£\n')

    updated_count = 0
    already_has_ticker = 0
    not_found = 0

    print('ğŸ” åŒ¹é…å¹¶æ›´æ–° SP500 å…¬å¸...\n')

    for doc in all_docs:
        data = doc.to_dict()
        normalized_name = data.get('normalized_name', '').lower()

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ ticker
        if data.get('ticker'):
            already_has_ticker += 1
            continue

        # åœ¨æ‰‹åŠ¨æ˜ å°„ä¸­æŸ¥æ‰¾
        if normalized_name in MANUAL_TICKER_MAPPING:
            ticker, full_name = MANUAL_TICKER_MAPPING[normalized_name]

            # æ›´æ–°æ–‡æ¡£
            doc.reference.update({
                'ticker': ticker,
                'company_full_name': full_name,
                'last_updated': datetime.utcnow()
            })

            updated_count += 1
            print(f'  âœ… [{ticker:6s}] {normalized_name:40s} â†’ {full_name}')
        else:
            not_found += 1

    print(f'\n{"="*80}')
    print('ğŸ“Š æ›´æ–°ç»“æœ')
    print('='*80)
    print(f'  æ›´æ–°äº† ticker: {updated_count} ä¸ªå…¬å¸')
    print(f'  å·²æœ‰ ticker: {already_has_ticker} ä¸ªå…¬å¸')
    print(f'  æœªæ‰¾åˆ°åŒ¹é…: {not_found} ä¸ªå…¬å¸')
    print(f'  æ€»æ–‡æ¡£æ•°: {len(all_docs)}')
    print()

    return updated_count


def rebuild_with_ai():
    """ä½¿ç”¨ AI ä» fec_company_index é‡å»º variants"""
    print('='*80)
    print('ğŸ¤– ä½¿ç”¨ AI ä» fec_company_index é‡å»º variants')
    print('='*80 + '\n')

    # Initialize Gemini API
    init_gemini_api()

    # Step 1: æ¸…ç†ç°æœ‰ collection
    print('ğŸ—‘ï¸  Step 1: æ¸…ç†ç°æœ‰ fec_company_name_variants...')
    try:
        # Delete in batches
        batch_size = 500
        while True:
            docs = db.collection('fec_company_name_variants').limit(batch_size).stream()
            deleted = 0

            batch = db.batch()
            for doc in docs:
                batch.delete(doc.reference)
                deleted += 1

            if deleted == 0:
                break

            batch.commit()
            print(f'  Deleted {deleted} documents...')

        print('âœ… Collection cleared\n')
    except Exception as e:
        print(f'âŒ Failed to clear collection: {e}')
        sys.exit(1)

    # Step 2: ä» index è·å–æ‰€æœ‰å…¬å¸
    print('ğŸ“‚ Step 2: Loading companies from fec_company_index...')
    index_docs = list(db.collection('fec_company_index').stream())
    print(f'âœ… Found {len(index_docs)} companies\n')

    # Step 3: ä¸ºæ¯ä¸ªå…¬å¸ç”Ÿæˆ variants
    print(f'ğŸ¤– Step 3: Generating variants with AI for {len(index_docs)} companies...')
    print('   (This may take a while...)\n')

    created_count = 0
    failed_count = 0
    progress_interval = 10

    for i, doc in enumerate(index_docs, 1):
        data = doc.to_dict()
        normalized_name = data.get('normalized_name', '')
        company_name = data.get('company_name', normalized_name)

        if not normalized_name:
            continue

        # Progress update
        if i % progress_interval == 0 or i == len(index_docs):
            print(f'  [{i}/{len(index_docs)}] Processing: {company_name}...')

        # Match SP500 ticker
        ticker = match_sp500_ticker(normalized_name)

        # Generate variants with AI
        try:
            variants = generate_variants_with_ai(company_name, normalized_name)

            # Create a SINGLE document for the company with ALL variants
            doc_id = normalized_name[:1500]  # Use normalized_name as doc ID

            # Prepare variant objects array
            variant_objects = []
            for variant in variants:
                variant_lower = variant.lower()
                variant_objects.append({
                    'variant_name': variant,
                    'variant_name_lower': variant_lower,
                    'source': 'ai_generated'
                })

            doc_data = {
                'normalized_name': normalized_name,
                'company_name': company_name,
                'variants': variant_objects,
                'created_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            # Add ticker if matched
            if ticker:
                doc_data['ticker'] = ticker
                doc_data['company_full_name'] = SP500_COMPANIES[ticker]

            db.collection('fec_company_name_variants').document(doc_id).set(doc_data)
            created_count += 1

        except Exception as e:
            failed_count += 1
            if failed_count <= 10:  # Only print first 10 failures
                print(f'    âš ï¸  Failed: {company_name} - {e}')

    print(f'\nâœ… Rebuild complete!')
    print(f'   Created: {created_count} variants')
    print(f'   Failed: {failed_count} companies')
    print()

    return created_count


def verify_data_consistency():
    """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
    print('='*80)
    print('âœ… éªŒè¯æ•°æ®ä¸€è‡´æ€§')
    print('='*80 + '\n')

    # è·å–indexä¸­çš„æ‰€æœ‰normalized_names
    index_docs = list(db.collection('fec_company_index').stream())
    index_names = {doc.to_dict().get('normalized_name', '') for doc in index_docs if doc.to_dict().get('normalized_name')}

    # è·å–variantsä¸­çš„æ‰€æœ‰normalized_names
    variant_docs = list(db.collection('fec_company_name_variants').stream())
    variant_names = {doc.to_dict().get('normalized_name', '') for doc in variant_docs if doc.to_dict().get('normalized_name')}

    print(f'ğŸ“Š Index å…¬å¸æ•°: {len(index_names)}')
    print(f'ğŸ“Š Variant å…¬å¸æ•°: {len(variant_names)}')

    # æ£€æŸ¥ç¼ºå¤±
    missing_in_variants = index_names - variant_names
    only_in_variants = variant_names - index_names

    # æ£€æŸ¥ç©ºè®°å½•
    empty_count = sum(1 for doc in variant_docs if not doc.to_dict().get('normalized_name') or not doc.to_dict().get('variant_name'))

    print()
    if missing_in_variants:
        print(f'âš ï¸  {len(missing_in_variants)} ä¸ªå…¬å¸åœ¨ index ä¸­ä½†ä¸åœ¨ variants ä¸­')
    else:
        print(f'âœ… æ‰€æœ‰ index å…¬å¸éƒ½æœ‰å¯¹åº”çš„ variants')

    if only_in_variants:
        print(f'âš ï¸  {len(only_in_variants)} ä¸ªå…¬å¸åœ¨ variants ä¸­ä½†ä¸åœ¨ index ä¸­')
    else:
        print(f'âœ… æ‰€æœ‰ variant å…¬å¸éƒ½åœ¨ index ä¸­')

    if empty_count > 0:
        print(f'âš ï¸  ä»æœ‰ {empty_count} ä¸ªç©ºè®°å½•')
    else:
        print(f'âœ… æ²¡æœ‰ç©ºè®°å½•')

    print()

    return len(missing_in_variants) == 0 and empty_count == 0


def save_report(variant_docs):
    """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
    report_file = REPORTS_DIR / '12-company-variants-report.json'

    print(f'ğŸ’¾ ä¿å­˜æŠ¥å‘Šåˆ° {report_file}...')

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_companies': len(variant_docs),
        'verified_companies': sum(1 for d in variant_docs if d.get('is_verified')),
        'auto_detected_companies': sum(1 for d in variant_docs if not d.get('is_verified')),
        'total_committees': sum(d['committee_count'] for d in variant_docs),
        'avg_variants_per_company': sum(d['variant_count'] for d in variant_docs) / len(variant_docs),
        'generated_at': datetime.utcnow().isoformat()
    }

    # Top 20 companies by committee count
    top_20 = sorted(variant_docs, key=lambda x: x['committee_count'], reverse=True)[:20]

    report = {
        'statistics': stats,
        'top_20_companies': [
            {
                'canonical_name': d['canonical_name'],
                'display_name': d['display_name'],
                'committee_count': d['committee_count'],
                'variant_count': d['variant_count'],
                'is_verified': d.get('is_verified', False)
            }
            for d in top_20
        ]
    }

    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)

    print(f'âœ… æŠ¥å‘Šå·²ä¿å­˜\n')

    # æ‰“å°ç»Ÿè®¡
    print('ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:')
    print(f'  æ€»å…¬å¸æ•°: {stats["total_companies"]}')
    print(f'  å·²éªŒè¯å…¬å¸: {stats["verified_companies"]}')
    print(f'  è‡ªåŠ¨æ£€æµ‹å…¬å¸: {stats["auto_detected_companies"]}')
    print(f'  æ€»å§”å‘˜ä¼šæ•°: {stats["total_committees"]}')
    print(f'  å¹³å‡æ¯å…¬å¸å˜ä½“æ•°: {stats["avg_variants_per_company"]:.1f}')
    print()

    print('ğŸ† Top 10 å…¬å¸ (æŒ‰å§”å‘˜ä¼šæ•°):')
    for i, company in enumerate(top_20[:10], 1):
        verified = 'âœ“' if company['is_verified'] else ' '
        print(f'  {i:2d}. [{verified}] {company["canonical_name"]:30s} - {company["committee_count"]:3d} ä¸ªå§”å‘˜ä¼š, {company["variant_count"]} ä¸ªå˜ä½“')
    print()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ„å»ºå’Œç®¡ç†å…¬å¸åç§°å˜ä½“æ˜ å°„è¡¨')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†ç©ºçš„variantè®°å½•')
    parser.add_argument('--rebuild-from-index', action='store_true', help='ä»fec_company_indexé‡å»ºç¼ºå¤±çš„variants')
    parser.add_argument('--rebuild-with-ai', action='store_true', help='ä½¿ç”¨AIé‡å»ºæ‰€æœ‰variants (æ¸…ç©ºcollectionå¹¶é‡æ–°ç”Ÿæˆ)')
    parser.add_argument('--verify', action='store_true', help='éªŒè¯æ•°æ®ä¸€è‡´æ€§')
    parser.add_argument('--add-tickers', action='store_true', help='æ‰‹åŠ¨ä¸ºSP500å…¬å¸æ·»åŠ tickerå­—æ®µ')
    args = parser.parse_args()

    print('\n' + '='*80)
    print('ğŸ¢ å…¬å¸åç§°å˜ä½“æ˜ å°„è¡¨ç®¡ç†')
    print('='*80 + '\n')

    # åˆå§‹åŒ–Firestore
    init_firestore()

    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ“ä½œï¼Œæ‰§è¡Œå¯¹åº”æ“ä½œ
    if args.cleanup:
        deleted = cleanup_empty_variants()
        print(f'\nâœ… æ¸…ç†å®Œæˆ: åˆ é™¤äº† {deleted} ä¸ªç©ºè®°å½•')
        return

    if args.add_tickers:
        updated = add_sp500_tickers_manually()
        print(f'\nâœ… Tickeræ·»åŠ å®Œæˆ: æ›´æ–°äº† {updated} ä¸ªå…¬å¸')
        return

    if args.rebuild_with_ai:
        # åˆå§‹åŒ– Gemini API
        init_gemini_api()

        print('='*80)
        print('ğŸ¤– ä½¿ç”¨ AI é‡å»ºæ‰€æœ‰ variants')
        print('='*80 + '\n')
        print('âš ï¸  è¿™å°†æ¸…ç©ºç°æœ‰çš„ fec_company_name_variants collection')
        print('âš ï¸  å¹¶ä½¿ç”¨ Gemini AI ä¸ºæ‰€æœ‰å…¬å¸ç”Ÿæˆæ–°çš„ name variants')
        print()

        # æ‰§è¡Œé‡å»º
        rebuild_with_ai()

        # é‡å»ºåè‡ªåŠ¨éªŒè¯
        print('\n')
        verify_data_consistency()

        print('\nâœ… AI é‡å»ºå®Œæˆï¼')
        return

    if args.rebuild_from_index:
        added = rebuild_variants_from_index()
        print(f'\nâœ… é‡å»ºå®Œæˆ: æ·»åŠ äº† {added} ä¸ªvariantè®°å½•')

        # é‡å»ºåè‡ªåŠ¨éªŒè¯
        print('\n')
        verify_data_consistency()
        return

    if args.verify:
        is_consistent = verify_data_consistency()
        if is_consistent:
            print('âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡')
        else:
            print('âš ï¸  æ•°æ®å­˜åœ¨ä¸ä¸€è‡´ï¼Œå»ºè®®è¿è¡Œ --cleanup å’Œ --rebuild-from-index')
        return

    # é»˜è®¤è¡Œä¸º: æ„å»ºvariants (åŸæœ‰é€»è¾‘)
    print('='*80)
    print('ğŸ¢ è‡ªåŠ¨æ„å»ºå…¬å¸åç§°å˜ä½“æ˜ å°„è¡¨')
    print('='*80 + '\n')

    # æ­¥éª¤1: æå–æ‰€æœ‰å…¬å¸åç§°
    print('='*80)
    print('æ­¥éª¤ 1/5: æå–å…¬å¸åç§°')
    print('='*80 + '\n')
    companies, verified_mapping = extract_all_company_names()

    # æ­¥éª¤2: åˆ†ç»„ç›¸ä¼¼å…¬å¸
    print('='*80)
    print('æ­¥éª¤ 2/5: åˆ†ç»„ç›¸ä¼¼å…¬å¸')
    print('='*80 + '\n')
    grouped = group_similar_companies(companies, similarity_threshold=85)

    # æ­¥éª¤3: æ„å»ºvariantæ–‡æ¡£
    print('='*80)
    print('æ­¥éª¤ 3/5: æ„å»ºvariantæ–‡æ¡£')
    print('='*80 + '\n')
    variant_docs = build_variant_documents(grouped, companies)

    # æ­¥éª¤4: æ·»åŠ å·²éªŒè¯å…¬å¸
    print('='*80)
    print('æ­¥éª¤ 4/5: æ·»åŠ å·²éªŒè¯å…¬å¸')
    print('='*80 + '\n')
    variant_docs = add_verified_companies(variant_docs)

    # æ­¥éª¤5: ä¸Šä¼ åˆ°Firestore
    print('='*80)
    print('æ­¥éª¤ 5/5: ä¸Šä¼ åˆ°Firestore')
    print('='*80 + '\n')

    print(f'å‡†å¤‡ä¸Šä¼  {len(variant_docs)} ä¸ªvariantæ–‡æ¡£åˆ°Firestore...\n')
    uploaded = upload_variants(variant_docs)

    # ä¿å­˜æŠ¥å‘Š
    save_report(variant_docs)

    print('='*80)
    print('âœ… å®Œæˆï¼')
    print('='*80)
    print(f'\nä¸‹ä¸€æ­¥:')
    print(f'  1. åœ¨Firebase ConsoleéªŒè¯ fec_company_name_variants collection')
    print(f'  2. æŸ¥çœ‹æŠ¥å‘Š: {REPORTS_DIR / "12-company-variants-report.json"}')
    print(f'  3. å¼€å§‹ä¸Šä¼ contributions: python3 02-upload-incremental.py')
    print()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­')
        sys.exit(0)
    except Exception as e:
        print(f'\nâŒ é”™è¯¯: {e}')
        import traceback
        traceback.print_exc()
        sys.exit(1)
