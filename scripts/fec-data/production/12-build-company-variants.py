#!/usr/bin/env python3
"""
è‡ªåŠ¨æ„å»ºå…¬å¸åç§°å˜ä½“æ˜ å°„è¡¨
ä»æ‰€æœ‰20,934ä¸ªå§”å‘˜ä¼šè®°å½•ä¸­æå–å…¬å¸åç§°ï¼Œä½¿ç”¨æ¨¡ç³ŠåŒ¹é…è‡ªåŠ¨åˆ†ç»„å˜ä½“
"""

import sys
import os
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import json
import re

try:
    import firebase_admin
    from firebase_admin import credentials, firestore
    from rapidfuzz import fuzz, process
except ImportError as e:
    print(f'âŒ ç¼ºå°‘ä¾èµ–åº“: {e}')
    print('å®‰è£…: pip install firebase-admin rapidfuzz')
    sys.exit(1)

PROJECT_ID = 'stanseproject'
REPORTS_DIR = Path(__file__).parent.parent / 'reports'
PROGRESS_FILE = REPORTS_DIR / '12-variant-building-progress.json'

db = None

# æ‰‹åŠ¨è¦†ç›–çš„9å®¶å·²éªŒè¯å…¬å¸
VERIFIED_COMPANIES = {
    'GOOGLE': {
        'canonical_name': 'GOOGLE',
        'display_name': 'Google Inc.',
        'variants': ['GOOGLE INC', 'GOOGLE LLC', 'ALPHABET INC', 'ALPHABET', 'GOOGLE'],
        'stock_ticker': 'GOOGL',
        'industry': 'Technology'
    },
    'MICROSOFT': {
        'canonical_name': 'MICROSOFT',
        'display_name': 'Microsoft Corporation',
        'variants': ['MICROSOFT CORP', 'MICROSOFT CORPORATION', 'MICROSOFT'],
        'stock_ticker': 'MSFT',
        'industry': 'Technology'
    },
    'AMAZON': {
        'canonical_name': 'AMAZON',
        'display_name': 'Amazon.com Inc.',
        'variants': ['AMAZON.COM INC', 'AMAZON COM INC', 'AMAZON', 'AMAZON INC'],
        'stock_ticker': 'AMZN',
        'industry': 'Technology'
    },
    'APPLE': {
        'canonical_name': 'APPLE',
        'display_name': 'Apple Inc.',
        'variants': ['APPLE INC', 'APPLE COMPUTER INC', 'APPLE'],
        'stock_ticker': 'AAPL',
        'industry': 'Technology'
    },
    'META': {
        'canonical_name': 'META',
        'display_name': 'Meta Platforms Inc.',
        'variants': ['META PLATFORMS INC', 'FACEBOOK INC', 'FACEBOOK', 'META'],
        'stock_ticker': 'META',
        'industry': 'Technology'
    },
    'JPMORGAN': {
        'canonical_name': 'JPMORGAN',
        'display_name': 'JPMorgan Chase & Co.',
        'variants': ['JPMORGAN CHASE & CO', 'JP MORGAN CHASE', 'JPMORGAN', 'JPMORGAN CHASE'],
        'stock_ticker': 'JPM',
        'industry': 'Financial Services'
    },
    'GOLDMAN SACHS': {
        'canonical_name': 'GOLDMAN SACHS',
        'display_name': 'Goldman Sachs Group Inc.',
        'variants': ['GOLDMAN SACHS GROUP INC', 'GOLDMAN SACHS', 'GOLDMAN SACHS & CO'],
        'stock_ticker': 'GS',
        'industry': 'Financial Services'
    },
    'BOEING': {
        'canonical_name': 'BOEING',
        'display_name': 'The Boeing Company',
        'variants': ['BOEING CO', 'BOEING COMPANY', 'BOEING', 'THE BOEING COMPANY'],
        'stock_ticker': 'BA',
        'industry': 'Aerospace & Defense'
    },
    'LOCKHEED MARTIN': {
        'canonical_name': 'LOCKHEED MARTIN',
        'display_name': 'Lockheed Martin Corporation',
        'variants': ['LOCKHEED MARTIN CORP', 'LOCKHEED MARTIN CORPORATION', 'LOCKHEED MARTIN'],
        'stock_ticker': 'LMT',
        'industry': 'Aerospace & Defense'
    }
}

def init_firestore():
    """åˆå§‹åŒ–Firestore - ä½¿ç”¨ADC"""
    global db
    print('ğŸ”§ åˆå§‹åŒ–Firestoreè¿æ¥...')

    try:
        if not firebase_admin._apps:
            print('  â„¹ï¸  ä½¿ç”¨é»˜è®¤å‡­è¯é“¾ï¼ˆgcloud/ç¯å¢ƒå˜é‡ï¼‰...')
            firebase_admin.initialize_app(options={'projectId': PROJECT_ID})

        db = firestore.client()
        print(f'âœ… Firestoreå·²è¿æ¥ (é¡¹ç›®: {PROJECT_ID})\n')
        return db
    except Exception as e:
        print(f'âŒ å¤±è´¥: {e}')
        sys.exit(1)

def normalize_name(name):
    """æ ‡å‡†åŒ–å…¬å¸åç§°"""
    if not name:
        return ''

    # è½¬å¤§å†™
    name = name.upper().strip()

    # ç§»é™¤å¸¸è§åç¼€
    suffixes = [
        ' INC', ' CORP', ' LLC', ' LLP', ' LP', ' LTD', ' CO',
        ' CORPORATION', ' INCORPORATED', ' COMPANY', ' LIMITED',
        ' & CO', ' AND CO', ',', '.'
    ]

    for suffix in suffixes:
        if name.endswith(suffix):
            name = name[:-len(suffix)].strip()

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    name = re.sub(r'[^\w\s&-]', '', name)

    # æ ‡å‡†åŒ–ç©ºæ ¼
    name = ' '.join(name.split())

    return name

def is_verified_company(normalized_name):
    """æ£€æŸ¥æ˜¯å¦æ˜¯å·²éªŒè¯çš„å…¬å¸ï¼ˆæˆ–å…¶å˜ä½“ï¼‰"""
    for canonical, data in VERIFIED_COMPANIES.items():
        for variant in data['variants']:
            if normalize_name(variant) == normalized_name:
                return canonical
    return None

def extract_all_company_names():
    """ä»æ‰€æœ‰å§”å‘˜ä¼šä¸­æå–å…¬å¸åç§°"""
    print('ğŸ“¥ ä»Firestoreè¯»å–æ‰€æœ‰å§”å‘˜ä¼šè®°å½•...')

    companies = {}  # {normalized_name: {'original': [åŸå§‹å], 'committee_ids': [id]}}
    verified_mapping = {}  # {normalized_name: canonical_name} for verified companies

    try:
        # è¯»å–æ‰€æœ‰å§”å‘˜ä¼š
        committees_ref = db.collection('fec_raw_committees')
        docs = committees_ref.stream()

        count = 0
        for doc in docs:
            count += 1
            if count % 1000 == 0:
                print(f'  å¤„ç†ä¸­: {count} æ¡å§”å‘˜ä¼šè®°å½•...')

            data = doc.to_dict()
            committee_id = data.get('committee_id', '')
            org_name = data.get('connected_org_name', '').strip()

            if not org_name or org_name == '':
                continue

            # æ ‡å‡†åŒ–åç§°
            normalized = normalize_name(org_name)
            if not normalized:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²éªŒè¯å…¬å¸
            verified = is_verified_company(normalized)
            if verified:
                if normalized not in verified_mapping:
                    verified_mapping[normalized] = verified
                continue  # è·³è¿‡å·²éªŒè¯å…¬å¸ï¼Œç¨åå•ç‹¬å¤„ç†

            # æ·»åŠ åˆ°å…¬å¸åˆ—è¡¨
            if normalized not in companies:
                companies[normalized] = {
                    'original': [],
                    'committee_ids': []
                }

            if org_name not in companies[normalized]['original']:
                companies[normalized]['original'].append(org_name)

            if committee_id not in companies[normalized]['committee_ids']:
                companies[normalized]['committee_ids'].append(committee_id)

        print(f'\nâœ… å¤„ç†å®Œæˆ: {count} æ¡å§”å‘˜ä¼šè®°å½•')
        print(f'  å‘ç° {len(companies)} ä¸ªç‹¬ç‰¹æ ‡å‡†åŒ–å…¬å¸åç§°')
        print(f'  å‘ç° {len(verified_mapping)} ä¸ªå·²éªŒè¯å…¬å¸å˜ä½“\n')

        return companies, verified_mapping

    except Exception as e:
        print(f'âŒ è¯»å–å¤±è´¥: {e}')
        sys.exit(1)

def group_similar_companies(companies, similarity_threshold=85):
    """ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…å°†ç›¸ä¼¼çš„å…¬å¸åç§°åˆ†ç»„"""
    print(f'ğŸ” åˆ†ç»„ç›¸ä¼¼å…¬å¸åç§° (ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}%)...')

    company_names = list(companies.keys())
    grouped = {}  # {canonical_name: [variant_names]}
    processed = set()

    for i, name in enumerate(company_names):
        if name in processed:
            continue

        if (i + 1) % 500 == 0:
            print(f'  å¤„ç†ä¸­: {i + 1}/{len(company_names)} å…¬å¸...')

        # æ‰¾å‡ºæ‰€æœ‰ç›¸ä¼¼çš„åç§°
        similar = [name]
        processed.add(name)

        # ä¸å‰©ä½™åç§°æ¯”è¾ƒ
        for other_name in company_names[i+1:]:
            if other_name in processed:
                continue

            # ä½¿ç”¨token_sort_ratioå¤„ç†è¯åºä¸åŒçš„æƒ…å†µ
            score = fuzz.token_sort_ratio(name, other_name)

            if score >= similarity_threshold:
                similar.append(other_name)
                processed.add(other_name)

        # é€‰æ‹©æœ€çŸ­çš„åç§°ä½œä¸ºcanonical name
        canonical = min(similar, key=len)
        grouped[canonical] = similar

    print(f'\nâœ… åˆ†ç»„å®Œæˆ:')
    print(f'  {len(company_names)} ä¸ªåç§° â†’ {len(grouped)} ä¸ªå…¬å¸ç»„')
    print(f'  å¹³å‡æ¯ç»„ {len(company_names)/len(grouped):.1f} ä¸ªå˜ä½“\n')

    return grouped

def build_variant_documents(grouped, companies):
    """æ„å»ºvariantæ–‡æ¡£"""
    print('ğŸ“ æ„å»ºvariantæ–‡æ¡£...')

    variant_docs = []

    for canonical, variants in grouped.items():
        # æ”¶é›†æ‰€æœ‰committee_idså’ŒåŸå§‹åç§°
        all_committee_ids = []
        all_original_names = []

        for variant in variants:
            all_committee_ids.extend(companies[variant]['committee_ids'])
            all_original_names.extend(companies[variant]['original'])

        # å»é‡
        all_committee_ids = list(set(all_committee_ids))
        all_original_names = list(set(all_original_names))

        # é€‰æ‹©æœ€å¸¸è§çš„åŸå§‹åç§°ä½œä¸ºdisplay_name
        display_name = max(all_original_names, key=len) if all_original_names else canonical

        doc = {
            'canonical_name': canonical,
            'display_name': display_name,
            'variants': variants,
            'original_names': all_original_names,
            'committee_ids': all_committee_ids,
            'committee_count': len(all_committee_ids),
            'variant_count': len(variants),
            'created_at': datetime.utcnow(),
            'last_updated': datetime.utcnow(),
            'is_verified': False
        }

        variant_docs.append(doc)

    print(f'âœ… åˆ›å»ºäº† {len(variant_docs)} ä¸ªvariantæ–‡æ¡£\n')
    return variant_docs

def add_verified_companies(variant_docs):
    """æ·»åŠ å·²éªŒè¯çš„å…¬å¸"""
    print('âœ… æ·»åŠ 9ä¸ªå·²éªŒè¯å…¬å¸...')

    # ä¸ºæ¯ä¸ªå·²éªŒè¯å…¬å¸åˆ›å»ºæ–‡æ¡£
    for canonical, data in VERIFIED_COMPANIES.items():
        # æ”¶é›†committee_ids
        committee_ids = []
        for variant in data['variants']:
            normalized = normalize_name(variant)
            # åœ¨Firestoreä¸­æŸ¥æ‰¾åŒ¹é…çš„å§”å‘˜ä¼š
            committees_ref = db.collection('fec_raw_committees')
            query = committees_ref.where('connected_org_nm', '>=', variant.upper()).where('connected_org_nm', '<=', variant.upper() + '\uf8ff')

            for doc in query.stream():
                committee_ids.append(doc.to_dict().get('committee_id'))

        doc = {
            'canonical_name': canonical,
            'display_name': data['display_name'],
            'variants': data['variants'],
            'original_names': data['variants'],
            'committee_ids': list(set(committee_ids)),
            'committee_count': len(set(committee_ids)),
            'variant_count': len(data['variants']),
            'stock_ticker': data.get('stock_ticker', ''),
            'industry': data.get('industry', ''),
            'created_at': datetime.utcnow(),
            'last_updated': datetime.utcnow(),
            'is_verified': True
        }

        variant_docs.append(doc)
        print(f'  âœ“ {canonical}: {len(set(committee_ids))} ä¸ªå§”å‘˜ä¼š')

    print()
    return variant_docs

def upload_variants(variant_docs, batch_size=500):
    """ä¸Šä¼ variantæ–‡æ¡£åˆ°Firestore"""
    print(f'ğŸ“¤ ä¸Šä¼  {len(variant_docs)} ä¸ªvariantæ–‡æ¡£åˆ°Firestore...')

    collection_ref = db.collection('fec_company_name_variants')
    uploaded = 0

    # æŒ‰batchä¸Šä¼ 
    for i in range(0, len(variant_docs), batch_size):
        batch = db.batch()
        batch_docs = variant_docs[i:i+batch_size]

        for doc in batch_docs:
            # ä½¿ç”¨canonical_nameä½œä¸ºdocument ID
            doc_id = doc['canonical_name'].lower().replace(' ', '_')
            doc_ref = collection_ref.document(doc_id)
            batch.set(doc_ref, doc)
            uploaded += 1

        batch.commit()
        print(f'  å·²ä¸Šä¼  {uploaded}/{len(variant_docs)} ä¸ªæ–‡æ¡£...')

    print(f'\nâœ… ä¸Šä¼ å®Œæˆ: {uploaded} ä¸ªæ–‡æ¡£\n')
    return uploaded

def save_report(variant_docs):
    """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
    report_file = REPORTS_DIR / '12-company-variants-report.json'

    print(f'ğŸ’¾ ä¿å­˜æŠ¥å‘Šåˆ° {report_file}...')

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_companies': len(variant_docs),
        'verified_companies': sum(1 for d in variant_docs if d.get('is_verified')),
        'auto_detected_companies': sum(1 for d in variant_docs if not d.get('is_verified')),
        'total_committees': sum(d['committee_count'] for d in variant_docs),
        'avg_variants_per_company': sum(d['variant_count'] for d in variant_docs) / len(variant_docs),
        'generated_at': datetime.utcnow().isoformat()
    }

    # Top 20 companies by committee count
    top_20 = sorted(variant_docs, key=lambda x: x['committee_count'], reverse=True)[:20]

    report = {
        'statistics': stats,
        'top_20_companies': [
            {
                'canonical_name': d['canonical_name'],
                'display_name': d['display_name'],
                'committee_count': d['committee_count'],
                'variant_count': d['variant_count'],
                'is_verified': d.get('is_verified', False)
            }
            for d in top_20
        ]
    }

    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)

    print(f'âœ… æŠ¥å‘Šå·²ä¿å­˜\n')

    # æ‰“å°ç»Ÿè®¡
    print('ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:')
    print(f'  æ€»å…¬å¸æ•°: {stats["total_companies"]}')
    print(f'  å·²éªŒè¯å…¬å¸: {stats["verified_companies"]}')
    print(f'  è‡ªåŠ¨æ£€æµ‹å…¬å¸: {stats["auto_detected_companies"]}')
    print(f'  æ€»å§”å‘˜ä¼šæ•°: {stats["total_committees"]}')
    print(f'  å¹³å‡æ¯å…¬å¸å˜ä½“æ•°: {stats["avg_variants_per_company"]:.1f}')
    print()

    print('ğŸ† Top 10 å…¬å¸ (æŒ‰å§”å‘˜ä¼šæ•°):')
    for i, company in enumerate(top_20[:10], 1):
        verified = 'âœ“' if company['is_verified'] else ' '
        print(f'  {i:2d}. [{verified}] {company["canonical_name"]:30s} - {company["committee_count"]:3d} ä¸ªå§”å‘˜ä¼š, {company["variant_count"]} ä¸ªå˜ä½“')
    print()

def main():
    """ä¸»å‡½æ•°"""
    print('\n' + '='*80)
    print('ğŸ¢ è‡ªåŠ¨æ„å»ºå…¬å¸åç§°å˜ä½“æ˜ å°„è¡¨')
    print('='*80 + '\n')

    # åˆå§‹åŒ–Firestore
    init_firestore()

    # æ­¥éª¤1: æå–æ‰€æœ‰å…¬å¸åç§°
    print('='*80)
    print('æ­¥éª¤ 1/5: æå–å…¬å¸åç§°')
    print('='*80 + '\n')
    companies, verified_mapping = extract_all_company_names()

    # æ­¥éª¤2: åˆ†ç»„ç›¸ä¼¼å…¬å¸
    print('='*80)
    print('æ­¥éª¤ 2/5: åˆ†ç»„ç›¸ä¼¼å…¬å¸')
    print('='*80 + '\n')
    grouped = group_similar_companies(companies, similarity_threshold=85)

    # æ­¥éª¤3: æ„å»ºvariantæ–‡æ¡£
    print('='*80)
    print('æ­¥éª¤ 3/5: æ„å»ºvariantæ–‡æ¡£')
    print('='*80 + '\n')
    variant_docs = build_variant_documents(grouped, companies)

    # æ­¥éª¤4: æ·»åŠ å·²éªŒè¯å…¬å¸
    print('='*80)
    print('æ­¥éª¤ 4/5: æ·»åŠ å·²éªŒè¯å…¬å¸')
    print('='*80 + '\n')
    variant_docs = add_verified_companies(variant_docs)

    # æ­¥éª¤5: ä¸Šä¼ åˆ°Firestore
    print('='*80)
    print('æ­¥éª¤ 5/5: ä¸Šä¼ åˆ°Firestore')
    print('='*80 + '\n')

    print(f'å‡†å¤‡ä¸Šä¼  {len(variant_docs)} ä¸ªvariantæ–‡æ¡£åˆ°Firestore...\n')
    uploaded = upload_variants(variant_docs)

    # ä¿å­˜æŠ¥å‘Š
    save_report(variant_docs)

    print('='*80)
    print('âœ… å®Œæˆï¼')
    print('='*80)
    print(f'\nä¸‹ä¸€æ­¥:')
    print(f'  1. åœ¨Firebase ConsoleéªŒè¯ fec_company_name_variants collection')
    print(f'  2. æŸ¥çœ‹æŠ¥å‘Š: {REPORTS_DIR / "12-company-variants-report.json"}')
    print(f'  3. å¼€å§‹ä¸Šä¼ contributions: python3 02-upload-incremental.py')
    print()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­')
        sys.exit(0)
    except Exception as e:
        print(f'\nâŒ é”™è¯¯: {e}')
        import traceback
        traceback.print_exc()
        sys.exit(1)
