#!/usr/bin/env python3
"""
FECå®Œæ•´è®¾ç½®è„šæœ¬
1. ä¸‹è½½æ‰€éœ€å¹´ä»½çš„æ•°æ®ï¼ˆ2024, 2026ï¼‰
2. ä¸Šä¼ æ‰€æœ‰rawæ•°æ®åˆ°Firebase
3. æ„å»ºcompany_indexå’Œcompany_party_summary
4. éªŒè¯æŸ¥è¯¢åŠŸèƒ½
"""

import sys
import os
import re
import time
import json
import subprocess
import random
import requests
import zipfile
from pathlib import Path
from datetime import datetime
from collections import defaultdict

try:
    import firebase_admin
    from firebase_admin import credentials, firestore
    from google.api_core.exceptions import ResourceExhausted, DeadlineExceeded
except ImportError:
    print('âŒ Firebaseåº“æœªå®‰è£…')
    print('è¯·è¿è¡Œ: pip install firebase-admin google-cloud-firestore')
    sys.exit(1)

# é…ç½®
DATA_DIR = Path(__file__).parent / 'raw_data'
PROJECT_ID = 'stanseproject'
PROGRESS_FILE = Path(__file__).parent.parent / 'reports' / '01-upload-progress.json'
BASE_URL = 'https://www.fec.gov/files/bulk-downloads'

# æ‰¹æ¬¡é…ç½®
BATCH_SIZE = 50
MIN_DELAY = 3.0
MAX_DELAY = 300.0
INITIAL_RETRY_DELAY = 30.0

# å…¨å±€å˜é‡
db = None

# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def save_progress(data):
    """ä¿å­˜ä¸Šä¼ è¿›åº¦"""
    with open(PROGRESS_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def load_progress():
    """åŠ è½½ä¸Šä¼ è¿›åº¦"""
    if PROGRESS_FILE.exists():
        with open(PROGRESS_FILE, 'r') as f:
            return json.load(f)
    return {}

def init_firestore():
    """åˆå§‹åŒ–Firestore"""
    global db
    print('\nğŸ”§ åˆå§‹åŒ–Firestoreè¿æ¥...')

    try:
        if not firebase_admin._apps:
            access_token = os.environ.get('GCLOUD_ACCESS_TOKEN')

            if not access_token:
                print('  â„¹ï¸  ä»gcloudè·å–access token...')
                result = subprocess.run(
                    ['gcloud', 'auth', 'print-access-token'],
                    capture_output=True, text=True, check=True
                )
                access_token = result.stdout.strip()
            else:
                print('  âœ“ ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„access token')

            from google.oauth2 import credentials as oauth_creds
            cred = oauth_creds.Credentials(access_token)
            firebase_admin.initialize_app(cred, options={'projectId': PROJECT_ID})

        db = firestore.client()
        print(f'âœ… Firestoreå·²è¿æ¥ (é¡¹ç›®: {PROJECT_ID})')
        return db
    except Exception as e:
        print(f'âŒ å¤±è´¥: {e}')
        sys.exit(1)

def commit_with_retry(batch, retry_count=0, max_retries=10):
    """æäº¤æ‰¹æ¬¡ï¼Œå¸¦æŒ‡æ•°é€€é¿é‡è¯•"""
    try:
        batch.commit()
        return True
    except (ResourceExhausted, DeadlineExceeded) as e:
        if retry_count >= max_retries:
            print(f'  âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})')
            return False

        delay = min(
            INITIAL_RETRY_DELAY * (2 ** retry_count) + random.uniform(0, 5),
            MAX_DELAY
        )

        print(f'  âš ï¸  é…é¢é™åˆ¶ï¼Œç­‰å¾… {delay:.1f} ç§’åé‡è¯•ï¼ˆç¬¬ {retry_count + 1}/{max_retries} æ¬¡ï¼‰...')
        time.sleep(delay)

        return commit_with_retry(batch, retry_count + 1, max_retries)
    except Exception as e:
        print(f'  âŒ æœªçŸ¥é”™è¯¯: {e}')
        return False

def normalize_company_name(name):
    """æ ‡å‡†åŒ–å…¬å¸åç§°ç”¨äºç´¢å¼•"""
    if not name:
        return ''
    normalized = name.lower()
    suffixes = ['corporation', 'corp', 'inc', 'incorporated', 'company', 'co',
                'llc', 'lp', 'ltd', 'limited', 'political action committee', 'pac']
    for suffix in suffixes:
        normalized = re.sub(rf'\b{suffix}\b\.?', '', normalized)
    normalized = re.sub(r'[^\w\s]', '', normalized)
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    return normalized

# ============================================================================
# æ­¥éª¤1: ä¸‹è½½æ•°æ®
# ============================================================================

def download_file(url, dest_path):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    if dest_path.exists():
        print(f'  â­ï¸  è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {dest_path.name}')
        return True

    print(f'  ğŸ“¥ ä¸‹è½½: {url}')
    try:
        response = requests.get(url, stream=True, timeout=60)
        response.raise_for_status()

        dest_path.parent.mkdir(parents=True, exist_ok=True)

        with open(dest_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        # å¦‚æœæ˜¯zipæ–‡ä»¶ï¼Œè§£å‹
        if dest_path.suffix == '.zip':
            print(f'  ğŸ“¦ è§£å‹: {dest_path.name}')
            with zipfile.ZipFile(dest_path, 'r') as zip_ref:
                zip_ref.extractall(dest_path.parent)

        print(f'  âœ… å®Œæˆ: {dest_path.name}')
        return True
    except Exception as e:
        print(f'  âŒ å¤±è´¥: {e}')
        return False

def download_year_data(year, suffix):
    """ä¸‹è½½æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰æ•°æ®"""
    print(f'\nğŸ“¥ ä¸‹è½½ {year} å¹´æ•°æ®...')

    files_to_download = [
        (f'{BASE_URL}/{year}/cm{suffix}.zip', DATA_DIR / 'committees' / f'cm{suffix}.zip'),
        (f'{BASE_URL}/{year}/cn{suffix}.zip', DATA_DIR / 'candidates' / f'cn{suffix}.zip'),
        (f'{BASE_URL}/{year}/pas2{suffix}.zip', DATA_DIR / 'contributions' / f'pas2{suffix}.zip'),
    ]

    success_count = 0
    for url, dest in files_to_download:
        if download_file(url, dest):
            success_count += 1

    print(f'âœ… {year}å¹´æ•°æ®ä¸‹è½½å®Œæˆ: {success_count}/3 ä¸ªæ–‡ä»¶')
    return success_count == 3

# ============================================================================
# æ­¥éª¤2: ä¸Šä¼ Rawæ•°æ®
# ============================================================================

def upload_data_for_year(year, suffix, progress):
    """ä¸Šä¼ æŒ‡å®šå¹´ä»½çš„æ‰€æœ‰æ•°æ®"""
    print(f'\n{"="*70}')
    print(f'ğŸ“¤ ä¸Šä¼  {year} å¹´æ•°æ®åˆ°Firebase')
    print(f'{"="*70}')

    year_key = f'year_{year}'
    if year_key not in progress:
        progress[year_key] = {}

    # ä¸Šä¼ committees
    if not progress[year_key].get('committees_completed'):
        from upload_incremental import upload_committees_incremental
        count = upload_committees_incremental(year, suffix, progress.setdefault(year_key, {}))
        print(f'âœ… Committees: {count} æ¡')
    else:
        print(f'âœ… Committeeså·²å®Œæˆ: {progress[year_key].get("committees_uploaded", 0)} æ¡')

    # ä¸Šä¼ candidates
    if not progress[year_key].get('candidates_completed'):
        from upload_incremental import upload_candidates_incremental
        count = upload_candidates_incremental(year, suffix, progress[year_key])
        print(f'âœ… Candidates: {count} æ¡')
    else:
        print(f'âœ… Candidateså·²å®Œæˆ: {progress[year_key].get("candidates_uploaded", 0)} æ¡')

    # ä¸Šä¼ contributions
    if not progress[year_key].get('contributions_completed'):
        from upload_incremental import upload_contributions_incremental
        count = upload_contributions_incremental(year, suffix, progress[year_key])
        print(f'âœ… Contributions: {count} æ¡')
    else:
        print(f'âœ… Contributionså·²å®Œæˆ: {progress[year_key].get("contributions_uploaded", 0)} æ¡')

    save_progress(progress)

# ============================================================================
# æ­¥éª¤3: æ„å»ºç´¢å¼•å’Œæ±‡æ€»
# ============================================================================

def build_company_index():
    """ä»committeesæ„å»ºcompany_index"""
    print(f'\n{"="*70}')
    print('ğŸ—ï¸  æ„å»ºCompany Index')
    print(f'{"="*70}')

    # ä»fec_raw_committeesæå–æ‰€æœ‰å”¯ä¸€å…¬å¸
    companies = {}

    print('  ğŸ“– è¯»å–committeesæ•°æ®...')
    committees_ref = db.collection('fec_raw_committees')
    docs = committees_ref.stream()

    count = 0
    for doc in docs:
        data = doc.to_dict()
        connected_org = data.get('connected_org_name', '').strip()
        committee_id = data.get('committee_id')
        year = data.get('data_year')

        if connected_org and committee_id:
            normalized = normalize_company_name(connected_org)

            if normalized not in companies:
                companies[normalized] = {
                    'company_name': connected_org,
                    'normalized_name': normalized,
                    'committee_ids': [],
                    'search_keywords': set()
                }

            companies[normalized]['committee_ids'].append({
                'committee_id': committee_id,
                'year': year
            })

            # ç”Ÿæˆæœç´¢å…³é”®è¯
            words = normalized.split()
            companies[normalized]['search_keywords'].update(words)

        count += 1
        if count % 1000 == 0:
            print(f'  å¤„ç† {count} æ¡committees...')

    print(f'  âœ… æå–åˆ° {len(companies)} ä¸ªå”¯ä¸€å…¬å¸')

    # ä¸Šä¼ åˆ°fec_company_index
    print('  ğŸ“¤ ä¸Šä¼ åˆ°fec_company_index...')
    batch = db.batch()
    batch_count = 0
    uploaded = 0

    for normalized_name, company_data in companies.items():
        doc_ref = db.collection('fec_company_index').document(normalized_name)

        doc_data = {
            'company_name': company_data['company_name'],
            'normalized_name': normalized_name,
            'committee_ids': company_data['committee_ids'],
            'search_keywords': list(company_data['search_keywords']),
            'created_at': datetime.utcnow(),
            'last_updated': datetime.utcnow()
        }

        batch.set(doc_ref, doc_data)
        batch_count += 1

        if batch_count >= BATCH_SIZE:
            if commit_with_retry(batch):
                uploaded += batch_count
                print(f'  âœ“ å·²ä¸Šä¼  {uploaded} ä¸ªå…¬å¸ç´¢å¼•')
                time.sleep(MIN_DELAY)
                batch = db.batch()
                batch_count = 0

    if batch_count > 0:
        if commit_with_retry(batch):
            uploaded += batch_count

    print(f'âœ… Company Indexæ„å»ºå®Œæˆ: {uploaded} ä¸ªå…¬å¸')
    return uploaded

def build_company_summaries():
    """æ„å»ºcompany_party_summary"""
    print(f'\n{"="*70}')
    print('ğŸ—ï¸  æ„å»ºCompany Party Summaries')
    print(f'{"="*70}')

    # ä»company_indexè·å–æ‰€æœ‰å…¬å¸
    print('  ğŸ“– è¯»å–company_index...')
    companies_ref = db.collection('fec_company_index')
    companies = list(companies_ref.stream())

    print(f'  æ‰¾åˆ° {len(companies)} ä¸ªå…¬å¸')

    uploaded = 0

    for company_doc in companies:
        company_data = company_doc.to_dict()
        normalized_name = company_data['normalized_name']
        committee_ids = [c['committee_id'] for c in company_data['committee_ids']]

        print(f'\n  å¤„ç†: {company_data["company_name"]}')

        # ä¸ºæ¯ä¸ªå¹´ä»½åˆ›å»ºæ±‡æ€»
        years_data = {}

        # è·å–è¯¥å…¬å¸çš„æ‰€æœ‰ææ¬¾
        print(f'    æŸ¥æ‰¾ææ¬¾è®°å½•...')
        contributions_ref = db.collection('fec_raw_contributions_pac_to_candidate')

        for committee_id in committee_ids:
            query = contributions_ref.where('committee_id', '==', committee_id)
            contributions = query.stream()

            for contrib_doc in contributions:
                contrib_data = contrib_doc.to_dict()
                year = contrib_data.get('data_year')
                candidate_id = contrib_data.get('candidate_id')
                amount = contrib_data.get('transaction_amount', 0)

                if not year or not candidate_id:
                    continue

                if year not in years_data:
                    years_data[year] = {}

                # æŸ¥æ‰¾å€™é€‰äººçš„æ”¿å…š
                cand_doc = db.collection('fec_raw_candidates').document(f'{candidate_id}_{year}').get()
                if cand_doc.exists:
                    cand_data = cand_doc.to_dict()
                    party = cand_data.get('party_affiliation', 'Unknown')

                    if party not in years_data[year]:
                        years_data[year][party] = {'total_amount': 0, 'contribution_count': 0}

                    years_data[year][party]['total_amount'] += amount
                    years_data[year][party]['contribution_count'] += 1

        # ä¸ºæ¯ä¸ªå¹´ä»½åˆ›å»ºæ±‡æ€»æ–‡æ¡£
        batch = db.batch()
        batch_count = 0

        for year, party_data in years_data.items():
            doc_id = f'{normalized_name}_{year}'
            doc_ref = db.collection('fec_company_party_summary').document(doc_id)

            total_contributed = sum(p['total_amount'] for p in party_data.values())

            doc_data = {
                'company_name': company_data['company_name'],
                'normalized_name': normalized_name,
                'data_year': year,
                'party_totals': party_data,
                'total_contributed': total_contributed,
                'created_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_count += 1

            if batch_count >= BATCH_SIZE:
                if commit_with_retry(batch):
                    uploaded += batch_count
                    print(f'    âœ“ å·²ä¸Šä¼  {uploaded} ä¸ªæ±‡æ€»')
                    time.sleep(MIN_DELAY)
                    batch = db.batch()
                    batch_count = 0

        if batch_count > 0:
            if commit_with_retry(batch):
                uploaded += batch_count

    print(f'\nâœ… Company Summariesæ„å»ºå®Œæˆ: {uploaded} ä¸ªæ±‡æ€»')
    return uploaded

# ============================================================================
# æ­¥éª¤4: éªŒè¯æŸ¥è¯¢
# ============================================================================

def test_query(company_name):
    """æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½"""
    print(f'\n{"="*70}')
    print(f'ğŸ” æµ‹è¯•æŸ¥è¯¢: {company_name}')
    print(f'{"="*70}')

    normalized = normalize_company_name(company_name)

    # æŸ¥æ‰¾å…¬å¸
    print(f'  æ­¥éª¤1: æŸ¥æ‰¾å…¬å¸ "{company_name}"')
    company_doc = db.collection('fec_company_index').document(normalized).get()

    if not company_doc.exists:
        print(f'  âŒ æœªæ‰¾åˆ°å…¬å¸')
        return False

    company_data = company_doc.to_dict()
    print(f'  âœ… æ‰¾åˆ°: {company_data["company_name"]}')
    print(f'     PACs: {len(company_data["committee_ids"])} ä¸ª')

    # è·å–æ”¿å…šæ±‡æ€»
    print(f'\n  æ­¥éª¤2: è·å–æ”¿å…šææ¬¾æ±‡æ€»')
    summaries_ref = db.collection('fec_company_party_summary')
    query = summaries_ref.where('normalized_name', '==', normalized)
    summaries = list(query.stream())

    if not summaries:
        print(f'  âš ï¸  æœªæ‰¾åˆ°æ±‡æ€»æ•°æ®')
        return False

    print(f'  âœ… æ‰¾åˆ° {len(summaries)} ä¸ªå¹´ä»½çš„æ•°æ®\n')

    for summary_doc in summaries:
        summary_data = summary_doc.to_dict()
        year = summary_data['data_year']
        party_totals = summary_data['party_totals']
        total = summary_data['total_contributed']

        print(f'  ğŸ“Š {year}å¹´:')
        print(f'     æ€»ææ¬¾: ${total/100:,.2f}')

        for party, info in sorted(party_totals.items(), key=lambda x: x[1]['total_amount'], reverse=True):
            amount = info['total_amount']
            count = info['contribution_count']
            percentage = (amount / total * 100) if total > 0 else 0
            print(f'     {party}: ${amount/100:,.2f} ({percentage:.1f}%) - {count} ç¬”')
        print()

    return True

# ============================================================================
# ä¸»æµç¨‹
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print('\n' + '='*70)
    print('ğŸš€ FECæ•°æ®å®Œæ•´è®¾ç½®æµç¨‹')
    print('='*70)

    # åŠ è½½è¿›åº¦
    progress = load_progress()

    # åˆå§‹åŒ–Firebase
    init_firestore()

    # æ­¥éª¤1: ä¸‹è½½2024å’Œ2026å¹´æ•°æ®
    print('\n' + '='*70)
    print('æ­¥éª¤1: ä¸‹è½½æ•°æ®')
    print('='*70)

    years_to_process = [
        (2024, '24'),
        (2026, '26'),
    ]

    for year, suffix in years_to_process:
        download_year_data(year, suffix)

    # æ­¥éª¤2: ä¸Šä¼ Rawæ•°æ®
    print('\n' + '='*70)
    print('æ­¥éª¤2: ä¸Šä¼ Rawæ•°æ®åˆ°Firebase')
    print('='*70)

    for year, suffix in years_to_process:
        upload_data_for_year(year, suffix, progress)

    # æ­¥éª¤3: æ„å»ºç´¢å¼•å’Œæ±‡æ€»
    print('\n' + '='*70)
    print('æ­¥éª¤3: æ„å»ºç´¢å¼•å’Œæ±‡æ€»è¡¨')
    print('='*70)

    if not progress.get('company_index_built'):
        build_company_index()
        progress['company_index_built'] = True
        save_progress(progress)
    else:
        print('âœ… Company Indexå·²æ„å»º')

    if not progress.get('company_summaries_built'):
        build_company_summaries()
        progress['company_summaries_built'] = True
        save_progress(progress)
    else:
        print('âœ… Company Summarieså·²æ„å»º')

    # æ­¥éª¤4: éªŒè¯æŸ¥è¯¢
    print('\n' + '='*70)
    print('æ­¥éª¤4: éªŒè¯æŸ¥è¯¢åŠŸèƒ½')
    print('='*70)

    test_companies = ['Hallmark', 'Microsoft', 'Boeing']
    for company in test_companies:
        test_query(company)
        time.sleep(1)

    print('\n' + '='*70)
    print('âœ… å®Œæ•´è®¾ç½®æµç¨‹å®Œæˆï¼')
    print('='*70)
    print('\nğŸ“Š ç³»ç»ŸçŠ¶æ€:')
    print(f'  - å¹´ä»½æ•°æ®: 2024, 2026')
    print(f'  - Raw Collections: committees, candidates, contributions')
    print(f'  - Processed Collections: company_index, company_party_summary')
    print(f'  - æŸ¥è¯¢åŠŸèƒ½: å·²éªŒè¯')
    print()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­')
        sys.exit(0)
    except Exception as e:
        print(f'\nâŒ é”™è¯¯: {e}')
        import traceback
        traceback.print_exc()
        sys.exit(1)
