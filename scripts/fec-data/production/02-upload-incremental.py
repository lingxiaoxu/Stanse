#!/usr/bin/env python3
"""
å¢é‡å¼FECæ•°æ®ä¸Šä¼  - å¸¦è‡ªåŠ¨é‡è¯•

ç‰¹ç‚¹ï¼š
1. æ£€æŸ¥å¹¶è·³è¿‡å·²ä¸Šä¼ çš„è®°å½•
2. é‡åˆ°é…é¢é”™è¯¯æ—¶è‡ªåŠ¨ç­‰å¾…å¹¶é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
3. ä¿å­˜è¿›åº¦ï¼Œå¯ä»¥ä¸­æ–­åç»§ç»­
4. è¯¦ç»†è¿›åº¦æ˜¾ç¤º
5. ä½¿ç”¨é»˜è®¤å‡­è¯ï¼Œæ— éœ€æ‰‹åŠ¨åˆ·æ–°token
6. å¯ä»¥æ— äººå€¼å®ˆè¿è¡Œç›´åˆ°å®Œæˆ
7. æ”¯æŒé™åˆ¶ä¸Šä¼ æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰

ç”¨æ³•ï¼š
  python3 02-upload-incremental.py              # ä¸Šä¼ å…¨éƒ¨
  python3 02-upload-incremental.py --limit 100  # åªä¸Šä¼ 100æ¡ï¼ˆæµ‹è¯•ï¼‰
"""

import sys
import re
import argparse
import time
import json
import os
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import random

try:
    import firebase_admin
    from firebase_admin import credentials, firestore
    from google.api_core.exceptions import ResourceExhausted, DeadlineExceeded, Unauthenticated
    import google.auth
except ImportError:
    print('âŒ Firebaseåº“æœªå®‰è£…')
    sys.exit(1)

# é…ç½®
DATA_DIR = Path(__file__).parent.parent / 'raw_data'
PROJECT_ID = 'stanseproject'
PROGRESS_FILE = Path(__file__).parent.parent / 'reports' / '01-upload-progress.json'

# æ•°æ®å¹´ä»½é…ç½® (é»˜è®¤ä½¿ç”¨2024å¹´æ•°æ®ï¼Œå¯ä¿®æ”¹ä¸º16/18/20/22/24)
DATA_YEAR = '24'  # å¯é€‰: '16', '18', '20', '22', '24'

# æ‰¹æ¬¡é…ç½®
BATCH_SIZE = 500  # Firestoreæœ€å¤§æ‰¹æ¬¡é™åˆ¶
MIN_DELAY = 0.1  # æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
MAX_DELAY = 300.0  # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
INITIAL_RETRY_DELAY = 30.0  # åˆå§‹é‡è¯•å»¶è¿Ÿ

db = None

def save_progress(data):
    """ä¿å­˜ä¸Šä¼ è¿›åº¦"""
    with open(PROGRESS_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def load_progress():
    """åŠ è½½ä¸Šä¼ è¿›åº¦"""
    if PROGRESS_FILE.exists():
        with open(PROGRESS_FILE, 'r') as f:
            return json.load(f)
    return {
        'committees_last_line': 0,
        'committees_uploaded': 0,
        'committees_skipped': 0,
        'candidates_last_line': 0,
        'candidates_uploaded': 0,
        'linkages_last_line': 0,
        'linkages_uploaded': 0,
        'linkages_completed': False,
        'transfers_last_line': 0,
        'transfers_uploaded': 0,
        'transfers_completed': False,
        'last_updated': None
    }

def init_firestore():
    """åˆå§‹åŒ–Firestore - ä½¿ç”¨gcloud auth"""
    global db
    print('\nğŸ”§ åˆå§‹åŒ–Firestoreè¿æ¥...')

    try:
        if not firebase_admin._apps:
            # ç›´æ¥ä½¿ç”¨ Firebase Adminï¼Œå®ƒä¼šè‡ªåŠ¨æŸ¥æ‰¾å‡­è¯
            # æŒ‰é¡ºåºå°è¯•: ç¯å¢ƒå˜é‡ -> ADC -> gcloud auth
            print('  â„¹ï¸  ä½¿ç”¨é»˜è®¤å‡­è¯é“¾ï¼ˆgcloud/ç¯å¢ƒå˜é‡ï¼‰...')
            firebase_admin.initialize_app(options={'projectId': PROJECT_ID})

        db = firestore.client()
        print(f'âœ… Firestoreå·²è¿æ¥ (é¡¹ç›®: {PROJECT_ID})')
        print('  ğŸ’¡ ä½¿ç”¨å·²ç™»å½•çš„ gcloud å‡­è¯')
        return db
    except Exception as e:
        print(f'âŒ å¤±è´¥: {e}')
        print('  æç¤º: è¯·ç¡®ä¿å·²è¿è¡Œ gcloud auth login')
        sys.exit(1)

def refresh_firestore_client():
    """åˆ·æ–°Firestoreå®¢æˆ·ç«¯å’Œtoken"""
    global db
    print('  ğŸ”„ åˆ·æ–°Firestoreè¿æ¥å’Œtoken...')

    try:
        # é‡æ–°è·å–Firestoreå®¢æˆ·ç«¯
        # Firebase Admin SDKä¼šè‡ªåŠ¨åˆ·æ–°ADC token
        db = firestore.client()
        print('  âœ… Tokenå·²åˆ·æ–°')
        return True
    except Exception as e:
        print(f'  âŒ åˆ·æ–°å¤±è´¥: {e}')
        return False

def commit_batch_with_token_refresh(batch_docs, collection_ref):
    """
    æäº¤æ‰¹æ¬¡æ–‡æ¡£ï¼Œè‡ªåŠ¨å¤„ç†tokenåˆ·æ–°

    Args:
        batch_docs: List of (doc_ref, doc_data) tuples
        collection_ref: Firestore collection reference

    Returns:
        True if successful, False otherwise
    """
    global db

    # åˆ›å»ºæ–°batch
    batch = db.batch()
    for doc_ref, doc_data in batch_docs:
        batch.set(doc_ref, doc_data)

    # å°è¯•æäº¤
    try:
        batch.commit()
        return True
    except Unauthenticated as e:
        print(f'  âš ï¸  Tokenè¿‡æœŸï¼Œæ­£åœ¨åˆ·æ–°å¹¶é‡è¯•...')
        if refresh_firestore_client():
            # Tokenåˆ·æ–°åï¼Œç”¨æ–°çš„dbå®¢æˆ·ç«¯é‡æ–°åˆ›å»ºbatch
            new_batch = db.batch()
            for doc_ref, doc_data in batch_docs:
                # ä½¿ç”¨æ–°çš„dbå®¢æˆ·ç«¯é‡æ–°åˆ›å»ºdoc_ref
                new_doc_ref = collection_ref.document(doc_ref.id)
                new_batch.set(new_doc_ref, doc_data)

            # é‡è¯•æäº¤
            try:
                new_batch.commit()
                print('  âœ… Tokenåˆ·æ–°åé‡è¯•æˆåŠŸ')
                return True
            except Exception as retry_err:
                print(f'  âŒ Tokenåˆ·æ–°åé‡è¯•ä»å¤±è´¥: {retry_err}')
                return False
        else:
            return False
    except Exception as e:
        print(f'  âŒ æäº¤å¤±è´¥: {e}')
        return False

def commit_with_retry(batch, retry_count=0, max_retries=10, batch_docs=None, collection_ref=None):
    """
    æäº¤æ‰¹æ¬¡ï¼Œå¸¦æŒ‡æ•°é€€é¿é‡è¯•å’Œtokenè‡ªåŠ¨åˆ·æ–°

    Args:
        batch: Firestore batch
        retry_count: å½“å‰é‡è¯•æ¬¡æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        batch_docs: List of (doc_ref, doc_data) - ç”¨äºtokenåˆ·æ–°æ—¶é‡å»ºbatch
        collection_ref: Collection reference - ç”¨äºtokenåˆ·æ–°æ—¶é‡å»ºbatch

    Returns:
        True if successful, False if max retries exceeded
    """
    try:
        batch.commit()
        return True
    except Unauthenticated as e:
        if batch_docs and collection_ref:
            print(f'  âš ï¸  Tokenè¿‡æœŸï¼Œæ­£åœ¨åˆ·æ–°å¹¶é‡æ–°æäº¤...')
            return commit_batch_with_token_refresh(batch_docs, collection_ref)
        else:
            print(f'  âŒ Tokenè¿‡æœŸä½†æ— æ³•è‡ªåŠ¨åˆ·æ–°ï¼ˆç¼ºå°‘batch_docsæˆ–collection_refï¼‰')
            return False
    except (ResourceExhausted, DeadlineExceeded) as e:
        if retry_count >= max_retries:
            print(f'  âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})')
            return False

        # æŒ‡æ•°é€€é¿ï¼šåˆå§‹å»¶è¿Ÿ * (2 ^ retry_count) + éšæœºæŠ–åŠ¨
        delay = min(
            INITIAL_RETRY_DELAY * (2 ** retry_count) + random.uniform(0, 5),
            MAX_DELAY
        )

        print(f'  âš ï¸  é…é¢é™åˆ¶ï¼Œç­‰å¾… {delay:.1f} ç§’åé‡è¯•ï¼ˆç¬¬ {retry_count + 1}/{max_retries} æ¬¡ï¼‰...')
        time.sleep(delay)

        # é€’å½’é‡è¯•
        return commit_with_retry(batch, retry_count + 1, max_retries, batch_docs, collection_ref)
    except Exception as e:
        print(f'  âŒ æœªçŸ¥é”™è¯¯: {e}')
        return False

def upload_committees_incremental(year, year_suffix, progress):
    """å¢é‡ä¸Šä¼ å§”å‘˜ä¼šæ•°æ®"""
    collection_name = 'fec_raw_committees'
    file_path = DATA_DIR / 'committees' / f'cm{DATA_YEAR}.txt'

    if not file_path.exists():
        print(f'âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return 0

    print(f'\nğŸ“¤ å¢é‡ä¸Šä¼ å§”å‘˜ä¼šæ•°æ® ({year})...')

    start_line = progress.get('committees_last_line', 0)
    uploaded = progress.get('committees_uploaded', 0)
    skipped = progress.get('committees_skipped', 0)

    if start_line > 0:
        print(f'  ğŸ“ ä»ç¬¬ {start_line} è¡Œç»§ç»­ï¼ˆå·²ä¸Šä¼  {uploaded} æ¡ï¼Œå·²è·³è¿‡ {skipped} æ¡ï¼‰')

    batch = db.batch()
    batch_count = 0
    current_line = 0
    total_processed = 0

    with open(file_path, 'r', encoding='latin-1') as f:
        for line_num, line in enumerate(f, 1):
            # è·³è¿‡å·²å¤„ç†çš„è¡Œ
            if line_num <= start_line:
                continue

            current_line = line_num
            fields = line.strip().split('|')
            if len(fields) < 15:
                continue

            committee_id = fields[0]
            if not committee_id:
                continue

            doc_id = f'{committee_id}_{year}'
            doc_ref = db.collection(collection_name).document(doc_id)

            # æ·»åŠ åˆ°æ‰¹æ¬¡
            doc_data = {
                'committee_id': fields[0],
                'committee_name': fields[1],
                'treasurer_name': fields[2],
                'street_1': fields[3],
                'street_2': fields[4],
                'city': fields[5],
                'state': fields[6],
                'zip': fields[7],
                'designation': fields[8],
                'committee_type': fields[9],
                'party': fields[10],
                'filing_frequency': fields[11],
                'interest_group_category': fields[12],
                'connected_org_name': fields[13],
                'candidate_id': fields[14],
                'data_year': year,
                'election_cycle': f'{year-1}-{year}',
                'source_file': f'cm{year_suffix}.zip',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_count += 1
            total_processed += 1

            # æäº¤æ‰¹æ¬¡
            if batch_count >= BATCH_SIZE:
                if commit_with_retry(batch):
                    uploaded += batch_count
                    print(f'  âœ“ ç¬¬ {current_line} è¡Œ | æœ¬æ¬¡ä¸Šä¼  {batch_count} æ¡ | æ€»è®¡: {uploaded} æ¡ (è·³è¿‡ {skipped})')

                    # ä¿å­˜è¿›åº¦
                    progress['committees_last_line'] = current_line
                    progress['committees_uploaded'] = uploaded
                    progress['committees_skipped'] = skipped
                    progress['last_updated'] = datetime.utcnow().isoformat()
                    save_progress(progress)

                    # æ­£å¸¸å»¶è¿Ÿ
                    time.sleep(MIN_DELAY + random.uniform(0, 2))
                    batch = db.batch()
                    batch_count = 0
                else:
                    # å¤±è´¥ï¼Œä¿å­˜è¿›åº¦å¹¶é€€å‡º
                    print(f'  ğŸ’¾ ä¿å­˜è¿›åº¦: ç¬¬ {current_line} è¡Œï¼Œå·²ä¸Šä¼  {uploaded} æ¡')
                    progress['committees_last_line'] = current_line
                    progress['committees_uploaded'] = uploaded
                    progress['committees_skipped'] = skipped
                    save_progress(progress)
                    return uploaded

    # æäº¤å‰©ä½™è®°å½•
    if batch_count > 0:
        if commit_with_retry(batch):
            uploaded += batch_count
            print(f'  âœ“ æœ€åæ‰¹æ¬¡ä¸Šä¼  {batch_count} æ¡')

    # ä¿å­˜æœ€ç»ˆè¿›åº¦
    progress['committees_last_line'] = current_line
    progress['committees_uploaded'] = uploaded
    progress['committees_skipped'] = skipped
    progress['committees_completed'] = True
    progress['last_updated'] = datetime.utcnow().isoformat()
    save_progress(progress)

    print(f'âœ… æˆåŠŸä¸Šä¼  {uploaded} æ¡æ–°è®°å½•ï¼Œè·³è¿‡ {skipped} æ¡å·²å­˜åœ¨è®°å½•')
    print(f'   å¤„ç†å®Œæˆåˆ°ç¬¬ {current_line} è¡Œ')
    return uploaded

def upload_candidates_incremental(year, year_suffix, progress):
    """å¢é‡ä¸Šä¼ å€™é€‰äººæ•°æ®"""
    collection_name = 'fec_raw_candidates'
    file_path = DATA_DIR / 'candidates' / f'cn{DATA_YEAR}.txt'

    if not file_path.exists():
        print(f'âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return 0

    print(f'\nğŸ“¤ å¢é‡ä¸Šä¼ å€™é€‰äººæ•°æ® ({year})...')

    start_line = progress.get('candidates_last_line', 0)
    uploaded = progress.get('candidates_uploaded', 0)

    if start_line > 0:
        print(f'  ğŸ“ ä»ç¬¬ {start_line} è¡Œç»§ç»­ï¼ˆå·²ä¸Šä¼  {uploaded} æ¡ï¼‰')

    batch = db.batch()
    batch_count = 0
    current_line = 0

    with open(file_path, 'r', encoding='latin-1') as f:
        for line_num, line in enumerate(f, 1):
            if line_num <= start_line:
                continue

            current_line = line_num
            fields = line.strip().split('|')
            if len(fields) < 15:
                continue

            candidate_id = fields[0]
            if not candidate_id:
                continue

            doc_id = f'{candidate_id}_{year}'
            doc_ref = db.collection(collection_name).document(doc_id)

            doc_data = {
                'candidate_id': fields[0],
                'candidate_name': fields[1],
                'party_affiliation': fields[2],
                'election_year': int(fields[3]) if fields[3] else year,
                'office_sought': fields[4],
                'state': fields[5],
                'district': fields[6],
                'incumbent_challenger_status': fields[7],
                'candidate_status': fields[8],
                'principal_committee_id': fields[9],
                'street_1': fields[10],
                'street_2': fields[11],
                'city': fields[12],
                'state_full': fields[13],
                'zip': fields[14],
                'data_year': year,
                'election_cycle': f'{year-1}-{year}',
                'source_file': f'cn{year_suffix}.zip',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_count += 1

            if batch_count >= BATCH_SIZE:
                if commit_with_retry(batch):
                    uploaded += batch_count
                    print(f'  âœ“ ç¬¬ {current_line} è¡Œ | å·²ä¸Šä¼  {uploaded} æ¡å€™é€‰äººè®°å½•')

                    progress['candidates_last_line'] = current_line
                    progress['candidates_uploaded'] = uploaded
                    progress['last_updated'] = datetime.utcnow().isoformat()
                    save_progress(progress)

                    time.sleep(MIN_DELAY + random.uniform(0, 2))
                    batch = db.batch()
                    batch_count = 0
                else:
                    progress['candidates_last_line'] = current_line
                    progress['candidates_uploaded'] = uploaded
                    save_progress(progress)
                    return uploaded

    if batch_count > 0:
        if commit_with_retry(batch):
            uploaded += batch_count

    progress['candidates_last_line'] = current_line
    progress['candidates_uploaded'] = uploaded
    progress['candidates_completed'] = True
    progress['last_updated'] = datetime.utcnow().isoformat()
    save_progress(progress)

    print(f'âœ… æˆåŠŸä¸Šä¼  {uploaded} æ¡å€™é€‰äººè®°å½•')
    return uploaded

def upload_contributions_incremental(year, year_suffix, progress, limit=None):
    """å¢é‡ä¸Šä¼ ææ¬¾æ•°æ®

    Args:
        year: æ•°æ®å¹´ä»½
        year_suffix: å¹´ä»½åç¼€
        progress: è¿›åº¦å­—å…¸
        limit: é™åˆ¶ä¸Šä¼ æ•°é‡ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼Œç”¨äºæµ‹è¯•ï¼‰
    """
    collection_name = f'fec_raw_contributions_pac_to_candidate_{DATA_YEAR}'
    file_path = DATA_DIR / 'contributions' / f'itpas2{DATA_YEAR}.txt'

    if not file_path.exists():
        print(f'âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: contributions/itpas2{DATA_YEAR}.txt')
        return 0

    print(f'\nğŸ“¤ å¢é‡ä¸Šä¼ ææ¬¾æ•°æ® ({year})...')

    start_line = progress.get('contributions_last_line', 0)
    uploaded = progress.get('contributions_uploaded', 0)

    if start_line > 0:
        print(f'  ğŸ“ ä»ç¬¬ {start_line} è¡Œç»§ç»­ï¼ˆå·²ä¸Šä¼  {uploaded} æ¡ï¼‰')

    batch = db.batch()
    batch_count = 0
    current_line = 0

    with open(file_path, 'r', encoding='latin-1') as f:
        for line_num, line in enumerate(f, 1):
            if line_num <= start_line:
                continue

            current_line = line_num
            fields = line.strip().split('|')
            if len(fields) < 17:
                continue

            committee_id = fields[0]
            candidate_id = fields[16]
            amount_str = fields[14]

            if not committee_id or not candidate_id:
                continue

            try:
                amount = float(amount_str) if amount_str else 0
                amount_cents = int(amount * 100)
            except (ValueError, TypeError):
                continue

            # åˆ›å»ºå”¯ä¸€ID: committee_candidate_linenumber
            doc_id = f'{committee_id}_{candidate_id}_{line_num}'
            doc_ref = db.collection(collection_name).document(doc_id)

            # âœ… æ ¹æ®FECå®˜æ–¹headeræ–‡ä»¶ pas2_header_file.csv çš„æ­£ç¡®æ˜ å°„
            # 0:CMTE_ID 1:AMNDT_IND 2:RPT_TP 3:TRANSACTION_PGI 4:IMAGE_NUM 5:TRANSACTION_TP
            # 6:ENTITY_TP 7:NAME 8:CITY 9:STATE 10:ZIP_CODE 11:EMPLOYER 12:OCCUPATION
            # 13:TRANSACTION_DT 14:TRANSACTION_AMT 15:OTHER_ID 16:CAND_ID 17:TRAN_ID
            doc_data = {
                'committee_id': fields[0],  # 0:CMTE_ID
                'amendment_indicator': fields[1] if len(fields) > 1 else '',  # 1:AMNDT_IND
                'report_type': fields[2] if len(fields) > 2 else '',  # 2:RPT_TP
                'election_type': fields[3] if len(fields) > 3 else '',  # 3:TRANSACTION_PGI
                'fec_record_number': fields[4] if len(fields) > 4 else '',  # 4:IMAGE_NUM
                'image_number': fields[4] if len(fields) > 4 else '',  # 4:IMAGE_NUM
                'transaction_type': fields[5] if len(fields) > 5 else '',  # 5:TRANSACTION_TP
                'entity_type': fields[6] if len(fields) > 6 else '',  # 6:ENTITY_TP
                'name': fields[7] if len(fields) > 7 else '',  # 7:NAME
                'city': fields[8] if len(fields) > 8 else '',  # 8:CITY
                'state': fields[9] if len(fields) > 9 else '',  # 9:STATE
                'zip': fields[10] if len(fields) > 10 else '',  # 10:ZIP_CODE
                'employer': fields[11] if len(fields) > 11 else '',  # 11:EMPLOYER
                'occupation': fields[12] if len(fields) > 12 else '',  # 12:OCCUPATION
                'transaction_date': fields[13] if len(fields) > 13 else '',  # 13:TRANSACTION_DT
                'transaction_amount': amount_cents,  # 14:TRANSACTION_AMT (ä»fields[14]è§£æ)
                'other_id': fields[15] if len(fields) > 15 else '',  # 15:OTHER_ID
                'candidate_id': fields[16],  # 16:CAND_ID
                'transaction_pgi': fields[3] if len(fields) > 3 else '',  # 3:TRANSACTION_PGI
                'data_year': year,
                'election_cycle': f'{year-1}-{year}',
                'source_file': f'pas2{year_suffix}.zip',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_count += 1

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æµ‹è¯•é™åˆ¶
            if limit and uploaded + batch_count >= limit:
                if commit_with_retry(batch):
                    uploaded += batch_count
                    print(f'  âœ“ ç¬¬ {current_line} è¡Œ | å·²ä¸Šä¼  {uploaded} æ¡ææ¬¾è®°å½•')
                    print(f'\nâš ï¸  å·²è¾¾åˆ°æµ‹è¯•é™åˆ¶ ({limit} æ¡)')

                    progress['contributions_last_line'] = current_line
                    progress['contributions_uploaded'] = uploaded
                    progress['last_updated'] = datetime.utcnow().isoformat()
                    save_progress(progress)
                    return uploaded
                else:
                    progress['contributions_last_line'] = current_line
                    progress['contributions_uploaded'] = uploaded
                    save_progress(progress)
                    return uploaded

            if batch_count >= BATCH_SIZE:
                if commit_with_retry(batch):
                    uploaded += batch_count
                    print(f'  âœ“ ç¬¬ {current_line} è¡Œ | å·²ä¸Šä¼  {uploaded} æ¡ææ¬¾è®°å½•')

                    progress['contributions_last_line'] = current_line
                    progress['contributions_uploaded'] = uploaded
                    progress['last_updated'] = datetime.utcnow().isoformat()
                    save_progress(progress)

                    time.sleep(MIN_DELAY + random.uniform(0, 2))
                    batch = db.batch()
                    batch_count = 0
                else:
                    progress['contributions_last_line'] = current_line
                    progress['contributions_uploaded'] = uploaded
                    save_progress(progress)
                    return uploaded

    if batch_count > 0:
        if commit_with_retry(batch):
            uploaded += batch_count

    progress['contributions_last_line'] = current_line
    progress['contributions_uploaded'] = uploaded
    progress['contributions_completed'] = True
    progress['last_updated'] = datetime.utcnow().isoformat()
    save_progress(progress)

    print(f'âœ… æˆåŠŸä¸Šä¼  {uploaded} æ¡ææ¬¾è®°å½•')
    return uploaded

def upload_linkages_incremental(year, year_suffix, progress):
    """å¢é‡ä¸Šä¼ linkagesæ•°æ®"""
    collection_name = 'fec_raw_linkages'
    file_path = DATA_DIR / 'linkages' / f'ccl{year_suffix}.txt'

    if not file_path.exists():
        print(f'âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return 0

    print(f'\nğŸ“¤ ä¸Šä¼  Linkages...')
    print(f'  æ–‡ä»¶: {file_path}')

    collection_ref = db.collection(collection_name)
    uploaded = 0
    skipped = 0
    batch = db.batch()
    batch_docs = []  # Store (doc_ref, doc_data) for token refresh
    batch_count = 0
    current_line = progress.get('linkages_last_line', 0)
    start_line = current_line

    with open(file_path, 'r', encoding='latin-1') as f:
        # è·³è¿‡å·²å¤„ç†çš„è¡Œ
        for _ in range(start_line):
            next(f, None)

        for line in f:
            current_line += 1
            fields = line.strip().split('|')

            if len(fields) < 7:
                continue

            candidate_id = fields[0]
            committee_id = fields[3]

            if not candidate_id or not committee_id:
                continue

            doc_id = f'{candidate_id}_{committee_id}_{year}'
            doc_ref = db.collection(collection_name).document(doc_id)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if doc_ref.get().exists:
                skipped += 1
                continue

            doc_data = {
                'candidate_id': candidate_id,
                'candidate_election_year': int(fields[1]) if fields[1] else year,
                'fec_election_year': int(fields[2]) if fields[2] else year,
                'committee_id': committee_id,
                'committee_type': fields[4] if len(fields) > 4 else '',
                'committee_designation': fields[5] if len(fields) > 5 else '',
                'linkage_id': fields[6] if len(fields) > 6 else '',
                'data_year': year,
                'source_file': f'ccl{year_suffix}.txt',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_docs.append((doc_ref, doc_data))
            batch_count += 1

            if batch_count >= BATCH_SIZE:
                if commit_with_retry(batch, batch_docs=batch_docs, collection_ref=collection_ref):
                    uploaded += batch_count
                    print(f'  âœ“ ç¬¬ {current_line} è¡Œ | å·²ä¸Šä¼  {uploaded} æ¡ | è·³è¿‡ {skipped} æ¡')

                    progress['linkages_last_line'] = current_line
                    progress['linkages_uploaded'] = uploaded
                    progress['last_updated'] = datetime.utcnow().isoformat()
                    save_progress(progress)

                    time.sleep(MIN_DELAY + random.uniform(0, 2))
                    batch = db.batch()
                    batch_docs = []
                    batch_count = 0
                else:
                    progress['linkages_last_line'] = current_line
                    progress['linkages_uploaded'] = uploaded
                    save_progress(progress)
                    return uploaded

    if batch_count > 0:
        if commit_with_retry(batch, batch_docs=batch_docs, collection_ref=collection_ref):
            uploaded += batch_count

    progress['linkages_last_line'] = current_line
    progress['linkages_uploaded'] = uploaded
    progress['linkages_completed'] = True
    progress['last_updated'] = datetime.utcnow().isoformat()
    save_progress(progress)

    print(f'âœ… Linkagesä¸Šä¼ å®Œæˆ: {uploaded} æ¡ (è·³è¿‡ {skipped} æ¡)')
    return uploaded

def upload_transfers_incremental(year, year_suffix, progress):
    """å¢é‡ä¸Šä¼ transfersæ•°æ®"""
    collection_name = 'fec_raw_transfers'
    file_path = DATA_DIR / 'transfers' / f'itoth{year_suffix}.txt'

    if not file_path.exists():
        print(f'âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        return 0

    print(f'\nğŸ“¤ ä¸Šä¼  Transfers...')
    print(f'  æ–‡ä»¶: {file_path}')
    print(f'  âš ï¸  æ³¨æ„: è¿™ä¸ªæ–‡ä»¶æœ‰ 1800+ ä¸‡è¡Œï¼Œéœ€è¦å¾ˆé•¿æ—¶é—´!')

    collection_ref = db.collection(collection_name)
    uploaded = 0
    skipped = 0
    batch = db.batch()
    batch_docs = []  # Store (doc_ref, doc_data) for token refresh
    batch_count = 0
    current_line = progress.get('transfers_last_line', 0)
    start_line = current_line
    start_time = time.time()

    if start_line > 0:
        print(f'  ğŸ“ ä»ç¬¬ {start_line:,} è¡Œç»§ç»­ï¼Œè·³è¿‡å·²å¤„ç†çš„è¡Œ...')

    with open(file_path, 'r', encoding='latin-1') as f:
        # è·³è¿‡å·²å¤„ç†çš„è¡Œ
        for _ in range(start_line):
            next(f, None)

        if start_line > 0:
            print(f'  âœ“ å·²è·³è¿‡ {start_line:,} è¡Œï¼Œç»§ç»­ä¸Šä¼ ...')

        for line in f:
            current_line += 1
            fields = line.strip().split('|')

            if len(fields) < 20:
                continue

            committee_id = fields[0]
            transaction_id = fields[16] if len(fields) > 16 else ''
            other_id = fields[15] if len(fields) > 15 else ''

            if not committee_id or not transaction_id:
                continue

            try:
                amount_str = fields[14] if len(fields) > 14 else '0'
                amount = float(amount_str) if amount_str else 0
                amount_cents = int(amount * 100)
            except (ValueError, TypeError):
                amount_cents = 0

            if other_id:
                doc_id = f'{committee_id}_{other_id}_{transaction_id}'
            else:
                doc_id = f'{committee_id}_{transaction_id}_{current_line}'

            doc_ref = db.collection(collection_name).document(doc_id)

            doc_data = {
                'committee_id': committee_id,
                'sender_committee_id': committee_id,
                'receiver_committee_id': other_id if other_id else '',
                'amendment_indicator': fields[1] if len(fields) > 1 else '',
                'report_type': fields[2] if len(fields) > 2 else '',
                'transaction_pgi': fields[3] if len(fields) > 3 else '',
                'image_number': fields[4] if len(fields) > 4 else '',
                'transaction_type': fields[5] if len(fields) > 5 else '',
                'entity_type': fields[6] if len(fields) > 6 else '',
                'name': fields[7] if len(fields) > 7 else '',
                'city': fields[8] if len(fields) > 8 else '',
                'state': fields[9] if len(fields) > 9 else '',
                'zip': fields[10] if len(fields) > 10 else '',
                'employer': fields[11] if len(fields) > 11 else '',
                'occupation': fields[12] if len(fields) > 12 else '',
                'transaction_date': fields[13] if len(fields) > 13 else '',
                'transaction_amount': amount_cents,
                'other_id': other_id,
                'transaction_id': transaction_id,
                'file_number': fields[17] if len(fields) > 17 else '',
                'memo_code': fields[18] if len(fields) > 18 else '',
                'memo_text': fields[19] if len(fields) > 19 else '',
                'sub_id': fields[20] if len(fields) > 20 else '',
                'data_year': year,
                'source_file': f'itoth{year_suffix}.txt',
                'uploaded_at': datetime.utcnow(),
                'last_updated': datetime.utcnow()
            }

            batch.set(doc_ref, doc_data)
            batch_docs.append((doc_ref, doc_data))
            batch_count += 1

            if batch_count >= BATCH_SIZE:
                if commit_with_retry(batch, batch_docs=batch_docs, collection_ref=collection_ref):
                    uploaded += batch_count

                    if (uploaded // BATCH_SIZE) % 10 == 0:
                        elapsed = time.time() - start_time
                        rate = uploaded / elapsed if elapsed > 0 else 0
                        remaining_lines = 18667266 - current_line
                        eta = remaining_lines / rate if rate > 0 else 0

                        print(f'  âœ“ ç¬¬ {current_line:,} è¡Œ | å·²ä¸Šä¼  {uploaded:,} æ¡ | è·³è¿‡ {skipped} æ¡ | '
                              f'{rate:.0f} è¡Œ/ç§’ | ETA: {eta/3600:.1f} å°æ—¶')

                    progress['transfers_last_line'] = current_line
                    progress['transfers_uploaded'] = uploaded
                    progress['last_updated'] = datetime.utcnow().isoformat()
                    save_progress(progress)

                    time.sleep(MIN_DELAY + random.uniform(0, 2))
                    batch = db.batch()
                    batch_docs = []
                    batch_count = 0
                else:
                    progress['transfers_last_line'] = current_line
                    progress['transfers_uploaded'] = uploaded
                    save_progress(progress)
                    return uploaded

    if batch_count > 0:
        if commit_with_retry(batch, batch_docs=batch_docs, collection_ref=collection_ref):
            uploaded += batch_count

    elapsed_total = time.time() - start_time
    progress['transfers_last_line'] = current_line
    progress['transfers_uploaded'] = uploaded
    progress['transfers_completed'] = True
    progress['last_updated'] = datetime.utcnow().isoformat()
    save_progress(progress)

    print(f'âœ… Transfersä¸Šä¼ å®Œæˆ: {uploaded:,} æ¡ (è·³è¿‡ {skipped} æ¡)')
    print(f'   æ€»è€—æ—¶: {elapsed_total/3600:.2f} å°æ—¶')
    return uploaded

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¢é‡ä¸Šä¼ FECæ•°æ®åˆ°Firestore')
    parser.add_argument('--limit', type=int, help='é™åˆ¶ä¸Šä¼ æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼Œä»…å¯¹contributionsç”Ÿæ•ˆï¼‰')
    parser.add_argument('--only', type=str, help='åªä¸Šä¼ æŒ‡å®šçš„è¡¨ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰: committees,candidates,contributions,linkages,transfers')
    args = parser.parse_args()

    # è§£æ --only å‚æ•°
    only_tables = None
    if args.only:
        only_tables = set(args.only.split(','))

    print('\n' + '='*70)
    print('ğŸš€ FECæ•°æ®å¢é‡ä¸Šä¼ ï¼ˆå¸¦è‡ªåŠ¨é‡è¯•ï¼‰')
    print('='*70)

    if args.limit:
        print(f'\nâš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…ä¸Šä¼  {args.limit} æ¡contributionè®°å½•')

    if only_tables:
        print(f'\nğŸ“‹ åªä¸Šä¼ æŒ‡å®šçš„è¡¨: {", ".join(only_tables)}')

    # åŠ è½½è¿›åº¦
    progress = load_progress()
    if progress.get('last_updated'):
        print(f'\nğŸ“ å‘ç°ä¹‹å‰çš„è¿›åº¦ï¼ˆæœ€åæ›´æ–°: {progress["last_updated"]}ï¼‰')
        print(f'   Committees: {progress.get("committees_uploaded", 0)} æ¡å·²ä¸Šä¼ ')
        print(f'   Candidates: {progress.get("candidates_uploaded", 0)} æ¡å·²ä¸Šä¼ ')
        print(f'   Contributions: {progress.get("contributions_uploaded", 0)} æ¡å·²ä¸Šä¼ ')
        print(f'   Linkages: {progress.get("linkages_uploaded", 0)} æ¡å·²ä¸Šä¼ ')
        print(f'   Transfers: {progress.get("transfers_uploaded", 0)} æ¡å·²ä¸Šä¼ ')

    init_firestore()

    if not DATA_DIR.exists():
        print(f'\nâŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}')
        sys.exit(1)

    year, year_suffix = 2024, '24'

    print(f'\nå¤„ç† {year} å¹´æ•°æ®')
    print('='*70)

    # ä¸Šä¼ å§”å‘˜ä¼šæ•°æ®
    if not only_tables or 'committees' in only_tables:
        if not progress.get('committees_completed'):
            committees_count = upload_committees_incremental(year, year_suffix, progress)
            print(f'\nâœ“ å§”å‘˜ä¼šæ•°æ®: å·²ä¸Šä¼  {committees_count} æ¡')
        else:
            print(f'\nâœ“ å§”å‘˜ä¼šæ•°æ®å·²å®Œæˆï¼ˆ{progress.get("committees_uploaded", 0)} æ¡ï¼‰')
    else:
        print(f'\nâ­ï¸  è·³è¿‡å§”å‘˜ä¼šæ•°æ®ï¼ˆä½¿ç”¨ --only å‚æ•°ï¼‰')

    # ä¸Šä¼ å€™é€‰äººæ•°æ®
    if not only_tables or 'candidates' in only_tables:
        if not progress.get('candidates_completed'):
            candidates_count = upload_candidates_incremental(year, year_suffix, progress)
            print(f'\nâœ“ å€™é€‰äººæ•°æ®: å·²ä¸Šä¼  {candidates_count} æ¡')
        else:
            print(f'\nâœ“ å€™é€‰äººæ•°æ®å·²å®Œæˆï¼ˆ{progress.get("candidates_uploaded", 0)} æ¡ï¼‰')
    else:
        print(f'\nâ­ï¸  è·³è¿‡å€™é€‰äººæ•°æ®ï¼ˆä½¿ç”¨ --only å‚æ•°ï¼‰')

    # ä¸Šä¼ ææ¬¾æ•°æ®
    if not only_tables or 'contributions' in only_tables:
        if not progress.get('contributions_completed'):
            contributions_count = upload_contributions_incremental(year, year_suffix, progress, limit=args.limit)
            print(f'\nâœ“ ææ¬¾æ•°æ®: å·²ä¸Šä¼  {contributions_count} æ¡')
            if args.limit and contributions_count >= args.limit:
                print(f'   âš ï¸  å·²è¾¾åˆ°æµ‹è¯•é™åˆ¶ï¼Œæœªæ ‡è®°ä¸ºå®Œæˆ')
        else:
            print(f'\nâœ“ ææ¬¾æ•°æ®å·²å®Œæˆï¼ˆ{progress.get("contributions_uploaded", 0)} æ¡ï¼‰')
    else:
        print(f'\nâ­ï¸  è·³è¿‡ææ¬¾æ•°æ®ï¼ˆä½¿ç”¨ --only å‚æ•°ï¼‰')

    # ä¸Šä¼ linkagesæ•°æ®
    if not only_tables or 'linkages' in only_tables:
        if not progress.get('linkages_completed'):
            linkages_count = upload_linkages_incremental(year, year_suffix, progress)
            print(f'\nâœ“ Linkagesæ•°æ®: å·²ä¸Šä¼  {linkages_count} æ¡')
        else:
            print(f'\nâœ“ Linkagesæ•°æ®å·²å®Œæˆï¼ˆ{progress.get("linkages_uploaded", 0)} æ¡ï¼‰')
    else:
        print(f'\nâ­ï¸  è·³è¿‡Linkagesæ•°æ®ï¼ˆä½¿ç”¨ --only å‚æ•°ï¼‰')

    # ä¸Šä¼ transfersæ•°æ®
    if not only_tables or 'transfers' in only_tables:
        if not progress.get('transfers_completed'):
            transfers_count = upload_transfers_incremental(year, year_suffix, progress)
            print(f'\nâœ“ Transfersæ•°æ®: å·²ä¸Šä¼  {transfers_count} æ¡')
        else:
            print(f'\nâœ“ Transfersæ•°æ®å·²å®Œæˆï¼ˆ{progress.get("transfers_uploaded", 0)} æ¡ï¼‰')
    else:
        print(f'\nâ­ï¸  è·³è¿‡Transfersæ•°æ®ï¼ˆä½¿ç”¨ --only å‚æ•°ï¼‰')

    print('\n' + '='*70)
    print('âœ… ä¸Šä¼ å®Œæˆï¼')
    print('='*70)
    print('\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:')
    print(f'  Committees: {progress.get("committees_uploaded", 0)} æ¡')
    print(f'  Candidates: {progress.get("candidates_uploaded", 0)} æ¡')
    print(f'  Contributions: {progress.get("contributions_uploaded", 0)} æ¡')
    print(f'  Linkages: {progress.get("linkages_uploaded", 0)} æ¡')
    print(f'  Transfers: {progress.get("transfers_uploaded", 0)} æ¡')
    print(f'\nğŸ’¡ è¿›åº¦æ–‡ä»¶: {PROGRESS_FILE}')
    print()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œè¿›åº¦å·²ä¿å­˜')
        print(f'   å¯ä»¥é‡æ–°è¿è¡Œæ­¤è„šæœ¬ç»§ç»­ä¸Šä¼ ')
        sys.exit(0)
    except Exception as e:
        print(f'\nâŒ é”™è¯¯: {e}')
        import traceback
        traceback.print_exc()
        sys.exit(1)
