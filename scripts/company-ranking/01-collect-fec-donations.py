#!/usr/bin/env python3
"""
FEC Political Donations Data Collection for Company Rankings

ä» Firebase FEC æ•°æ®åº“æ„å»º SP500 å…¬å¸æ”¿æ²»ææ¬¾æŠ¥å‘Š
è¾“å‡º: Firebase `company_rankings/{ticker}/fec_data`

è¿è¡Œæ–¹å¼:
    python3 01-collect-fec-donations.py

æ•°æ®ç»“æ„:
    - è¯»å–: fec_company_party_summary (æ¯å®¶å…¬å¸çš„å¹´åº¦/æ”¿å…šæ±‡æ€»æ•°æ®)
    - å†™å…¥: company_rankings/{ticker}/fec_data
"""

import os
import sys
import json
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import firebase_admin
from firebase_admin import credentials, firestore
from google.cloud.firestore_v1.base_query import FieldFilter
import time
import requests

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° pathï¼Œä»¥ä¾¿å¯¼å…¥ SP500 å…¬å¸åˆ—è¡¨
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(SCRIPT_DIR))

# å¯¼å…¥é‚®ä»¶é€šçŸ¥æ¨¡å—
sys.path.insert(0, SCRIPT_DIR)
from email_notifier import log_completion_notification, send_success_email

# Import from unified data module
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from data.sp500Companies import SP500_TICKERS, TICKER_TO_NAME, get_company_name

# Company name mapping is now loaded dynamically from fec_company_name_variants collection
# This allows AI-generated variants and ticker-based search without hardcoding

class FECDonationCollector:
    """FEC ææ¬¾æ•°æ®é‡‡é›†å™¨"""

    def __init__(self, credentials_path: Optional[str] = None):
        """åˆå§‹åŒ– Firebase è¿æ¥"""
        if not firebase_admin._apps:
            if credentials_path:
                cred = credentials.Certificate(credentials_path)
            else:
                # ä½¿ç”¨é»˜è®¤å‡­è¯ï¼ˆCloud Run ç¯å¢ƒï¼‰
                cred = credentials.ApplicationDefault()

            firebase_admin.initialize_app(cred, {
                'projectId': 'stanseproject'
            })

        self.db = firestore.client()
        print(f"âœ… Firebase initialized (project: stanseproject)")

        # åˆå§‹åŒ– Gemini API (ç”¨äº AI éªŒè¯)
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')

        # å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ï¼Œä» Google Secret Manager è·å–
        if not self.gemini_api_key:
            try:
                print("ğŸ”‘ Loading GEMINI_API_KEY from Secret Manager...")
                result = subprocess.run(
                    [
                        'gcloud', 'secrets', 'versions', 'access', 'latest',
                        '--secret', 'gemini-api-key',
                        '--project', 'gen-lang-client-0960644135'
                    ],
                    capture_output=True,
                    text=True,
                    check=True
                )
                self.gemini_api_key = result.stdout.strip()
                print("âœ… GEMINI_API_KEY loaded from Secret Manager")
            except Exception as e:
                print(f"âš ï¸  Failed to load GEMINI_API_KEY from Secret Manager: {e}")

        self.gemini_api_url = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent'

        if self.gemini_api_key:
            print(f"âœ… Gemini 2.5 Flash API initialized for AI verification")
        else:
            print(f"âš ï¸  Gemini API key not found, AI verification disabled")

        # åˆå§‹åŒ–æœç´¢å…³é”®è¯ç¼“å­˜ï¼ˆä» fec_company_name_variants åŠ è½½ï¼‰
        self._search_keywords_cache = {}

        # åˆå§‹åŒ– ticker åˆ° company_name æ˜ å°„ç¼“å­˜
        self._ticker_to_name_cache = {}

    def _get_company_name_from_db(self, ticker: str) -> str:
        """ä»ç»Ÿä¸€æ•°æ®æºè·å–å…¬å¸åç§°"""
        # ä½¿ç”¨ç»Ÿä¸€çš„ SP500 æ•°æ®æº
        company_name = get_company_name(ticker)
        if company_name:
            print(f"  ğŸ“ Resolved {ticker} â†’ {company_name}")
            return company_name
        # Fallback: è¿”å› ticker æœ¬èº«
        print(f"  âš ï¸  No name found for {ticker}, using ticker as name")
        return ticker

    def normalize_company_name(self, name: str) -> str:
        """
        æ ‡å‡†åŒ–å…¬å¸åç§°ï¼ˆä¸ fecService.ts é€»è¾‘ä¸€è‡´ï¼‰
        """
        if not name:
            return ''

        normalized = name.lower().strip()

        # ç§»é™¤å¸¸è§åç¼€
        suffixes = [
            'corporation', 'corp', 'inc', 'incorporated', 'company', 'co',
            'llc', 'lp', 'ltd', 'limited', 'political action committee', 'pac'
        ]

        for suffix in suffixes:
            import re
            pattern = r'\b' + suffix + r'\b\.?'
            normalized = re.sub(pattern, '', normalized, flags=re.IGNORECASE)

        # ç§»é™¤æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼
        normalized = re.sub(r'[^\w\s]', '', normalized)
        normalized = re.sub(r'\s+', ' ', normalized).strip()

        return normalized

    def verify_company_match_with_ai(self, ticker: str, company_name: str, candidate_name: str) -> Tuple[bool, str]:
        """
        ä½¿ç”¨ AI éªŒè¯å€™é€‰å…¬å¸æ˜¯å¦æ˜¯ç›®æ ‡å…¬å¸

        Args:
            ticker: è‚¡ç¥¨ä»£ç  (å¦‚ 'AAPL')
            company_name: ç›®æ ‡å…¬å¸å (å¦‚ 'Apple')
            candidate_name: å€™é€‰å…¬å¸å (å¦‚ 'us apple association')

        Returns:
            (is_match: bool, reason: str)
        """
        if not self.gemini_api_key:
            # å¦‚æœæ²¡æœ‰ API keyï¼Œé»˜è®¤æ¥å—ï¼ˆfallback åˆ°æ—§è¡Œä¸ºï¼‰
            return (True, "AI verification disabled")

        # æ„å»º prompt
        prompt = f"""You are a company name matching expert. Your task is to determine if a candidate company name matches a target company.

Target Company:
- Stock Ticker: {ticker}
- Company Name: {company_name}

Candidate Company Name: {candidate_name}

Question: Is "{candidate_name}" the same as "{company_name}" (ticker: {ticker}), or a PAC/subsidiary/affiliate directly associated with {company_name}?

Answer with ONLY one of these responses:
- YES: If it's the same company or directly affiliated
- NO: If it's a completely different company

Provide your reasoning in one sentence after your answer.

Format:
Answer: YES/NO
Reason: <one sentence explanation>"""

        # Call Gemini API
        try:
            request_body = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.1,  # ä½æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
                    "maxOutputTokens": 200,
                }
            }

            response = requests.post(
                f"{self.gemini_api_url}?key={self.gemini_api_key}",
                json=request_body,
                headers={"Content-Type": "application/json"},
                timeout=10
            )

            if response.status_code != 200:
                print(f"  âš ï¸  AI verification API error: {response.status_code}")
                return (True, f"API error {response.status_code}, accepting match")

            result = response.json()

            # æå– AI å“åº”
            try:
                text = result['candidates'][0]['content']['parts'][0]['text']

                # è§£æå“åº”
                lines = text.strip().split('\n')
                answer_line = None
                reason_line = None

                for line in lines:
                    if 'Answer:' in line:
                        answer_line = line
                    elif 'Reason:' in line:
                        reason_line = line

                if answer_line:
                    is_match = 'YES' in answer_line.upper()
                    reason = reason_line.split(':', 1)[1].strip() if reason_line else text
                    return (is_match, reason)
                else:
                    # Fallback: å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å« YES/NO
                    is_match = 'YES' in text.upper() and 'NO' not in text.upper()
                    return (is_match, text[:100])

            except (KeyError, IndexError) as e:
                print(f"  âš ï¸  AI response parsing error: {e}")
                return (True, "Parse error, accepting match")

        except requests.exceptions.Timeout:
            print(f"  âš ï¸  AI verification timeout")
            return (True, "Timeout, accepting match")
        except Exception as e:
            print(f"  âš ï¸  AI verification error: {str(e)}")
            return (True, f"Error: {str(e)}, accepting match")

    def _get_company_name_from_db(self, ticker: str) -> str:
        """
        ä» fec_company_name_variants collection åŠ è½½ ticker å¯¹åº”çš„å…¬å¸åç§°
        ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æŸ¥è¯¢

        è¿”å›: company_full_name æˆ– company_name, fallback åˆ° ticker
        """
        # æ£€æŸ¥ç¼“å­˜
        if ticker in self._ticker_to_name_cache:
            return self._ticker_to_name_cache[ticker]

        # ä»æ•°æ®åº“åŠ è½½
        try:
            # æŸ¥è¯¢è¯¥ ticker çš„æ–‡æ¡£
            docs = self.db.collection('fec_company_name_variants').where('ticker', '==', ticker).limit(1).stream()

            for doc in docs:
                data = doc.to_dict()
                # ä¼˜å…ˆä½¿ç”¨ company_full_nameï¼Œfallback åˆ° company_name
                company_name = data.get('company_full_name') or data.get('company_name', ticker)

                # ç¼“å­˜ç»“æœ
                self._ticker_to_name_cache[ticker] = company_name
                return company_name

            # æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ ticker ä½œä¸º fallback
            self._ticker_to_name_cache[ticker] = ticker
            return ticker

        except Exception as e:
            print(f"  âš ï¸  Error loading company name for {ticker}: {e}")
            return ticker

    def _load_search_keywords_from_db(self, ticker: str) -> List[str]:
        """
        ä» fec_company_name_variants collection åŠ è½½æŒ‡å®š ticker çš„æœç´¢å…³é”®è¯
        ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æŸ¥è¯¢

        æ–°æ•°æ®ç»“æ„: æ¯ä¸ªå…¬å¸ä¸€ä¸ªæ–‡æ¡£,variants æ˜¯æ•°ç»„
        {
            'normalized_name': 'att',
            'ticker': 'T',
            'variants': [
                {'variant_name': 'AT&T INC.', 'variant_name_lower': 'at&t inc.'},
                {'variant_name': 'att', 'variant_name_lower': 'att'}
            ]
        }
        """
        # æ£€æŸ¥ç¼“å­˜
        if ticker in self._search_keywords_cache:
            return self._search_keywords_cache[ticker]

        # ä»æ•°æ®åº“åŠ è½½
        try:
            # æŸ¥è¯¢è¯¥ ticker çš„æ–‡æ¡£
            docs = self.db.collection('fec_company_name_variants').where('ticker', '==', ticker).stream()

            keywords = []
            for doc in docs:
                data = doc.to_dict()
                variants = data.get('variants', [])

                # ä» variants æ•°ç»„ä¸­æå–æ‰€æœ‰ variant_name_lower
                for variant in variants:
                    variant_lower = variant.get('variant_name_lower', '').strip()
                    if variant_lower and variant_lower not in keywords:
                        keywords.append(variant_lower)

            # ç¼“å­˜ç»“æœ
            self._search_keywords_cache[ticker] = keywords

            if keywords:
                print(f"  ğŸ“– Loaded {len(keywords)} search keywords from database for {ticker}")
            return keywords

        except Exception as e:
            print(f"  âš ï¸  Error loading search keywords for {ticker}: {e}")
            return []

    def find_variants_by_fuzzy_search(self, ticker: str, company_name: str) -> List[str]:
        """
        ç›´æ¥åœ¨ fec_company_consolidated ä¸­é€šè¿‡å…¬å¸åæ¨¡ç³ŠåŒ¹é…æŸ¥æ‰¾æ–‡æ¡£
        æœç´¢æ¯ä¸ªæ–‡æ¡£çš„ normalized_name å­—æ®µï¼ˆè€Œä¸æ˜¯æ–‡æ¡£ IDï¼‰
        è¿”å›åŒ¹é…çš„ normalized_name åˆ—è¡¨

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æœç´¢æ–‡æ¡£çš„ normalized_name å­—æ®µï¼ˆä¸æ˜¯æ–‡æ¡£ IDï¼‰
        2. è¦æ±‚å…³é”®è¯ä½œä¸ºç‹¬ç«‹å•è¯å‡ºç°ï¼ˆword boundaryï¼‰
        3. æœ€å°å…³é”®è¯é•¿åº¦ >= 3 å­—ç¬¦ï¼ˆé¿å…å•å­—æ¯è¯¯åŒ¹é…ï¼‰
        4. ä½¿ç”¨ AI éªŒè¯æœ€ç»ˆåŒ¹é…
        """
        import re

        # ä»æ•°æ®åº“åŠ è½½æœç´¢å…³é”®è¯ï¼ˆå¸¦ç¼“å­˜ï¼‰
        search_keywords = self._load_search_keywords_from_db(ticker)

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ï¼Œä½¿ç”¨å…¬å¸åä½œä¸ºåå¤‡
        if not search_keywords:
            search_keywords = [company_name.lower()]
            print(f"  âš ï¸  No search keywords found in database for {ticker}, using company name: {company_name}")

        # è¿‡æ»¤æ‰å¤ªçŸ­çš„å…³é”®è¯ï¼ˆ< 3 å­—ç¬¦ï¼‰ï¼Œé¿å…è¯¯åŒ¹é…
        search_keywords = [kw for kw in search_keywords if len(kw) >= 3]

        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨å…¬å¸å
        if not search_keywords:
            search_keywords = [company_name.lower()]

        # è·å–æ‰€æœ‰ fec_company_consolidated æ–‡æ¡£
        # é‡è¦ï¼šæˆ‘ä»¬éœ€è¦è¯»å–æ–‡æ¡£çš„ normalized_name å­—æ®µï¼Œè€Œä¸æ˜¯æ–‡æ¡£ ID
        all_docs = self.db.collection('fec_company_consolidated').stream()

        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡æ¡£ (with AI verification)
        matched_normalized_names = set()
        rejected_candidates = []  # è®°å½•è¢« AI æ‹’ç»çš„å€™é€‰
        total_checked = 0

        for doc in all_docs:
            data = doc.to_dict()
            normalized_name = data.get('normalized_name', '')

            if not normalized_name:
                continue

            normalized_name_lower = normalized_name.lower()

            # åœ¨ normalized_name å­—æ®µä¸­æœç´¢å…³é”®è¯
            for keyword in search_keywords:
                # ä½¿ç”¨ word boundary ç¡®ä¿å…³é”®è¯æ˜¯ç‹¬ç«‹å•è¯
                # \b ç¡®ä¿ "visa" ä¸ä¼šåŒ¹é… "television"
                pattern = r'\b' + re.escape(keyword.lower()) + r'\b'

                if re.search(pattern, normalized_name_lower):
                    total_checked += 1

                    # ğŸ¤– AI éªŒè¯: ç¡®è®¤è¿™ä¸ªå€™é€‰å…¬å¸æ˜¯å¦çœŸçš„æ˜¯ç›®æ ‡å…¬å¸
                    is_match, reason = self.verify_company_match_with_ai(
                        ticker, company_name, normalized_name
                    )

                    if is_match:
                        print(f"  âœ… AI verified: '{normalized_name}' matches {ticker}")
                        print(f"     Reason: {reason}")
                        matched_normalized_names.add(normalized_name)
                    else:
                        print(f"  âŒ AI rejected: '{normalized_name}' does NOT match {ticker}")
                        print(f"     Reason: {reason}")
                        rejected_candidates.append((normalized_name, reason))

                    break  # æ‰¾åˆ°å…³é”®è¯åŒ¹é…åï¼Œè·³è¿‡è¯¥æ–‡æ¡£çš„å…¶ä»–å…³é”®è¯æ£€æŸ¥

        # è¾“å‡ºè¢«æ‹’ç»çš„å€™é€‰ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if rejected_candidates:
            print(f"  ğŸ“‹ AI rejected {len(rejected_candidates)} candidates for {ticker}:")
            for candidate, reason in rejected_candidates[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"     - {candidate}: {reason}")

        print(f"  ğŸ“Š Fuzzy search stats: {total_checked} candidates checked, {len(matched_normalized_names)} accepted")

        return list(matched_normalized_names)

    def find_company_variants(self, ticker: str) -> List[str]:
        """
        æŸ¥æ‰¾å…¬å¸çš„æ‰€æœ‰å˜ä½“åç§°
        """
        # ä» ticker è·å–å…¬å¸å (ä»æ•°æ®åº“åŠ è½½)
        company_name = self._get_company_name_from_db(ticker)
        normalized = self.normalize_company_name(company_name)

        # æŸ¥è¯¢ fec_company_name_variants collection
        variants_ref = self.db.collection('fec_company_name_variants')

        # å°è¯•ç›´æ¥åŒ¹é…
        canonical_upper = company_name.upper()
        doc_ref = variants_ref.document(canonical_upper)
        doc = doc_ref.get()

        if doc.exists:
            data = doc.to_dict()
            variants = data.get('variants', [])
            # Extract variant_name_lower from dict objects
            variant_names = []
            for variant in variants:
                if isinstance(variant, dict):
                    variant_names.append(variant.get('variant_name_lower', ''))
                elif isinstance(variant, str):
                    variant_names.append(variant.lower())
            print(f"  â””â”€ Found {len(variant_names)} variants for {ticker} ({company_name})")
            return variant_names

        # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³Šæœç´¢
        # æŸ¥è¯¢æ‰€æœ‰åŒ…å«å…¬å¸åå…³é”®è¯çš„æ–‡æ¡£
        all_docs = variants_ref.stream()
        matched_variants = []

        for doc in all_docs:
            doc_id = doc.id
            data = doc.to_dict()

            if normalized in self.normalize_company_name(doc_id):
                variants = data.get('variants', [])
                matched_variants.extend(variants)

        if matched_variants:
            # Extract variant_name_lower from each dict and deduplicate
            variant_names = []
            for variant_dict in matched_variants:
                if isinstance(variant_dict, dict):
                    name_lower = variant_dict.get('variant_name_lower', '')
                    if name_lower and name_lower not in variant_names:
                        variant_names.append(name_lower)
                elif isinstance(variant_dict, str):
                    # Handle legacy string format
                    name_lower = variant_dict.lower()
                    if name_lower and name_lower not in variant_names:
                        variant_names.append(name_lower)
            print(f"  â””â”€ Found {len(variant_names)} variants via fuzzy match for {ticker}")
            return variant_names

        # Fallback: ç›´æ¥åœ¨ fec_company_consolidated ä¸­æœç´¢å…¬å¸å
        print(f"  â””â”€ No variants found, trying direct fuzzy search in fec_company_consolidated...")
        fuzzy_variants = self.find_variants_by_fuzzy_search(ticker, company_name)
        if fuzzy_variants:
            print(f"  â””â”€ Found {len(fuzzy_variants)} matches via direct fuzzy search")
            return fuzzy_variants

        print(f"  â””â”€ No FEC data found for {ticker} ({company_name})")
        return []  # è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®

    def collect_fec_for_company(self, ticker: str) -> Optional[Dict[str, Any]]:
        """
        æŸ¥è¯¢ FEC æ•°æ®åº“ï¼Œæ±‡æ€»è¯¥å…¬å¸çš„ææ¬¾æƒ…å†µ

        æŸ¥è¯¢ fec_company_party_summary collectionï¼Œè¯¥ collection çš„æ•°æ®ç»“æ„ï¼š
        - normalized_name: å…¬å¸æ ‡å‡†åŒ–åç§°
        - data_year: æ•°æ®å¹´ä»½
        - party_totals: {DEM: {...}, REP: {...}, OTH: {...}}
        - total_contributed: æ€»ææ¬¾é‡‘é¢ï¼ˆcentsï¼‰
        """
        print(f"\nğŸ“Š Processing {ticker}...")

        # æŸ¥æ‰¾å…¬å¸å˜ä½“
        variants = self.find_company_variants(ticker)

        if not variants:
            print(f"  â””â”€ No data found, skipping")
            return None

        # åˆå§‹åŒ–æ±‡æ€»æ•°æ®
        party_totals = {
            'DEM': {'total_amount': 0, 'count': 0},
            'REP': {'total_amount': 0, 'count': 0},
            'OTH': {'total_amount': 0, 'count': 0}
        }
        years_found = set()

        # Normalize variants for querying (convert to lowercase, remove spaces)
        import re
        normalized_variants = []
        for variant in variants:
            # ä½¿ç”¨ä¸ fecService.ts ç›¸åŒçš„æ ‡å‡†åŒ–é€»è¾‘
            normalized = variant.lower().strip()
            suffixes = [
                'corporation', 'corp', 'inc', 'incorporated', 'company', 'co',
                'llc', 'lp', 'ltd', 'limited', 'political action committee', 'pac'
            ]
            for suffix in suffixes:
                pattern = r'\b' + suffix + r'\b\.?'
                normalized = re.sub(pattern, '', normalized, flags=re.IGNORECASE)
            normalized = re.sub(r'[^\w\s]', '', normalized)
            normalized = re.sub(r'\s+', ' ', normalized).strip()
            if normalized:
                normalized_variants.append(normalized)

        print(f"  â”œâ”€ Normalized variants: {len(normalized_variants)}")

        # æŸ¥è¯¢ fec_company_consolidated collection (åŒ…å« linkage + PAC transfer data)
        summary_ref = self.db.collection('fec_company_consolidated')

        # å¯¹æ¯ä¸ªæ ‡å‡†åŒ–çš„å˜ä½“ï¼ŒæŸ¥è¯¢å…¶æ‰€æœ‰æ•°æ®
        for normalized_variant in normalized_variants:
            query = summary_ref.where(filter=FieldFilter('normalized_name', '==', normalized_variant))
            docs = query.stream()

            for doc in docs:
                data = doc.to_dict()
                year = data.get('data_year')  # æ³¨æ„å­—æ®µåæ˜¯ data_year
                party_data = data.get('party_totals', {})  # party_totals æ˜¯ä¸€ä¸ªå¯¹è±¡

                if year:
                    years_found.add(year)

                # èšåˆ party_totals ä¸­çš„æ•°æ®
                for party, amounts in party_data.items():
                    if party not in party_totals:
                        party_totals[party] = {'total_amount': 0, 'count': 0}

                    party_totals[party]['total_amount'] += amounts.get('total_amount', 0)
                    party_totals[party]['count'] += amounts.get('contribution_count', 0)

        # ç¡®ä¿è‡³å°‘æœ‰ DEM, REP, OTH ä¸‰ä¸ªæ”¿å…š
        for party in ['DEM', 'REP', 'OTH']:
            if party not in party_totals:
                party_totals[party] = {'total_amount': 0, 'count': 0}

        # è®¡ç®—æ€»é¢å’Œç™¾åˆ†æ¯”
        total_cents = sum(p['total_amount'] for p in party_totals.values())
        total_usd = total_cents / 100

        if total_usd == 0:
            print(f"  â””â”€ No donation data found")
            return None

        # è®¡ç®—ç™¾åˆ†æ¯”
        for party in party_totals:
            party_totals[party]['percentage'] = (
                party_totals[party]['total_amount'] / total_cents * 100
                if total_cents > 0 else 0
            )
            party_totals[party]['total_amount_usd'] = party_totals[party]['total_amount'] / 100

        # è®¡ç®—æ”¿æ²»å€¾å‘åˆ†æ•° (-100 to 100)
        # -100 = 100% REP, 0 = å¹³è¡¡, +100 = 100% DEM
        dem_pct = party_totals['DEM']['percentage']
        rep_pct = party_totals['REP']['percentage']
        political_lean_score = dem_pct - rep_pct

        result = {
            'ticker': ticker,
            'display_name': self._get_company_name_from_db(ticker),
            'variants_found': variants,
            'normalized_variants': normalized_variants,
            'party_totals': party_totals,
            'total_contributed_cents': int(total_cents),
            'total_usd': total_usd,
            'political_lean_score': round(political_lean_score, 2),
            'years': sorted(list(years_found), reverse=True),
            'last_updated': firestore.SERVER_TIMESTAMP,
            'data_source': 'fec_company_consolidated'  # åŒ…å« linkage + PAC transfer data
        }

        print(f"  â”œâ”€ Total: ${total_usd:,.0f}")
        print(f"  â”œâ”€ DEM: {dem_pct:.1f}% | REP: {rep_pct:.1f}% | OTH: {party_totals['OTH']['percentage']:.1f}%")
        print(f"  â””â”€ Political Lean: {political_lean_score:+.1f} ({'DEM' if political_lean_score > 0 else 'REP' if political_lean_score < 0 else 'BALANCED'})")

        return result

    def save_to_firebase(self, ticker: str, data: Dict[str, Any]):
        """
        ä¿å­˜åˆ° company_rankings_by_ticker collectionï¼Œå¹¶ä¿å­˜å†å²ç‰ˆæœ¬

        ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥ï¼ˆç»Ÿä¸€æ–¹æ¡ˆï¼‰ï¼š
        1. æ„å»ºå®Œæ•´çš„æ–°æ•°æ®ï¼ˆfec_data + ticker + last_updatedï¼‰
        2. å…ˆå°†æ–°æ•°æ®ä¿å­˜åˆ° history/{å½“å‰æ—¶é—´æˆ³} ä½œä¸ºå¿«ç…§
        3. ç„¶åç”¨æ–°æ•°æ®æ›´æ–°ä¸»æ–‡æ¡£ï¼ˆmerge=True ä¿ç•™å…¶ä»–è„šæœ¬æ·»åŠ çš„å­—æ®µï¼‰

        ç»“æœï¼š
        - history é‡Œçš„æœ€æ–°ç‰ˆæœ¬ = ä¸»æ–‡æ¡£å½“å‰å†…å®¹ï¼ˆæœ‰é‡å æ˜¯æ­£å¸¸çš„ï¼‰
        - ä¸»æ–‡æ¡£æ°¸è¿œæ˜¯æœ€æ–°æ•°æ®
        - merge=True ç¡®ä¿ ESG/News ç­‰å…¶ä»–è„šæœ¬æ·»åŠ çš„å­—æ®µä¸è¢«è¦†ç›–

        æ•°æ®ç»“æ„:
        - ä¸»æ–‡æ¡£: company_rankings_by_ticker/{ticker}
          - fec_data: {...}  (FEC æ•°æ®çš„å®Œæ•´ç»“æ„)
          - ticker: "AAPL"
          - last_updated: timestamp
          - (å¯èƒ½è¿˜æœ‰å…¶ä»–å­—æ®µå¦‚ esg_data, news_data ç­‰)

        - å†å²ç‰ˆæœ¬: company_rankings_by_ticker/{ticker}/history/{YYYYmmdd_HHMMSS}
          - fec_data: {...}  (è¯¥æ—¶åˆ»çš„ FEC æ•°æ®å¿«ç…§)
          - ticker: "AAPL"
          - last_updated: timestamp
          - (è¯¥æ—¶åˆ»çš„æ‰€æœ‰å…¶ä»–å­—æ®µ)
        """
        # 1. ç”Ÿæˆæ—¶é—´æˆ³
        now = datetime.now()
        timestamp_str = now.strftime('%Y%m%d_%H%M%S')

        # 2. æ„å»ºå®Œæ•´çš„æ–°æ•°æ®
        new_data = {
            'fec_data': data,  # ä¿ç•™å®Œæ•´çš„ FEC æ•°æ®ç»“æ„ï¼ˆåŒ…å«æ‰€æœ‰åµŒå¥—å­—æ®µï¼‰
            'ticker': ticker,
            'last_updated': firestore.SERVER_TIMESTAMP
        }

        doc_ref = self.db.collection('company_rankings_by_ticker').document(ticker)

        # 3. å…ˆä¿å­˜æ–°æ•°æ®åˆ° historyï¼ˆä½œä¸ºå½“å‰æ—¶åˆ»çš„å¿«ç…§ï¼‰
        history_ref = doc_ref.collection('history').document(timestamp_str)
        history_ref.set(new_data)
        print(f"  ğŸ“¦ Saved to history: {timestamp_str}")

        # 4. ç„¶åæ›´æ–°ä¸»æ–‡æ¡£ï¼ˆmerge=True ä¿ç•™å…¶ä»–å­—æ®µï¼‰
        doc_ref.set(new_data, merge=True)

        print(f"  âœ… Saved to Firebase: company_rankings_by_ticker/{ticker}/fec_data")

    def run(self, tickers_to_process: Optional[List[str]] = None):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®é‡‡é›†æµç¨‹

        å‚æ•°:
            tickers_to_process: å¯é€‰çš„ ticker åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™åªå¤„ç†è¿™äº›å…¬å¸
        """
        start_time = time.time()

        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šçš„ tickersï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™ä½¿ç”¨å®Œæ•´åˆ—è¡¨
        tickers = tickers_to_process if tickers_to_process else SP500_TICKERS

        print(f"\n{'='*60}")
        print(f"ğŸ”„ FEC Political Donations Data Collection")
        print(f"{'='*60}")
        print(f"ğŸ“¦ Total companies to process: {len(tickers)}")
        if tickers_to_process:
            print(f"ğŸ“‹ Processing specific tickers (retry mode)")
        print(f"ğŸ•’ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        success_count = 0
        no_data_count = 0
        error_count = 0
        failed_tickers = []

        for i, ticker in enumerate(tickers, 1):
            try:
                print(f"\n[{i}/{len(tickers)}] {ticker}", end='')

                fec_data = self.collect_fec_for_company(ticker)

                if fec_data:
                    self.save_to_firebase(ticker, fec_data)
                    success_count += 1
                else:
                    no_data_count += 1

            except Exception as e:
                print(f"  âŒ Error: {str(e)}")
                error_count += 1
                failed_tickers.append(ticker)

        execution_time = time.time() - start_time

        print(f"\n\n{'='*60}")
        print(f"âœ… FEC Data Collection Complete")
        print(f"{'='*60}")
        print(f"âœ… Success: {success_count}/{len(SP500_TICKERS)}")
        print(f"âš ï¸  No Data: {no_data_count}/{len(SP500_TICKERS)}")
        print(f"âŒ Errors: {error_count}/{len(SP500_TICKERS)}")
        print(f"ğŸ•’ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")

        # å‘é€å®Œæˆé€šçŸ¥ï¼ˆæ—¥å¿—æ–¹å¼ï¼‰
        log_completion_notification(
            job_name="FEC Donations Collector",
            total_companies=len(SP500_TICKERS),
            successful_companies=success_count,
            failed_companies=failed_tickers,
            execution_time_seconds=execution_time
        )

        # å‘é€é‚®ä»¶é€šçŸ¥
        send_success_email(
            job_name="FEC Donations Collector",
            total_companies=len(tickers),
            successful_companies=success_count,
            failed_companies=failed_tickers,
            execution_time_seconds=execution_time
        )


def main():
    """ä¸»å‡½æ•°"""
    credentials_path = os.getenv('FIREBASE_CREDENTIALS_PATH')
    tickers_file = None
    tickers_to_process = None

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # å¦‚æœç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ .txt æ–‡ä»¶ï¼Œè®¤ä¸ºæ˜¯ ticker åˆ—è¡¨æ–‡ä»¶
        if sys.argv[1].endswith('.txt'):
            tickers_file = sys.argv[1]
        # å¦‚æœç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ .json æ–‡ä»¶ï¼Œè®¤ä¸ºæ˜¯ credentials æ–‡ä»¶
        elif sys.argv[1].endswith('.json'):
            credentials_path = sys.argv[1]
        else:
            # å…¶ä»–æƒ…å†µä¹Ÿå°è¯•ä½œä¸º credentialsï¼ˆå‘åå…¼å®¹ï¼‰
            credentials_path = sys.argv[1]

    # å¦‚æœæä¾›äº† ticker æ–‡ä»¶ï¼Œè¯»å–å®ƒ
    if tickers_file:
        try:
            with open(tickers_file, 'r') as f:
                # è¯»å–æ‰€æœ‰éç©ºè¡Œï¼Œå»é™¤ç©ºæ ¼å’Œæ¢è¡Œç¬¦
                tickers_to_process = [
                    line.strip()
                    for line in f
                    if line.strip() and not line.strip().startswith('#')
                ]
            print(f"ğŸ“‹ Loaded {len(tickers_to_process)} tickers from {tickers_file}")
        except FileNotFoundError:
            print(f"âŒ ERROR: Ticker file not found: {tickers_file}")
            sys.exit(1)

    collector = FECDonationCollector(credentials_path)
    collector.run(tickers_to_process=tickers_to_process)


if __name__ == "__main__":
    main()
