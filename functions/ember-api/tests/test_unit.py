"""
å•å…ƒæµ‹è¯•å¥—ä»¶

æµ‹è¯•æ‰€æœ‰æœåŠ¡çš„æ ¸å¿ƒåŠŸèƒ½
"""

import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„
api_path = Path(__file__).parent.parent
sys.path.insert(0, str(api_path))
ember_path = api_path.parent.parent / "ember-main" / "src"
sys.path.insert(0, str(ember_path))


def test_cost_optimizer():
    """æµ‹è¯•æˆæœ¬ä¼˜åŒ–å™¨"""
    from services.cost_optimizer_service import get_cost_optimizer

    optimizer = get_cost_optimizer()

    # æµ‹è¯•å¤æ‚åº¦åˆ†æ
    complexity, score = optimizer.analyze_complexity("ä»€ä¹ˆæ˜¯AI?")
    assert complexity in ["simple", "medium", "complex"]
    print(f"âœ“ å¤æ‚åº¦åˆ†æ: {complexity} (score: {score:.2f})")

    # æµ‹è¯•æ¨¡å¼ä¼˜åŒ–
    mode, reason, savings = optimizer.optimize_mode_selection("ensemble", "ä½ å¥½", "free")
    print(f"âœ“ æ¨¡å¼ä¼˜åŒ–: {mode} (èŠ‚çœ: {savings:.1f}%)")

    # æµ‹è¯•ç¼“å­˜å»ºè®®
    should_cache, reason = optimizer.suggest_cache_usage("ä»€ä¹ˆæ˜¯AI?", "default")
    print(f"âœ“ ç¼“å­˜å»ºè®®: {should_cache} ({reason})")

    print("âœ… æˆæœ¬ä¼˜åŒ–å™¨æµ‹è¯•é€šè¿‡")


def test_model_selection():
    """æµ‹è¯•æ™ºèƒ½æ¨¡å‹é€‰æ‹©"""
    from services.ember_service import EmberService

    service = EmberService()

    # æµ‹è¯•çŸ­é—®é¢˜
    model = service._select_model("ä½ å¥½", "auto")
    assert model == "gemini-2.5-flash"
    print(f"âœ“ çŸ­é—®é¢˜é€‰æ‹©: {model}")

    # æµ‹è¯•æ·±åº¦é—®é¢˜
    model = service._select_model("ä¸ºä»€ä¹ˆAIå¾ˆé‡è¦? è¯¦ç»†åˆ†æåŸå› ", "auto")
    assert model == "gpt-5"
    print(f"âœ“ æ·±åº¦é—®é¢˜é€‰æ‹©: {model}")

    # æµ‹è¯•ç”¨æˆ·åå¥½
    model = service._select_model("ä»»æ„é—®é¢˜", "fast")
    assert model == "gemini-2.5-flash"
    print(f"âœ“ ç”¨æˆ·åå¥½é€‰æ‹©: {model}")

    print("âœ… æ¨¡å‹é€‰æ‹©æµ‹è¯•é€šè¿‡")


def test_prompt_building():
    """æµ‹è¯• prompt æ„å»º"""
    from services.ember_service import EmberService

    service = EmberService()

    # æµ‹è¯•æ— ä¸Šä¸‹æ–‡
    prompt = service._build_prompt("ä½ å¥½", None, "ZH")
    assert prompt == "ä½ å¥½"
    print("âœ“ æ— ä¸Šä¸‹æ–‡ prompt æ­£ç¡®")

    # æµ‹è¯•æœ‰ä¸Šä¸‹æ–‡
    context = {
        "economic": -2.5,
        "social": 3.1,
        "label": "Social Democrat"
    }
    prompt = service._build_prompt("é—®é¢˜", context, "ZH")
    assert "Social Democrat" in prompt
    assert "ç»æµè§‚ç‚¹" in prompt
    print("âœ“ ä¸Šä¸‹æ–‡ prompt æ­£ç¡®")

    print("âœ… Prompt æ„å»ºæµ‹è¯•é€šè¿‡")


def test_cache_key_generation():
    """æµ‹è¯•ç¼“å­˜é”®ç”Ÿæˆ"""
    from services.cache_service import CacheService

    service = CacheService()

    # æµ‹è¯•ç›¸åŒè¾“å…¥ç”Ÿæˆç›¸åŒé”®
    key1 = service.generate_cache_key("ä½ å¥½", "default", {"economic": -2.5})
    key2 = service.generate_cache_key("ä½ å¥½", "default", {"economic": -2.5})
    assert key1 == key2
    print("âœ“ ç›¸åŒè¾“å…¥ç”Ÿæˆç›¸åŒé”®")

    # æµ‹è¯•ä¸åŒè¾“å…¥ç”Ÿæˆä¸åŒé”®
    key3 = service.generate_cache_key("ä½ å¥½", "multi", {"economic": -2.5})
    assert key1 != key3
    print("âœ“ ä¸åŒè¾“å…¥ç”Ÿæˆä¸åŒé”®")

    # æµ‹è¯•æ ‡å‡†åŒ–ï¼ˆå¾®å°å·®å¼‚åº”ç”Ÿæˆç›¸åŒé”®ï¼‰
    key4 = service.generate_cache_key("ä½ å¥½", "default", {"economic": -2.52})
    assert key1 == key4  # -2.5 å’Œ -2.52 åº”å››èˆäº”å…¥ä¸º -2.5
    print("âœ“ æ ‡å‡†åŒ–æ­£ç¡®")

    print("âœ… ç¼“å­˜é”®ç”Ÿæˆæµ‹è¯•é€šè¿‡")


if __name__ == '__main__':
    print("=" * 80)
    print(" " * 28 + "å•å…ƒæµ‹è¯•")
    print("=" * 80)
    print()

    print("ğŸ“‹ æµ‹è¯• 1: æˆæœ¬ä¼˜åŒ–å™¨")
    print("-" * 80)
    test_cost_optimizer()
    print()

    print("ğŸ“‹ æµ‹è¯• 2: æ™ºèƒ½æ¨¡å‹é€‰æ‹©")
    print("-" * 80)
    test_model_selection()
    print()

    print("ğŸ“‹ æµ‹è¯• 3: Prompt æ„å»º")
    print("-" * 80)
    test_prompt_building()
    print()

    print("ğŸ“‹ æµ‹è¯• 4: ç¼“å­˜é”®ç”Ÿæˆ")
    print("-" * 80)
    test_cache_key_generation()
    print()

    print("=" * 80)
    print("âœ… æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼")
    print("=" * 80)
