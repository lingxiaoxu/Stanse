"""
æ€§èƒ½æµ‹è¯•å¥—ä»¶

æµ‹è¯•å„ç§åœºæ™¯ä¸‹çš„æ€§èƒ½æŒ‡æ ‡
"""

import sys
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ è·¯å¾„
api_path = Path(__file__).parent.parent
sys.path.insert(0, str(api_path))
ember_path = api_path.parent.parent / "ember-main" / "src"
sys.path.insert(0, str(ember_path))

from services.ember_service import get_ember_service


def test_default_mode_latency():
    """æµ‹è¯• Default æ¨¡å¼å»¶è¿Ÿ"""
    service = get_ember_service()

    print("æµ‹è¯• Default æ¨¡å¼æ€§èƒ½...")

    start = time.time()
    result = service.chat(
        message="ä½ å¥½",
        mode="default",
        language="ZH"
    )
    latency = time.time() - start

    print(f"  å“åº”æ—¶é—´: {latency:.2f}ç§’")
    print(f"  ç›®æ ‡: < 3ç§’")

    if latency < 3.0:
        print(f"  âœ… é€šè¿‡")
    else:
        print(f"  âš ï¸  è¶…è¿‡ç›®æ ‡")

    return latency


def test_multi_mode_latency():
    """æµ‹è¯• Multi æ¨¡å¼å»¶è¿Ÿ"""
    service = get_ember_service()

    print("æµ‹è¯• Multi æ¨¡å¼æ€§èƒ½...")

    start = time.time()
    result = service.chat(
        message="AIæ˜¯ä»€ä¹ˆ? ç®€çŸ­å›ç­”",
        mode="multi",
        language="ZH"
    )
    latency = time.time() - start

    print(f"  å“åº”æ—¶é—´: {latency:.2f}ç§’")
    print(f"  ç›®æ ‡: < 6ç§’")

    if latency < 6.0:
        print(f"  âœ… é€šè¿‡")
    else:
        print(f"  âš ï¸  è¶…è¿‡ç›®æ ‡")

    return latency


def test_concurrent_requests():
    """æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›"""
    service = get_ember_service()

    print("æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›...")

    questions = [
        "ä»€ä¹ˆæ˜¯Python?",
        "ä»€ä¹ˆæ˜¯JavaScript?",
        "ä»€ä¹ˆæ˜¯Rust?",
        "ä»€ä¹ˆæ˜¯Go?",
        "ä»€ä¹ˆæ˜¯TypeScript?"
    ]

    def process_question(q):
        return service.chat(message=q, mode="default")

    start = time.time()

    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(process_question, questions))

    total_time = time.time() - start

    print(f"  å¤„ç† {len(questions)} ä¸ªè¯·æ±‚")
    print(f"  æ€»æ—¶é—´: {total_time:.2f}ç§’")
    print(f"  å¹³å‡: {total_time/len(questions):.2f}ç§’/è¯·æ±‚")
    print(f"  ååé‡: {len(questions)/total_time:.2f} req/s")

    successful = sum(1 for r in results if r['success'])
    print(f"  æˆåŠŸç‡: {successful}/{len(questions)} ({successful/len(questions)*100:.1f}%)")

    if total_time / len(questions) < 3.0:
        print(f"  âœ… æ€§èƒ½è¾¾æ ‡")
    else:
        print(f"  âš ï¸  æ€§èƒ½å¾…ä¼˜åŒ–")

    return total_time / len(questions)


if __name__ == '__main__':
    print("=" * 80)
    print(" " * 26 + "æ€§èƒ½å‹æµ‹")
    print("=" * 80)
    print()

    print("ğŸ“Š æµ‹è¯• 1: Default æ¨¡å¼å»¶è¿Ÿ")
    print("-" * 80)
    default_latency = test_default_mode_latency()
    print()

    print("ğŸ“Š æµ‹è¯• 2: Multi æ¨¡å¼å»¶è¿Ÿ")
    print("-" * 80)
    multi_latency = test_multi_mode_latency()
    print()

    print("ğŸ“Š æµ‹è¯• 3: å¹¶å‘å¤„ç†èƒ½åŠ›")
    print("-" * 80)
    concurrent_latency = test_concurrent_requests()
    print()

    print("=" * 80)
    print(" " * 28 + "æ€§èƒ½æ€»ç»“")
    print("=" * 80)
    print()
    print(f"  Default å»¶è¿Ÿ:  {default_latency:.2f}s (ç›®æ ‡: <3s)")
    print(f"  Multi å»¶è¿Ÿ:    {multi_latency:.2f}s (ç›®æ ‡: <6s)")
    print(f"  å¹¶å‘å¹³å‡:      {concurrent_latency:.2f}s (ç›®æ ‡: <3s)")
    print()

    all_passed = (
        default_latency < 3.0 and
        multi_latency < 6.0 and
        concurrent_latency < 3.0
    )

    if all_passed:
        print("âœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æ€§èƒ½æµ‹è¯•æœªè¾¾æ ‡ï¼Œéœ€è¦ä¼˜åŒ–")

    print("=" * 80)
